{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Chapter 1, section 1.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time you import nltk on your local machine, you need to download the data sets that are used in the course. You will get a pop-up window to select datasets. The minimal data set you need is  \"book\". Take your time to check out the different TABs and have an idea what is there. Make sure you have sufficient disk space to store what you want.\n",
    "\n",
    "If you did download once, you can skip the next step as the data are in your local drive. If you need another dataset of package, run it again and take your pick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "nltk.download() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Chapter 1, section 3.1 Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "<FreqDist with 19317 samples and 260819 outcomes>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "906"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "from nltk import FreqDist\n",
    "fdist1 = FreqDist(text1) \n",
    "print(fdist1)\n",
    "fdist1.most_common(50)\n",
    "fdist1['whale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: The best way to go about these assignments is to first read the section in the NLTK book that is mentioned, then start playing around with it. That should give you the hints to deal with an error message such as  \"NameError: name 'FreqDist' is not defined\". \n",
    "\n",
    "It's like cooking a recipe for the first time, you probably want to read the full recipe first, before you start cooking, in case you find out at step 5 that you should have soaked your beans 12 hours in advance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation with NLTK \n",
    "\n",
    "NLTK Chapter 5, section 1 Using a Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'll\",\n",
       " 'refuse',\n",
       " 'to',\n",
       " 'permit',\n",
       " 'you',\n",
       " 'to',\n",
       " 'obtain',\n",
       " 'the',\n",
       " 'refuse',\n",
       " 'permit.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I'll refuse to permit you to obtain the refuse permit.\".split()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about the above line, is it actually the same as tokenising? \n",
    "Let's try a real tokenizer...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'ll\",\n",
       " 'refuse',\n",
       " 'to',\n",
       " 'permit',\n",
       " 'you',\n",
       " 'to',\n",
       " 'obtain',\n",
       " 'the',\n",
       " 'refuse',\n",
       " 'permit',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "text = word_tokenize(\"I'll refuse to permit you to obtain the refuse permit.\")\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also call the tokenizer directly from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'ll\",\n",
       " 'refuse',\n",
       " 'to',\n",
       " 'permit',\n",
       " 'you',\n",
       " 'to',\n",
       " 'obtain',\n",
       " 'the',\n",
       " 'refuse',\n",
       " 'permit',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"I'll refuse to permit you to obtain the refuse permit.\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " (\"'ll\", 'MD'),\n",
       " ('refuse', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('permit', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('refuse', 'NN'),\n",
       " ('permit', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Make sure you know what each tag means. Would you have tagged each word using the same tag or do you disagree with the tagger? \n",
    "\n",
    "Observe some word contexts to get an idea of why NLTK tags the way it tags (Chapter 1, section 1.3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more on cleaning up text and getting the basic forms for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\"Here are some very simple basic sentences.\",\n",
    "\"They won't be very interesting, I'm afraid.\",\n",
    "\"The point of these examples is to _learn how basic text cleaning works_ on *very simple* data.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences', '.'], ['They', 'wo', \"n't\", 'be', 'very', 'interesting', ',', 'I', \"'m\", 'afraid', '.'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', '_learn', 'how', 'basic', 'text', 'cleaning', 'works_', 'on', '*very', 'simple*', 'data', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing text into bags of words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences'], ['They', 'wo', 'nt', 'be', 'very', 'interesting', 'I', 'm', 'afraid'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', 'learn', 'how', 'basic', 'text', 'cleaning', 'works', 'on', 'very', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuation\n",
    "import re\n",
    "import string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) \n",
    "#see documentation here: http://docs.python.org/2/library/string.html\n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    \n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'simple', 'basic', 'sentences'], ['They', 'wo', 'nt', 'interesting', 'I', 'afraid'], ['The', 'point', 'examples', 'learn', 'basic', 'text', 'cleaning', 'works', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "# Cleaning text of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_docs_no_stopwords = []\n",
    "\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in doc:\n",
    "        if not word in stopwords.words('english'):\n",
    "            new_term_vector.append(word)\n",
    "    \n",
    "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
    "\n",
    "print(tokenized_docs_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "What are stopwords and why would you want to remove these from a text?\n",
    "What would be another way to define stop words than making a list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and lemmatizing\n",
    "NLTK has various modules for stripping inflection of words (stemming) or finding the lemma (the form you can find in a dicitonary). Below is a script to stem and lemmatize the words in a text example after tokizing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw=\"SHUT UP! Enough already, Ballstein! Who cares about Derek Zoolander anyway? The man has only one look, for Christ's sake! Blue Steel? Ferrari? Le Tigra?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter\n",
      "['shut', 'UP', '!', 'enough', 'alreadi', ',', 'ballstein', '!', 'who', 'care', 'about', 'derek', 'zooland', 'anyway', '?', 'the', 'man', 'ha', 'onli', 'one', 'look', ',', 'for', 'christ', \"'s\", 'sake', '!', 'blue', 'steel', '?', 'ferrari', '?', 'Le', 'tigra', '?']\n",
      "Snowball\n",
      "['shut', 'up', '!', 'enough', 'alreadi', ',', 'ballstein', '!', 'who', 'care', 'about', 'derek', 'zooland', 'anyway', '?', 'the', 'man', 'has', 'onli', 'one', 'look', ',', 'for', 'christ', \"'s\", 'sake', '!', 'blue', 'steel', '?', 'ferrari', '?', 'le', 'tigra', '?']\n",
      "Wordnet\n",
      "['SHUT', 'UP', '!', 'Enough', 'already', ',', 'Ballstein', '!', 'Who', 'care', 'about', 'Derek', 'Zoolander', 'anyway', '?', 'The', 'man', 'ha', 'only', 'one', 'look', ',', 'for', 'Christ', \"'s\", 'sake', '!', 'Blue', 'Steel', '?', 'Ferrari', '?', 'Le', 'Tigra', '?']\n"
     ]
    }
   ],
   "source": [
    "# Stemming and Lemmatizing\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "porterlemmas = []\n",
    "wordnetlemmas = []\n",
    "snowballlemmas = []\n",
    "\n",
    "for word in tokens:\n",
    "    porterlemmas.append(porter.stem(word))\n",
    "    snowballlemmas.append(snowball.stem(word))\n",
    "    wordnetlemmas.append(wordnet.lemmatize(word))\n",
    "\n",
    "print('Porter')\n",
    "print(porterlemmas)\n",
    "print('Snowball')\n",
    "print(snowballlemmas)\n",
    "print('Wordnet')\n",
    "print(wordnetlemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "What do you notice as a difference? What will be better for finding words in a dictionary? What output is better for looking up entities in Wikipedia?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Do something on chunking, parsing and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "== START OF THE LAB ASSIGNMENT == "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab assignment consists of 3 parts: Sentence Boundary detection, Reading Tagged Corpora and Stemming . Submit your ipynb with the answers via Canvas. Please note that submissions should contain a title, your names and a description of the assignment, such that the document is interpretable on its own. At the least this means copying the task description!\n",
    "\n",
    "You don't need to provide a long explanation for your commands, simply the commands you used, and maybe some output will suffice. We will strive to give you feedback before the next Lab session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence boundary detection with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK offers a sentence boundary detection function, similar to the word tokenisation function. It is called sent_tokenize(). Submit the commands that you use to print the last sentence of a sentence split text using NLTK. The task can be carried out in 4 or 5 steps. \n",
    "\n",
    "- Create a string that contains several sentences. \n",
    "- Import the sent_tokenize function \n",
    "- Tokenize / detect sentence boundaries in your text and store the results in a list \n",
    "- Get the number of sentences (number of list items) (can be skipped for those more familiar with Python) \n",
    "- print the last sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Chapter 5, Section 2.2 & 2.3 Reading tagged corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: NLTK contains a Dutch tagged corpus, can you print the most common tags from it? \n",
    "\n",
    "For this you need to know that the Dutch tags are contained in the conll2002 corpus. \n",
    "\n",
    "The conll2002 corpus also contains Spanish text, so you need to only read in the Dutch part. \n",
    "\n",
    "You can check what types of information the corpus class contains with the dir command. \n",
    "\n",
    "When you create a subset of the conll corpus, you need to include the files that start with 'ned'.\n",
    "\n",
    "Check the example of getting the 10 most common tags from the Brown corpus in Section 2.3 of chapter 5.\n",
    "\n",
    "Note: the conll2002  corpus doesn't contain the universal POS tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error propagation \n",
    "(this means that an error in an earlier step of the pipeline, for example in the tokeniser, later steps harms, see http://ceur-ws.org/Vol-1386/piling_up.pdf for a more in-depth discussion) \n",
    "\n",
    "Tokenisation and POS-tagging are often used as pre-processing steps for higher level tasks such as named entity recognition. If you run the text \"SHUT UP! Enough already, Ballstein! Who cares about Derek Zoolander anyway? The man has only one look, for Christ's sake! Blue Steel? Ferrari? Le Tigra?\" through NLTK's tokeniser and POS-tagger, which errors do you think may arise that can harm the named entity recogniser at a later stage? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "== END OF THE LAB ASSIGNMENT =="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Working towards the final assignment & further tips & ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['README', 'connectives', 'propernames', 'web2', 'web2a', 'words']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading your own text \n",
    "\n",
    "## NLTK Chapter 2, section 1.9\n",
    "\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "corpus_root = '/usr/share/dict' \n",
    "\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*') \n",
    "\n",
    "wordlists.fileids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'of', 'and', 'to', 'a', 'in', 'that', 'is', ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlists.words('connectives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "Homework/Working towards the final assignment:\n",
    "\n",
    "Check out the data for the final assignment and tokenise and POS tag the texts you selected. You find the link and description of the data in \"Course Documents\".\n",
    "\n",
    "If you perform these steps now, and document them for yourself, you can 'grow' your data for the final assignment. \n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "Tips and tricks:\n",
    "\n",
    "If you are not familiar with storing your code in scripts (for easy reuse) you can find some suggestions in Chapter 2, section 3\n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "Further reading & hacking: \n",
    "\n",
    "More information on dealing with character encodings:\n",
    "\n",
    "NLTK 3.3 http://www.nltk.org/book/ch03.html#sec-unicode \n",
    "\n",
    "Should you be interested in tokenising and POS tagging text in languages other than Dutch, you can find information on training your own tokeniser for a variety of languages at:\n",
    "\n",
    "http://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
