{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLTK\n",
    "\n",
    "This notebook demonstrates how to use NLTK for various NLP tasks. NLTK is an older package of tools that let's you work with low level modules for text processing.\n",
    "\n",
    "## Background reading\n",
    "\n",
    "Before you check out this notebook, read through:\n",
    "\n",
    "https://www.nltk.org/book/ch01.html\n",
    "* section 1.2\n",
    "\n",
    "https://www.nltk.org/book/ch03.html\n",
    "\n",
    "* section 3.4 Regular Expressions for Detecting Word Patterns\n",
    "* section 3.5 Useful Applications of Regular Expressions\n",
    "* section 3.6 Normalizing Text\n",
    "* section 3.7 Regular Expressions for Tokenizing Text\n",
    "* section 3.8 Segmentation\n",
    "\n",
    "https://www.nltk.org/book/ch05.html\n",
    "\n",
    "* section 5.1 Using a Tagger\n",
    "* section 5.4 Automatic Tagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: The best way to go about these assignments is to first read the section in the NLTK book that is mentioned, then start playing around with it. That should give you the hints to deal with an error message such as  \"NameError: name 'FreqDist' is not defined\". \n",
    "\n",
    "It's like cooking a recipe for the first time, you probably want to read the full recipe first, before you start cooking, in case you find out at step 5 that you should have soaked your beans 12 hours in advance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Chapter 1, section 1.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time you import nltk on your local machine, you need to download the data sets that are used in the course. You will get a pop-up window to select datasets. The minimal data set you need is  \"book\". Take your time to check out the different TABs and have an idea what is there. Make sure you have sufficient disk space to store what you want.\n",
    "\n",
    "If you did download once, you can skip the next step as the data are in your local drive. If you need another dataset of package, run it again and take your pick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation with NLTK \n",
    "\n",
    "NLTK Chapter 5, section 1 Using a Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'll\",\n",
       " 'refuse',\n",
       " 'to',\n",
       " 'permit',\n",
       " 'you',\n",
       " 'to',\n",
       " 'obtain',\n",
       " 'the',\n",
       " 'refuse',\n",
       " 'permit.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = \"I'll refuse to permit you to obtain the refuse permit.\"\n",
    "text = example_text.split()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about the above line, is it actually the same as tokenising? \n",
    "Let's try a real tokenizer...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'ll\",\n",
       " 'refuse',\n",
       " 'to',\n",
       " 'permit',\n",
       " 'you',\n",
       " 'to',\n",
       " 'obtain',\n",
       " 'the',\n",
       " 'refuse',\n",
       " 'permit',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "text = word_tokenize(\"I'll refuse to permit you to obtain the refuse permit.\")\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also call the tokenizer directly from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'ll\",\n",
       " 'refuse',\n",
       " 'to',\n",
       " 'permit',\n",
       " 'you',\n",
       " 'to',\n",
       " 'obtain',\n",
       " 'the',\n",
       " 'refuse',\n",
       " 'permit',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"I'll refuse to permit you to obtain the refuse permit.\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " (\"'ll\", 'MD'),\n",
       " ('refuse', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('permit', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('refuse', 'NN'),\n",
       " ('permit', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Make sure you know what each tag means. Would you have tagged each word using the same tag or do you disagree with the tagger? \n",
    "\n",
    "Observe some word contexts to get an idea of why NLTK tags the way it tags (Chapter 1, section 1.3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more on cleaning up text and getting the basic forms for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\"Here are some very simple basic sentences.\",\n",
    "\"They won't be very interesting, I'm afraid.\",\n",
    "\"The point of these examples is to _learn how basic text cleaning works_ on *very simple* data.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences', '.'], ['They', 'wo', \"n't\", 'be', 'very', 'interesting', ',', 'I', \"'m\", 'afraid', '.'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', '_learn', 'how', 'basic', 'text', 'cleaning', 'works_', 'on', '*very', 'simple*', 'data', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing text into bags of words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences'], ['They', 'wo', 'nt', 'be', 'very', 'interesting', 'I', 'm', 'afraid'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', 'learn', 'how', 'basic', 'text', 'cleaning', 'works', 'on', 'very', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuation\n",
    "import re\n",
    "import string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) \n",
    "#see documentation here: http://docs.python.org/2/library/string.html\n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    \n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'simple', 'basic', 'sentences'], ['They', 'wo', 'nt', 'interesting', 'I', 'afraid'], ['The', 'point', 'examples', 'learn', 'basic', 'text', 'cleaning', 'works', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "# Cleaning text of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_docs_no_stopwords = []\n",
    "\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in doc:\n",
    "        if not word in stopwords.words('english'):\n",
    "            new_term_vector.append(word)\n",
    "    \n",
    "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
    "\n",
    "print(tokenized_docs_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "What are stopwords and why would you want to remove these from a text?\n",
    "What would be another way to define stop words than making a list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and lemmatizing\n",
    "NLTK has various modules for stripping inflection of words (stemming) or finding the lemma (the form you can find in a dicitonary). Below is a script to stem and lemmatize the words in a text example after tokizing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw=\"SHUT UP! Enough already, Ballstein! Who cares about Derek Zoolander anyway? The man has only one look, for Christ's sake! Blue Steel? Ferrari? Le Tigra?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter\n",
      "['shut', 'UP', '!', 'enough', 'alreadi', ',', 'ballstein', '!', 'who', 'care', 'about', 'derek', 'zooland', 'anyway', '?', 'the', 'man', 'ha', 'onli', 'one', 'look', ',', 'for', 'christ', \"'s\", 'sake', '!', 'blue', 'steel', '?', 'ferrari', '?', 'Le', 'tigra', '?']\n",
      "Snowball\n",
      "['shut', 'up', '!', 'enough', 'alreadi', ',', 'ballstein', '!', 'who', 'care', 'about', 'derek', 'zooland', 'anyway', '?', 'the', 'man', 'has', 'onli', 'one', 'look', ',', 'for', 'christ', \"'s\", 'sake', '!', 'blue', 'steel', '?', 'ferrari', '?', 'le', 'tigra', '?']\n",
      "Wordnet\n",
      "['SHUT', 'UP', '!', 'Enough', 'already', ',', 'Ballstein', '!', 'Who', 'care', 'about', 'Derek', 'Zoolander', 'anyway', '?', 'The', 'man', 'ha', 'only', 'one', 'look', ',', 'for', 'Christ', \"'s\", 'sake', '!', 'Blue', 'Steel', '?', 'Ferrari', '?', 'Le', 'Tigra', '?']\n"
     ]
    }
   ],
   "source": [
    "# Stemming and Lemmatizing\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "porterlemmas = []\n",
    "wordnetlemmas = []\n",
    "snowballlemmas = []\n",
    "\n",
    "for word in tokens:\n",
    "    porterlemmas.append(porter.stem(word))\n",
    "    snowballlemmas.append(snowball.stem(word))\n",
    "    wordnetlemmas.append(wordnet.lemmatize(word))\n",
    "\n",
    "print('Porter')\n",
    "print(porterlemmas)\n",
    "print('Snowball')\n",
    "print(snowballlemmas)\n",
    "print('Wordnet')\n",
    "print(wordnetlemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "What do you notice as a difference? What will be better for finding words in a dictionary? What output is better for looking up entities in Wikipedia?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIP\n",
    "How to get a list of input files from a directory and read the complete content using NLTK PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 text files in this folder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['README.txt', 'plot.tok.gt9.5000', 'quote.tok.gt9.5000']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading your own text \n",
    "\n",
    "## NLTK Chapter 2, section 1.9\n",
    "\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "corpus_root = '/Users/piek/nltk_data/corpora/subjectivity'\n",
    "\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*') \n",
    "\n",
    "print(\"There are 3 text files in this folder\")\n",
    "wordlists.fileids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subjectivity', 'Dataset', 'v1', '.', '0', ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can access all the words from each through a list\n",
    "wordlists.words('README.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A running example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from pprint import pprint # for pretty print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the text on the Apple-Samsung patent law cases. Download the examples file from Canvas and store it on your local drive. Adapt the path to your location in the examples below. Note that Windows uses backward slashes where Linux and Mac use forwards slashes for directory paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.telegraph.co.uk/technology/apple/9702716/Apple-Samsung-lawsuit-six-more-products-under-scrutiny.html\n",
      "\n",
      "Documents filed to the San Jose federal court in California on November 23 list six Samsung products running the \"Jelly Bean\" and \"Ice Cream Sandwich\" operating systems, which Apple claims infringe its patents.\n",
      "The six phones and tablets affected are the Galaxy S III, running the new Jelly Bean system, the Galaxy Tab 8.9 Wifi tablet, the Galaxy Tab 2 10.1, Galaxy Rugby Pro and Galaxy S III mini.\n",
      "Apple stated it had “acted quickly and diligently\" in order to \"determine that these newly released products do infringe many of the same claims already asserted by Apple.\"\n",
      "In August, Samsung lost a US patent case to Apple and was ordered to pay its rival $1.05bn (£0.66bn) in damages for copying features of the iPad and iPhone in its Galaxy range of devices. Samsung, which is the world's top mobile phone maker, is appealing the ruling.\n",
      "A similar case in the UK found in Samsung's favour and ordered Apple to publish an apology making clear that the South Korean firm had not copied its iPad when designing its own devices.\n"
     ]
    }
   ],
   "source": [
    "# the next function reads the content of a file. Make sure the path is correct.\n",
    "f=open('/Users/piek/Desktop/TextMiningFEW-2019/text-mining-ba-git/notebooks/Lab1-apple-samsung-example.txt','r')\n",
    "example_text=f.read()\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the sentences from this document in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = nltk.sent_tokenize(example_text)\n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"A similar case in the UK found in Samsung's favour and ordered Apple to \"\n",
      " 'publish an apology making clear that the South Korean firm had not copied '\n",
      " 'its iPad when designing its own devices.')\n"
     ]
    }
   ],
   "source": [
    "# To get the last sentence\n",
    "pprint(sents[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging a text\n",
    "\n",
    "https://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('similar', 'JJ'),\n",
       " ('case', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('UK', 'NNP'),\n",
       " ('found', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('Samsung', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('favour', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('ordered', 'VBD'),\n",
       " ('Apple', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('publish', 'VB'),\n",
       " ('an', 'DT'),\n",
       " ('apology', 'NN'),\n",
       " ('making', 'VBG'),\n",
       " ('clear', 'JJ'),\n",
       " ('that', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('South', 'JJ'),\n",
       " ('Korean', 'JJ'),\n",
       " ('firm', 'NN'),\n",
       " ('had', 'VBD'),\n",
       " ('not', 'RB'),\n",
       " ('copied', 'VBN'),\n",
       " ('its', 'PRP$'),\n",
       " ('iPad', 'NN'),\n",
       " ('when', 'WRB'),\n",
       " ('designing', 'VBG'),\n",
       " ('its', 'PRP$'),\n",
       " ('own', 'JJ'),\n",
       " ('devices', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lastsentence = preprocess(sents[-1])\n",
    "lastsentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "patternNP = 'NP: {<DT>?<JJ>*<NNP>}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  A/DT\n",
      "  similar/JJ\n",
      "  case/NN\n",
      "  in/IN\n",
      "  (NP the/DT UK/NNP)\n",
      "  found/VBD\n",
      "  in/IN\n",
      "  (NP Samsung/NNP)\n",
      "  's/POS\n",
      "  favour/NN\n",
      "  and/CC\n",
      "  ordered/VBD\n",
      "  (NP Apple/NNP)\n",
      "  to/TO\n",
      "  publish/VB\n",
      "  an/DT\n",
      "  apology/NN\n",
      "  making/VBG\n",
      "  clear/JJ\n",
      "  that/IN\n",
      "  the/DT\n",
      "  South/JJ\n",
      "  Korean/JJ\n",
      "  firm/NN\n",
      "  had/VBD\n",
      "  not/RB\n",
      "  copied/VBN\n",
      "  its/PRP$\n",
      "  iPad/NN\n",
      "  when/WRB\n",
      "  designing/VBG\n",
      "  its/PRP$\n",
      "  own/JJ\n",
      "  devices/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "constituent_parser = nltk.RegexpParser(patternNP)\n",
    "constituent_structure = constituent_parser.parse(lastsentence)\n",
    "print(constituent_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate \n",
    "constituent_structure.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you need to exit the program with the drawing that is shown as a pop up. The notebook will hang untill you killed the pop-up program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a constituent parser yourself using several rules, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = nltk.RegexpParser('''\n",
    "... NP: {<DT>? <JJ>* <NN>*} # NP\n",
    "... P: {<IN>}           # Preposition\n",
    "... V: {<V.*>}          # Verb\n",
    "... PP: {<P> <NP>}      # PP -> P NP\n",
    "... VP: {<V> <NP|PP>*}  # VP -> V (NP|PP)*\n",
    "... ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT similar/JJ case/NN)\n",
      "  (PP (P in/IN) (NP the/DT))\n",
      "  UK/NNP\n",
      "  (VP (V found/VBD))\n",
      "  (P in/IN)\n",
      "  Samsung/NNP\n",
      "  's/POS\n",
      "  (NP favour/NN)\n",
      "  and/CC\n",
      "  (VP (V ordered/VBD))\n",
      "  Apple/NNP\n",
      "  to/TO\n",
      "  (VP (V publish/VB) (NP an/DT apology/NN))\n",
      "  (VP\n",
      "    (V making/VBG)\n",
      "    (NP clear/JJ)\n",
      "    (PP (P that/IN) (NP the/DT South/JJ Korean/JJ firm/NN)))\n",
      "  (VP (V had/VBD))\n",
      "  not/RB\n",
      "  (VP (V copied/VBN))\n",
      "  its/PRP$\n",
      "  (NP iPad/NN)\n",
      "  when/WRB\n",
      "  (VP (V designing/VBG))\n",
      "  its/PRP$\n",
      "  (NP own/JJ)\n",
      "  devices/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "constituent_structure = parser.parse(lastsentence)\n",
    "print(constituent_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a drawing\n",
    "constituent_structure.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dont forget to kill the drawing. You can save it to a PostScript file before exiting the program. The PostScript file can be loaded as a PDF (and coverted to other formats)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entities in NLTK\n",
    "\n",
    "You can use the ne_chunk module from NLTK to assign entity types to the tokeized and part-of-speech output of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  A/DT\n",
      "  similar/JJ\n",
      "  case/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION UK/NNP)\n",
      "  found/VBD\n",
      "  in/IN\n",
      "  (GPE Samsung/NNP)\n",
      "  's/POS\n",
      "  favour/NN\n",
      "  and/CC\n",
      "  ordered/VBD\n",
      "  (PERSON Apple/NNP)\n",
      "  to/TO\n",
      "  publish/VB\n",
      "  an/DT\n",
      "  apology/NN\n",
      "  making/VBG\n",
      "  clear/JJ\n",
      "  that/IN\n",
      "  the/DT\n",
      "  (LOCATION South/JJ Korean/JJ)\n",
      "  firm/NN\n",
      "  had/VBD\n",
      "  not/RB\n",
      "  copied/VBN\n",
      "  its/PRP$\n",
      "  iPad/NN\n",
      "  when/WRB\n",
      "  designing/VBG\n",
      "  its/PRP$\n",
      "  own/JJ\n",
      "  devices/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "ne_tree = ne_chunk(lastsentence)\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting tagged tokens to CoNLL format\n",
    "The next shows how tokenized texts with simple labels (PoS and entity types) can be converted to the CoNLL format. This is a common format in NLP for which there is a lot of training and evaluation data for many languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT', 'O'),\n",
      " ('similar', 'JJ', 'O'),\n",
      " ('case', 'NN', 'O'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('the', 'DT', 'O'),\n",
      " ('UK', 'NNP', 'B-ORGANIZATION'),\n",
      " ('found', 'VBD', 'O'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('Samsung', 'NNP', 'B-GPE'),\n",
      " (\"'s\", 'POS', 'O'),\n",
      " ('favour', 'NN', 'O'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('ordered', 'VBD', 'O'),\n",
      " ('Apple', 'NNP', 'B-PERSON'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('publish', 'VB', 'O'),\n",
      " ('an', 'DT', 'O'),\n",
      " ('apology', 'NN', 'O'),\n",
      " ('making', 'VBG', 'O'),\n",
      " ('clear', 'JJ', 'O'),\n",
      " ('that', 'IN', 'O'),\n",
      " ('the', 'DT', 'O'),\n",
      " ('South', 'JJ', 'B-LOCATION'),\n",
      " ('Korean', 'JJ', 'I-LOCATION'),\n",
      " ('firm', 'NN', 'O'),\n",
      " ('had', 'VBD', 'O'),\n",
      " ('not', 'RB', 'O'),\n",
      " ('copied', 'VBN', 'O'),\n",
      " ('its', 'PRP$', 'O'),\n",
      " ('iPad', 'NN', 'O'),\n",
      " ('when', 'WRB', 'O'),\n",
      " ('designing', 'VBG', 'O'),\n",
      " ('its', 'PRP$', 'O'),\n",
      " ('own', 'JJ', 'O'),\n",
      " ('devices', 'NNS', 'O'),\n",
      " ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "iob_tagged = tree2conlltags(ne_tree)\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
