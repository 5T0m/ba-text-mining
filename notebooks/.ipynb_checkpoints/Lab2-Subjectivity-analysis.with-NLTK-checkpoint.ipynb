{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2 Subjectivity analysis using NLTK\n",
    "\n",
    "You are going to create a Subjectivity classifier from data provided in NLTK\n",
    "\n",
    "## Background reading\n",
    "\n",
    "* http://www.nltk.org/api/nltk.sentiment.html#module-nltk.sentiment.util\n",
    "* https://www.nltk.org/book/ch06.html\n",
    "    * section 6.1\n",
    "    * section 6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the datasets (subjective and objective sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "# Loading stuff\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import * # needed for the mark_negation function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first obtain the subjectivity corpus that is included in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this data set we are going to select 200 sentences for training and testing.\n",
    "The package subjectivity.sents defines which sentences are subjective ('subj') and which ones are objective ('obj')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_instances = 100\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "len(subj_docs), len(obj_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now balanced. Why is this important for a NaiveBayesClassifier? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Document is represented by a tuple (ie. in the form <sentence, label>. The sentence is tokenised, so it is represented by a list of strings. The labels is subj or obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"there's\",\n",
       "  'lots',\n",
       "  'of',\n",
       "  'cool',\n",
       "  'stuff',\n",
       "  'packed',\n",
       "  'into',\n",
       "  \"espn's\",\n",
       "  'ultimate',\n",
       "  'x',\n",
       "  '.'],\n",
       " 'subj')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj_docs[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subjective and objective instances were split separately, to keep a balanced uniform class distribution in both train and test sets. We create the train and test set by taking the first 80 sentences as train and the last 20 sentences as test. We then concatenate the subjective and objective sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subj_docs = subj_docs[:80]\n",
    "test_subj_docs = subj_docs[80:100]\n",
    "train_obj_docs = obj_docs[:80]\n",
    "test_obj_docs = obj_docs[80:100]\n",
    "training_docs = train_subj_docs+train_obj_docs\n",
    "testing_docs = test_subj_docs+test_obj_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initialize a SentimentAnalyser and use a mark_negation function for negative words. mark_negationis a utility function that marks words that are negations that can switch the polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple unigram word features are then used, handling negation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "len(unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'the', ',', 'a', 'and', 'of', 'to', 'is', 'in', 'with']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first 10\n",
    "unigram_feats[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, features are applied to obtain a feature-value representation of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the feature presentation of the test_set. Do you understand what it represents? Why are so many features False?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'contains(.)': True,\n",
       "  'contains(the)': True,\n",
       "  'contains(,)': False,\n",
       "  'contains(a)': True,\n",
       "  'contains(and)': False,\n",
       "  'contains(of)': True,\n",
       "  'contains(to)': False,\n",
       "  'contains(is)': False,\n",
       "  'contains(in)': False,\n",
       "  'contains(with)': True,\n",
       "  'contains(it)': False,\n",
       "  'contains(that)': False,\n",
       "  'contains(his)': False,\n",
       "  'contains(on)': False,\n",
       "  'contains(for)': True,\n",
       "  'contains(an)': False,\n",
       "  'contains(who)': False,\n",
       "  'contains(by)': False,\n",
       "  'contains(he)': False,\n",
       "  'contains(from)': False,\n",
       "  'contains(her)': False,\n",
       "  'contains(\")': False,\n",
       "  'contains(film)': False,\n",
       "  'contains(as)': False,\n",
       "  'contains(this)': False,\n",
       "  'contains(movie)': False,\n",
       "  'contains(their)': False,\n",
       "  'contains(but)': False,\n",
       "  'contains(one)': False,\n",
       "  'contains(at)': False,\n",
       "  'contains(about)': False,\n",
       "  'contains(the_NEG)': False,\n",
       "  'contains(a_NEG)': False,\n",
       "  'contains(to_NEG)': False,\n",
       "  'contains(are)': False,\n",
       "  \"contains(there's)\": False,\n",
       "  'contains(()': False,\n",
       "  'contains(story)': False,\n",
       "  'contains(when)': False,\n",
       "  'contains(so)': False,\n",
       "  'contains(be)': False,\n",
       "  'contains(,_NEG)': False,\n",
       "  'contains())': False,\n",
       "  'contains(they)': False,\n",
       "  'contains(you)': False,\n",
       "  'contains(not)': False,\n",
       "  'contains(have)': False,\n",
       "  'contains(like)': False,\n",
       "  'contains(will)': False,\n",
       "  'contains(all)': False,\n",
       "  'contains(into)': False,\n",
       "  'contains(out)': False,\n",
       "  'contains(she)': False,\n",
       "  'contains(what)': False,\n",
       "  'contains(life)': False,\n",
       "  'contains(has)': False,\n",
       "  'contains(its)': False,\n",
       "  'contains(only)': False,\n",
       "  'contains(more)': False,\n",
       "  'contains(even)': False,\n",
       "  'contains(--)': False,\n",
       "  'contains(:)': False,\n",
       "  'contains(can)': False,\n",
       "  'contains(;)': False,\n",
       "  'contains(home)': False,\n",
       "  'contains(look)': False,\n",
       "  \"contains(it's)\": False,\n",
       "  'contains(if)': False,\n",
       "  'contains(where)': False,\n",
       "  'contains(most)': False,\n",
       "  'contains(him)': False,\n",
       "  'contains(search)': False,\n",
       "  'contains(but_NEG)': False,\n",
       "  'contains(love)': False,\n",
       "  'contains(both)': False,\n",
       "  'contains(make)': False,\n",
       "  'contains(begins)': False,\n",
       "  'contains(some)': False,\n",
       "  'contains(two)': False,\n",
       "  'contains(of_NEG)': False,\n",
       "  'contains(made)': False,\n",
       "  'contains(which)': False,\n",
       "  'contains(them)': False},\n",
       " 'subj')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first\n",
    "test_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we are ready to train our classifier on the training set, and output the evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.8\n",
      "F-measure [obj]: 0.8\n",
      "F-measure [subj]: 0.8\n",
      "Precision [obj]: 0.8\n",
      "Precision [subj]: 0.8\n",
      "Recall [obj]: 0.8\n",
      "Recall [subj]: 0.8\n"
     ]
    }
   ],
   "source": [
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "# output: Training classifier\n",
    "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "    print('{0}: {1}'.format(key, value))\n",
    "#Outputs:\n",
    "#Evaluating NaiveBayesClassifier results...\n",
    "#Accuracy: 0.8\n",
    "#F-measure [obj]: 0.8\n",
    "#F-measure [subj]: 0.8\n",
    "#Precision [obj]: 0.8\n",
    "#Precision [subj]: 0.8\n",
    "#Recall [obj]: 0.8\n",
    "#Recall [subj]: 0.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
