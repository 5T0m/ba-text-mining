{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2 - Sentiment analysis using NLTK\n",
    "\n",
    "In this notebook, we show you how to create a Sentiment classifier from movie reviews provided in NLTK.\n",
    "\n",
    "**at the end of this notebook, you will be able to**:\n",
    "* inspect the training data, i.e., the movie reviews\n",
    "* extracting features from training data\n",
    "* training and evaluating the *NaiveBayesClassifier*\n",
    "* apply the classifier\n",
    "* train the *NaiveBayesClassifier* on your own data\n",
    "\n",
    "**If you want to learn more, you might find the following links useful:**\n",
    "* http://www.nltk.org/api/nltk.sentiment.html#module-nltk.sentiment.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "import nltk\n",
    "from nltk import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the [movie reviews](http://www.cs.cornell.edu/people/pabo/movie-review-data/) ([README](http://www.cs.cornell.edu/people/pabo/movie-review-data/poldata.README.2.0.txt)) dataset.\n",
    "We are going to inspect the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which sentiment categories are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "categories = movie_reviews.categories()\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are just two. A review is marked either **positive** or **negative**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many positive and negative reviews are there in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative reviews 1000\n",
      "number of positive reviews 1000\n"
     ]
    }
   ],
   "source": [
    "print('number of negative reviews', len(negids))\n",
    "print('number of positive reviews', len(posids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the dataset is balanced, meaning that each category has the same amount of training instances!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at an example of a positive and a negative movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random negative file id neg/cv335_16299.txt\n",
      "first 30 words of review ['the', 'most', 'interesting', 'thing', 'about', 'virus', 'is', 'that', 'the', 'title', 'of', 'the', 'film', 'does', 'not', 'refer', 'to', 'the', 'clunky', 'robotic', 'animals', 'that', 'try', 'to', 'kill', 'our', 'heroes', '.', 'alas', ',']\n"
     ]
    }
   ],
   "source": [
    "random_negid = choice(negids)\n",
    "negative_example = movie_reviews.words(fileids=[random_negid])\n",
    "print('random negative file id', random_negid)\n",
    "print('first 30 words of review', negative_example[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random negative file id pos/cv908_16009.txt\n",
      "first 30 words of review ['i', 'actually', 'am', 'a', 'fan', 'of', 'the', 'original', '1961', 'or', 'so', 'live', '-', 'action', '-', 'disney', 'flick', 'of', 'the', 'same', 'name', 'starring', 'hayley', 'mills', 'twice', 'as', 'a', 'pair', 'of', 'twins']\n"
     ]
    }
   ],
   "source": [
    "random_posid = choice(posids)\n",
    "positive_example = movie_reviews.words(fileids=[random_posid])\n",
    "print('random negative file id', random_posid)\n",
    "print('first 30 words of review', positive_example[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from training data\n",
    "We show how to train the classifier using the simplest feature: the words.\n",
    "Our feature representation will be a dictionary, for which we use the following function.\n",
    "Obviously, more complex features are possible, but for now, we focus on a simple feature for the sake of clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    return {word: True for word in words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each movie review, we are going to extract its features, e.g., its words. Together the words form a Bag of Words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "negfeats = []\n",
    "label = 'neg'\n",
    "for neg_fileid in negids:\n",
    "    features = word_feats(movie_reviews.words(fileids=[neg_fileid]))\n",
    "    negfeats.append((features, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "posfeats = []\n",
    "label = 'pos'\n",
    "for pos_fileid in posids:\n",
    "    features = word_feats(movie_reviews.words(fileids=[pos_fileid]))\n",
    "    posfeats.append((features, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect a training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_negfeat = negfeats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(example_negfeat), len(example_negfeat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it's a tuple of length 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element in the tuple is a dictionary containing all the words from the movie review.\n",
    "The second element is the sentiment category annotated for the movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 10\n",
    "features, label = example_negfeat\n",
    "print('label', label)\n",
    "print('features', type(features))\n",
    "print()\n",
    "for index, (word, boolean) in enumerate(features.items()):\n",
    "    print(word, boolean)\n",
    "    if index == counter:\n",
    "        break\n",
    "    index += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Would you include all of the features as shown above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train, we need a training part and a test part. \n",
    "We will use 80% of both the positive and negative reviews for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_training = 0.8\n",
    "perc_test = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negcutoff = int(len(negfeats) * 0.8)\n",
    "poscutoff = int(len(posfeats) * 0.8)\n",
    "\n",
    "print(negcutoff)\n",
    "print(poscutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have **inspected** the data, **extracted features** from it, and split the data into training and test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating classifier\n",
    "Most of the work has been done. Now you just call the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "movie_review_classifier = NaiveBayesClassifier.train(trainfeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has a method to evaluate the performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy:', nltk.classify.util.accuracy(movie_review_classifier, testfeats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only words as features already yields a high accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have some more insight in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_review_classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying our sentiment analyzer to unseen text\n",
    "Our classifier makes a prediction for each word in a sentence. Here is a code snippet to show you how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsentence = \"Awesome eggs, I do not liked them\"\n",
    "words = nltk.word_tokenize(testsentence)\n",
    "predicted_class = movie_review_classifier.classify(word_feats(words))\n",
    "print('testsentence', predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below we show how you can train and test with your own data in a simple way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# simple function that turns a list of words into word_feats (word features)\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "# In a lexical approach, you would predefine the positive, negative and neutral words and only use these to train a classifier\n",
    "positive_vocab = ['awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)']\n",
    "negative_vocab = ['bad', 'terrible','useless', 'hate', ':(']\n",
    "neutral_vocab = ['movie','the','sound','was','is','actors','did','know','words','not']\n",
    "\n",
    "# Assume you have a collections of texts that are negative and neutral\n",
    "negsentence = \"I do not like green eggs and ham, and I do not like them too!\"\n",
    "possentence = \"I like green eggs and ham, and I like them too!\"\n",
    "neusentence = \"It exists and it is, there why would it be?\"\n",
    "\n",
    "# By using the tokenization function, you can turn them into word negative and positive lists\n",
    "negtokens = nltk.word_tokenize(negsentence)\n",
    "postokens = nltk.word_tokenize(possentence)\n",
    "neutokens = nltk.word_tokenize(neusentence)\n",
    "\n",
    "# Next we use the simple word feature function to turn them into features that can be used for training the classifier \n",
    "positive_features = []\n",
    "negative_features = []\n",
    "neutral_features = []\n",
    "\n",
    "for information in [positive_vocab, postokens]:\n",
    "    positive_features.append((word_feats(information), 'pos'))\n",
    "    \n",
    "for information in [negative_vocab, negtokens]:\n",
    "    negative_features.append((word_feats(information), 'neg'))\n",
    "\n",
    "for information in [neutral_vocab, neutokens]:   \n",
    "    neutral_features.append((word_feats(information), 'neu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What would be another way to obtain neutral word features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How would you do this for a data set where positive and negative texts are stored in two separate directories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_training = 0.8\n",
    "perc_test = 0.2\n",
    "\n",
    "training = []\n",
    "test = []\n",
    "\n",
    "for feature_set in [negative_features, neutral_features, positive_features]:\n",
    "    num_items = len(feature_set)\n",
    "    cutoff = int(num_items * perc_training)\n",
    "    training_part = feature_set[:cutoff]\n",
    "    test_part = feature_set[cutoff:]\n",
    "    print(num_items, len(training_part), len(test_part))\n",
    "    \n",
    "    training.extend(training_part)\n",
    "    test.extend(test_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy:', nltk.classify.util.accuracy(classifier, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsentence = \"Awesome eggs, I do not liked them\"\n",
    "words = nltk.word_tokenize(testsentence)\n",
    "predicted_class = classifier.classify(word_feats(words))\n",
    "print('testsentence', predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
