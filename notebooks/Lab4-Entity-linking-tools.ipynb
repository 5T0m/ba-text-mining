{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disambiguating with existing tools\n",
    "\n",
    "There are many existing entity linking tools out there. We will not build our own, but instead run two existing tools and dig deeper into their output. In order to do this, we will do the following steps:\n",
    "1. Setup your environment (installation of modules and download of dataset)\n",
    "2. Load the dataset\n",
    "3. Run disambiguation with our tools on top of recognized mentions\n",
    "4. Run entity annotation from scratch: recognition and disambiguation together\n",
    "\n",
    "### 1. Setup your environment\n",
    "\n",
    "#### 1.1 Install the needed modules\n",
    "\n",
    "For the purpose of this week's coding exercises, we will need some new libraries that are probably not installed on your computer:\n",
    "* [rdflib](https://pypi.org/project/rdflib/)\n",
    "* [agdistispy](https://pypi.org/project/agdistispy/)\n",
    "* [pyspotlight](https://pypi.org/project/pyspotlight/)\n",
    "* [lxml](https://pypi.org/project/lxml/)\n",
    "\n",
    "You should install these libraries using `conda` or `pip`.\n",
    "\n",
    "Let's now check if all needed libraries are installed on your computer and can be imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef\n",
    "import urllib\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import requests\n",
    "import urllib.parse\n",
    "import xml.etree.cElementTree as ET\n",
    "from lxml import etree\n",
    "import time\n",
    "\n",
    "from agdistispy.agdistis import Agdistis\n",
    "import spotlight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you see any errors with the imports:**\n",
    "* read the error carefully\n",
    "* install the library that is missing\n",
    "* try to execute the imports again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Let's download the N3 Reuters-128 dataset\n",
    "\n",
    "We will work with a small dataset called Reuters-128. This dataset contains 128 Reuters documents annotated with entity mentions and links to DBpedia. \n",
    "You can download this dataset from Canvas or from https://raw.githubusercontent.com/dice-group/n3-collection/master/Reuters-128.ttl.\n",
    "\n",
    "Store the dataset file 'Reuters-128.ttl' in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Before proceding, please verify that your setup of libraries (1.1) is correct and that the dataset is downloaded in the right location (1.2).*\n",
    "\n",
    "#### 2. Load the data from N3\n",
    "\n",
    "**Let's now parse this dataset to a list of news items objects that contain the text and the entity mentions for each news item.**\n",
    "\n",
    "The data is in a .ttl format called NIF (don't worry about these formats for now, we provide a function to parse this dataset to python classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_file='Reuters-128.ttl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsItem:\n",
    "    \"\"\"\n",
    "    class containing information about a news item\n",
    "    \"\"\"\n",
    "    def __init__(self, identifier, content=\"\",\n",
    "                 dct=None):\n",
    "        self.identifier = identifier  # string, the original document name in the dataset\n",
    "        self.dct = dct                # e.g. \"2005-05-14T02:00:00.000+02:00\" -> document creation time\n",
    "        self.content = content        # the text of the news article\n",
    "        self.entity_mentions = []  # set of instances of EntityMention class\n",
    "        \n",
    "class EntityMention:\n",
    "    \"\"\"\n",
    "    class containing information about an entity mention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mention,\n",
    "                 begin_index, end_index,\n",
    "                 gold_link=None,\n",
    "                 the_type=None, sentence=None, agdistis_link=None,\n",
    "                 spotlight_link=None): #, exact_match=False):\n",
    "        self.sentence = sentence         # e.g. 4 -> which sentence is the entity mentioned in\n",
    "        self.mention = mention           # e.g. \"John Smith\" -> the mention of an entity as found in text\n",
    "        self.the_type = the_type         # e.g. \"Person\" | \"http://dbpedia.org/ontology/Person\"\n",
    "        self.begin_index = begin_index   # e.g. 15 -> begin offset\n",
    "        self.end_index = end_index       # e.g. 25 -> end offset\n",
    "        self.gold_link = gold_link       # gold link if existing\n",
    "        self.agdistis_link = agdistis_link    # AGDISTIS link\n",
    "        self.spotlight_link = spotlight_link             # Spotlight link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeURL(s):\n",
    "    \"\"\"\n",
    "    Normalize a URI by removing its Wikipedia/DBpedia prefix.\n",
    "    \"\"\"\n",
    "    if s:\n",
    "        if s.startswith('http://aksw.org/notInWiki'):\n",
    "            return 'NIL'\n",
    "        else:\n",
    "            return urllib.parse.unquote(s.replace(\"http://en.wikipedia.org/wiki/\", \"\").\n",
    "                                        replace(\"http://dbpedia.org/resource/\", \"\"). \n",
    "                                        replace(\"http://dbpedia.org/page/\", \"\").\n",
    "                                        strip().\n",
    "                                        strip('\"'))\n",
    "    else:\n",
    "        return 'NIL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_article_from_nif_file(nif_file):\n",
    "    \"\"\"\n",
    "    Load a dataset in NIF format.\n",
    "    \"\"\"\n",
    "    g=Graph()\n",
    "    g.parse(nif_file, format=\"n3\")\n",
    "\n",
    "    news_items=[]\n",
    "\n",
    "    articles = g.query(\n",
    "    \"\"\" SELECT ?articleid ?date ?string\n",
    "    WHERE {\n",
    "        ?articleid nif:isString ?string .\n",
    "        OPTIONAL { ?articleid <http://purl.org/dc/elements/1.1/date> ?date . }\n",
    "    }\n",
    "    \"\"\")\n",
    "    for article in articles:\n",
    "        news_item_obj=NewsItem(\n",
    "            content=article['string'],\n",
    "            identifier=article['articleid'], \n",
    "            dct=article['date']\n",
    "        )\n",
    "        query=\"\"\" SELECT ?id ?mention ?start ?end ?gold\n",
    "        WHERE {\n",
    "            ?id nif:anchorOf ?mention ;\n",
    "            nif:beginIndex ?start ;\n",
    "            nif:endIndex ?end ;\n",
    "            nif:referenceContext <%s> .\n",
    "            OPTIONAL { ?id itsrdf:taIdentRef ?gold . }\n",
    "        } ORDER BY ?start\"\"\" % str(article['articleid'])\n",
    "        qres_entities = g.query(query)\n",
    "        for entity in qres_entities:\n",
    "            gold_link=normalizeURL(str(entity['gold']))\n",
    "            if gold_link.startswith('http://aksw.org/notInWiki'):\n",
    "                gold_link='NIL'\n",
    "            entity_obj = EntityMention(\n",
    "                begin_index=int(entity['start']),\n",
    "                end_index=int(entity['end']),\n",
    "                mention=str(entity['mention']),\n",
    "                gold_link=gold_link\n",
    "            )\n",
    "            news_item_obj.entity_mentions.append(entity_obj)\n",
    "        news_items.append(news_item_obj)\n",
    "    return news_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=load_article_from_nif_file(reuters_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Disambiguation with existing entity linking tools\n",
    "\n",
    "Then we will parse this dataset with two modern tools, called AGDISTIS and DBpedia Spotlight.\n",
    "\n",
    "#### 3.1 AGDISTIS\n",
    "\n",
    "AGDISTIS (also called Multilingual AGDISTIS, or MAG) is an entity linking system that puts all entity candidates in a graph network and then performs a probabilistic optimization to find the best connected candidate in this graph for each of the entity mentions. \n",
    "\n",
    "More description can be found in their paper: https://arxiv.org/pdf/1707.05288.pdf\n",
    "\n",
    "You can play with AGDISTIS by using their demo page: http://agdistis.aksw.org/demo/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = Agdistis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agdistis_disambiguation(articles):\n",
    "    \"\"\"\n",
    "    Perform disambiguation with AGDISTIS.\n",
    "    \"\"\"\n",
    "    with tqdm(total=len(articles), file=sys.stdout) as pbar:\n",
    "        for i, article in enumerate(articles):\n",
    "                                    \n",
    "            # AGDISTIS expects entity mentions that are pre-marked inside text. \n",
    "            # For example, the sentence \"Obama visited Paris today\", \n",
    "            # should be transformed to \"<entity>Obama</entity> visited <entity>Paris</entity> today.\"\n",
    "            # We do this in the next 5 lines of code.\n",
    "            original_content = article.content\n",
    "            new_content=original_content\n",
    "            for entity in reversed(article.entity_mentions):\n",
    "                entity_span=new_content[entity.begin_index: entity.end_index]\n",
    "                new_content=new_content[:entity.begin_index] + '<entity>' + entity_span + '</entity>' + new_content[entity.end_index:]\n",
    "\n",
    "            # Now, we can run the AGDISTIS library with this string.\n",
    "            results = ag.disambiguate(new_content)\n",
    "            \n",
    "            # Let's normalize the disambiguated entiies.\n",
    "            # This means mostly removing the first part of the URI which is always the same (http://dbpedia.org/resource)\n",
    "            # and leaving only the entity identification part (e.g., Barack_Obama).\n",
    "            dis_entities={}\n",
    "            for dis_entity in results:\n",
    "                dis_entities[str(dis_entity['start'])] = normalizeURL(dis_entity['disambiguatedURL'])\n",
    "                \n",
    "            # We can now store the entity to our class instance for later processing.\n",
    "            for entity in article.entity_mentions:\n",
    "                start = entity.begin_index\n",
    "                dis_url = dis_entities[str(start)]\n",
    "                entity.agdistis_link = dis_url\n",
    "\n",
    "            # The next two lines only update the progress bar\n",
    "            pbar.set_description('processed: %d' % (1 + i))\n",
    "            pbar.update(1)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_agdistis=agdistis_disambiguation(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 DBpedia Spotlight\n",
    "\n",
    "DBpedia Spotlight is an entity recognition and linking tool that performs linking to DBpedia. The core of their method is a vector space model to compute similarity between the text to annotate and the Wikipedia pages of all entity candidates for a mention. Then the entities with largest similarity are chosen.\n",
    "\n",
    "Here is their paper: http://oa.upm.es/8923/1/DBpedia_Spotlight.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spotlight_disambiguate(articles, spotlight_url):\n",
    "    \"\"\"\n",
    "    Perform disambiguation with DBpedia Spotlight.\n",
    "    \"\"\"\n",
    "    with tqdm(total=len(articles), file=sys.stdout) as pbar:\n",
    "        for i, article in enumerate(articles):\n",
    "            # Similar as with AGDISTIS, we first prepare the document text and the mentions\n",
    "            # in order to provide these to Spotlight as input.\n",
    "            annotation = etree.Element(\"annotation\", text=article.content)\n",
    "            for mention in article.entity_mentions:\n",
    "                sf = etree.SubElement(annotation, \"surfaceForm\")\n",
    "                sf.set(\"name\", mention.mention)\n",
    "                sf.set(\"offset\", str(mention.begin_index))\n",
    "            my_xml=etree.tostring(annotation, xml_declaration=True, encoding='UTF-8')\n",
    "            \n",
    "            # Send a disambiguation request to spotlight\n",
    "            results=requests.post(spotlight_url, urllib.parse.urlencode({'text':my_xml, 'confidence': 0.5}), \n",
    "                                  headers={'Accept': 'application/json'})\n",
    "            \n",
    "            # Process the results and normalize the entity URIs\n",
    "            j=results.json()\n",
    "            dis_entities={}\n",
    "            if 'Resources' in j: \n",
    "                resources=j['Resources']\n",
    "            else: \n",
    "                resources=[]\n",
    "            for dis_entity in resources:\n",
    "                dis_entities[str(dis_entity['@offset'])] = normalizeURL(dis_entity['@URI'])\n",
    "            \n",
    "            # Let's now store the URLs by Spotlight to our class for later analysis.\n",
    "            for entity in article.entity_mentions:\n",
    "                start = entity.begin_index\n",
    "                if str(start) in dis_entities:\n",
    "                    dis_url = dis_entities[str(start)]\n",
    "                else:\n",
    "                    dis_url = 'NIL'\n",
    "                entity.spotlight_link = dis_url\n",
    "    \n",
    "            # The next two lines only update the progress bar\n",
    "            pbar.set_description('processed: %d' % (1 + i))\n",
    "            pbar.update(1)\n",
    "                \n",
    "            # Pause for 100ms to prevent overloading the server\n",
    "            time.sleep(0.1)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spotlight_url=\"http://model.dbpedia-spotlight.org/en/disambiguate\" # Uses data from February 2018\n",
    "spotlight_url=\"http://spotlight.fii800.lod.labs.vu.nl/rest/disambiguate\" # Uses data from April 2016 (same as AGDISTIS)\n",
    "\n",
    "processed_spotlight=spotlight_disambiguate(processed_agdistis, spotlight_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Comparing the output of AGDISTIS and Spotlight to the gold links\n",
    "\n",
    "Let's pick an article and print the decisions made by AGDISTIS and Spotlight on that article, and then compare that to the gold link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_number=3\n",
    "\n",
    "for m in articles[article_number].entity_mentions:\n",
    "    print('%s\\t%s\\t%s' % (m.gold_link, m.agdistis_link, m.spotlight_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Entity annotation from scratch: Performing recognition and disambiguation together\n",
    "\n",
    "Some tools only perform disambiguation (AGDISTIS is an example), whereas others (like Spotlight) can perform both recognition and disambiguation.\n",
    "\n",
    "Here we will use Spotlight annotate some text with entities without prior marking of entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "On November 24, Aziz Karimov, a journalist based in Baku, received an email from Facebook notifying him of a request to reset his password. \n",
    "Karimov knew something was wrong since he hadn’t requested a password change. \n",
    "Ninety minutes later, as he struggled to regain access to his account, he received four more notifications from Facebook. \n",
    "He was informed that he had also been removed as an administrator from four other pages, including one belonging to Turan News Agency, \n",
    "Azerbaijan’s only independent news agency.\n",
    "'''\n",
    "annotations = spotlight.annotate('http://model.dbpedia-spotlight.org/en/annotate',\n",
    "                                  text,\n",
    "                                  confidence=0.5, support=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
