{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disambiguating with existing tools\n",
    "\n",
    "There are many existing entity linking tools out there. We will not build our own, but instead run two existing tools and dig deeper into their output. In order to do this, we will do the following steps:\n",
    "1. Setup your environment (installation of modules and download of dataset)\n",
    "2. Load the dataset\n",
    "3. Run disambiguation with our tools on top of recognized mentions\n",
    "4. Run entity annotation from scratch: recognition and disambiguation together\n",
    "\n",
    "### 1. Setup your environment\n",
    "\n",
    "#### 1.1 Install the needed modules\n",
    "\n",
    "For the purpose of this week's coding exercises, we will need some new libraries that are probably not installed on your computer:\n",
    "* [rdflib](https://pypi.org/project/rdflib/)\n",
    "* [pyspotlight](https://pypi.org/project/pyspotlight/)\n",
    "* (perhaps) [lxml](https://pypi.org/project/lxml/)\n",
    "\n",
    "You should install these libraries using `conda` or `pip`.\n",
    "\n",
    "Let's now check if all needed libraries are installed on your computer and can be imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import requests\n",
    "import urllib\n",
    "import urllib.parse\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.parse import urlencode\n",
    "import xml.etree.cElementTree as ET\n",
    "from lxml import etree\n",
    "import time\n",
    "import json\n",
    "\n",
    "import spotlight\n",
    "\n",
    "import lab4_utils as utils\n",
    "import lab4_classes as classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you see any errors with the imports:**\n",
    "* read the error carefully\n",
    "* install the library that is missing\n",
    "* try to execute the imports again\n",
    "\n",
    "Don't proceed before the imports work; we will use this modules in the explanation below and in the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Obtain the N3 Reuters-128 dataset\n",
    "\n",
    "We will work with a small dataset called Reuters-128. This dataset contains 128 Reuters documents annotated with entity mentions and links to DBpedia. \n",
    "You can download this dataset from Canvas or from https://raw.githubusercontent.com/dice-group/n3-collection/master/Reuters-128.ttl.\n",
    "\n",
    "Store the dataset file 'Reuters-128.ttl' in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Before proceding, please verify that your setup of libraries (1.1) is correct and that the dataset is downloaded in the right location (1.2).*\n",
    "\n",
    "Congratulations, you are all set up!\n",
    "\n",
    "### 2. Load the data from N3\n",
    "\n",
    "**Let's now parse this dataset to a list of news items objects that contain: 1) the text and 2) the entity mentions+links for each news item.**\n",
    "\n",
    "The data is in a .ttl format called NIF (don't worry about these formats for now, we provide a function to parse this dataset to python classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_file='Reuters-128.ttl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=utils.load_article_from_nif_file(reuters_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data description**\n",
    "\n",
    "`articles` is a list of news articles (members of the `NewsItem` which we define ourselves). To help you understand, we now present the information that we store for each news article. \n",
    "\n",
    "Each news item contains the following fields: `identifier` (the original identifier of this document in the dataset), `dct` (the document creation time, if known), `content` (the text of this document), and `entity_mentions` (a list of entity mention occurrences that were found in this document).\n",
    "\n",
    "For each of the entity mentions, we store then the following information: `sentence` (in which sentence was this mention found), `mention` (the exact phrase of this mention), `the_type` (its type, if known), `begin_index` (the starting offset of this mention), `end_index` (the ending offset of this mention), `gold_link` (the gold link for this mention), `agdistis_link` (the link for this mention proposed by Agdistis), and `spotlight_link` (the link for this mention proposed by Spotlight)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Disambiguation with existing entity linking tools\n",
    "\n",
    "We will perform entity linking on this dataset with two modern tools, called AGDISTIS and DBpedia Spotlight.\n",
    "\n",
    "#### 3.1 AGDISTIS\n",
    "\n",
    "AGDISTIS (also called Multilingual AGDISTIS, or MAG) is an entity linking system that puts all entity candidates in a graph network and then performs a probabilistic optimization to find the best connected candidate in this graph for each of the entity mentions. \n",
    "\n",
    "More description can be found in their paper: https://arxiv.org/pdf/1707.05288.pdf\n",
    "\n",
    "You can play with AGDISTIS by using their demo page: http://agdistis.aksw.org/demo/.\n",
    "\n",
    "In the function `agdistis_disambiguation` below, we do the following: \n",
    "* we iterate the 128 documents of the Reuters-128 dataset (called `articles`)\n",
    "* for each document, we combine the text and the marking of the entity mentions in a single string `new_content`\n",
    "* we send a request to Agdistis to disambiguate the entities in this string\n",
    "* we store the entity links back to the `articles` in the field `agdistis_link`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agdistis_disambiguation(articles, agdistis_url):\n",
    "    \"\"\"\n",
    "    Perform disambiguation with AGDISTIS.\n",
    "    \"\"\"\n",
    "    with tqdm(total=len(articles), file=sys.stdout) as pbar:\n",
    "        for i, article in enumerate(articles):\n",
    "                                    \n",
    "            # AGDISTIS expects entity mentions that are pre-marked inside text. \n",
    "            # For example, the sentence \"Obama visited Paris today\", \n",
    "            # should be transformed to \"<entity>Obama</entity> visited <entity>Paris</entity> today.\"\n",
    "            # We do this in the next 5 lines of code.\n",
    "            original_content = article.content\n",
    "            new_content=original_content\n",
    "            for entity in reversed(article.entity_mentions):\n",
    "                entity_span=new_content[entity.begin_index: entity.end_index]\n",
    "                new_content=new_content[:entity.begin_index] + '<entity>' + entity_span + '</entity>' + new_content[entity.end_index:]\n",
    "\n",
    "            # Now, we can run the AGDISTIS library with this string.\n",
    "            #results=ag.disambiguate(new_content)\n",
    "            params={\"text\": new_content, \"type\": 'agdistis'}\n",
    "            request = Request(agdistis_url, urlencode(params).encode())\n",
    "            this_json = urlopen(request).read().decode()\n",
    "            results=json.loads(this_json)\n",
    "            \n",
    "            # Let's normalize the disambiguated enties.\n",
    "            # This means mostly removing the first part of the URI which is always the same (http://dbpedia.org/resource)\n",
    "            # and leaving only the entity identification part (e.g., Barack_Obama).\n",
    "            dis_entities={}\n",
    "            for dis_entity in results:\n",
    "                dis_entities[str(dis_entity['start'])] = utils.normalizeURL(dis_entity['disambiguatedURL'])\n",
    "                \n",
    "            # We can now store the entity to our class instance for later processing.\n",
    "            for entity in article.entity_mentions:\n",
    "                start = entity.begin_index\n",
    "                dis_url = dis_entities[str(start)]\n",
    "                entity.agdistis_link = dis_url\n",
    "\n",
    "            # The next two lines only update the progress bar\n",
    "            pbar.set_description('processed: %d' % (1 + i))\n",
    "            pbar.update(1)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 128: 100%|██████████| 128/128 [02:42<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "agdistis_url = \"http://akswnc9.informatik.uni-leipzig.de:8113/AGDISTIS\"\n",
    "\n",
    "processed_agdistis=agdistis_disambiguation(articles, agdistis_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 DBpedia Spotlight\n",
    "\n",
    "DBpedia Spotlight (or only \"Spotlight\" for brevity) is an entity recognition and linking tool that performs linking to DBpedia. The core of their method is a vector space model to compute similarity between the text to annotate and the Wikipedia pages of all entity candidates for a mention. Then the entities with largest similarity are chosen.\n",
    "\n",
    "Here is their paper: http://oa.upm.es/8923/1/DBpedia_Spotlight.pdf\n",
    "\n",
    "You can try out their demo as well: http://dbpedia-spotlight.github.com/demo/.\n",
    "\n",
    "In the function `spotlight_disambiguation` below, we do the following: \n",
    "* we iterate the 128 documents of the Reuters-128 dataset (called `articles`)\n",
    "* for each document, we combine the text and the marking of the entity mentions in a single string `new_content`\n",
    "* we send a request to Spotlight to disambiguate the entities in this string\n",
    "* we store the entity links back to the `articles` in the field `spotlight_link`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spotlight_disambiguate(articles, spotlight_url):\n",
    "    \"\"\"\n",
    "    Perform disambiguation with DBpedia Spotlight.\n",
    "    \"\"\"\n",
    "    with tqdm(total=len(articles), file=sys.stdout) as pbar:\n",
    "        for i, article in enumerate(articles):\n",
    "            # Similar as with AGDISTIS, we first prepare the document text and the mentions\n",
    "            # in order to provide these to Spotlight as input.\n",
    "            annotation = etree.Element(\"annotation\", text=article.content)\n",
    "            for mention in article.entity_mentions:\n",
    "                sf = etree.SubElement(annotation, \"surfaceForm\")\n",
    "                sf.set(\"name\", mention.mention)\n",
    "                sf.set(\"offset\", str(mention.begin_index))\n",
    "            my_xml=etree.tostring(annotation, xml_declaration=True, encoding='UTF-8')\n",
    "            \n",
    "            # Send a disambiguation request to spotlight\n",
    "            results=requests.post(spotlight_url, urllib.parse.urlencode({'text':my_xml, 'confidence': 0.5}), \n",
    "                                  headers={'Accept': 'application/json'})\n",
    "            \n",
    "            # Process the results and normalize the entity URIs\n",
    "            j=results.json()\n",
    "            dis_entities={}\n",
    "            if 'Resources' in j: \n",
    "                resources=j['Resources']\n",
    "            else: \n",
    "                resources=[]\n",
    "            for dis_entity in resources:\n",
    "                dis_entities[str(dis_entity['@offset'])] = utils.normalizeURL(dis_entity['@URI'])\n",
    "            \n",
    "            # Let's now store the URLs by Spotlight to our class for later analysis.\n",
    "            for entity in article.entity_mentions:\n",
    "                start = entity.begin_index\n",
    "                if str(start) in dis_entities:\n",
    "                    dis_url = dis_entities[str(start)]\n",
    "                else:\n",
    "                    dis_url = 'NIL'\n",
    "                entity.spotlight_link = dis_url\n",
    "    \n",
    "            # The next two lines only update the progress bar\n",
    "            pbar.set_description('processed: %d' % (1 + i))\n",
    "            pbar.update(1)\n",
    "                \n",
    "            # Pause for 100ms to prevent overloading the server\n",
    "            time.sleep(0.1)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 128: 100%|██████████| 128/128 [00:19<00:00,  6.64it/s]\n"
     ]
    }
   ],
   "source": [
    "#spotlight_url=\"http://model.dbpedia-spotlight.org/en/disambiguate\" # Uses data from February 2018\n",
    "spotlight_url=\"http://spotlight.fii800.lod.labs.vu.nl/rest/disambiguate\" # Uses data from April 2016 (same as AGDISTIS)\n",
    "\n",
    "processed_both=spotlight_disambiguate(processed_agdistis, spotlight_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Comparing the output of AGDISTIS and Spotlight to the gold links\n",
    "\n",
    "Because we stored the decisions by both tools in our list `articles`, we can now compare their output.\n",
    "\n",
    "Let's pick an article and print the decisions made by AGDISTIS and Spotlight on that article, and then compare that to the gold link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://aksw.org/N3/Reuters-128/16#char=0,409\n",
      "Toronto Dominion Bank\tToronto-Dominion_Bank\tToronto-Dominion_Bank\tToronto-Dominion_Bank\n",
      "Hambros Bank Ltd\tHambros_Bank\tHambros_Bank\tNIL\n",
      "London\tLondon\tLondon\tLondon\n"
     ]
    }
   ],
   "source": [
    "article_number=3\n",
    "\n",
    "an_article=processed_both[article_number]\n",
    "doc_id=an_article.identifier\n",
    "print(doc_id)\n",
    "for m in an_article.entity_mentions:\n",
    "    print('%s\\t%s\\t%s\\t%s' % (m.mention, m.gold_link, m.agdistis_link, m.spotlight_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Entity annotation from scratch: Performing recognition and disambiguation together\n",
    "\n",
    "Some tools only perform disambiguation (AGDISTIS is an example), whereas others (like Spotlight) can perform both recognition and disambiguation.\n",
    "\n",
    "Here we will use Spotlight annotate some text with entities without prior marking of entities. Notice that now we call the function `annotate` instead of sending a request to `disambiguate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "On November 24, Aziz Karimov, a journalist based in Baku, received an email from Facebook notifying him of a request to reset his password. \n",
    "Karimov knew something was wrong since he hadn’t requested a password change. \n",
    "Ninety minutes later, as he struggled to regain access to his account, he received four more notifications from Facebook. \n",
    "He was informed that he had also been removed as an administrator from four other pages, including one belonging to Turan News Agency, \n",
    "Azerbaijan’s only independent news agency.\n",
    "'''\n",
    "annotations = spotlight.annotate('http://model.dbpedia-spotlight.org/en/annotate',\n",
    "                                  text,\n",
    "                                  confidence=0.5, support=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which entity mentions were recognized by Spotlight and which links were assigned to these entity mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'URI': 'http://dbpedia.org/resource/Islam_Karimov',\n",
       "  'support': 363,\n",
       "  'types': 'Http://xmlns.com/foaf/0.1/Person,Wikidata:Q5,Wikidata:Q24229398,Wikidata:Q215627,DUL:NaturalPerson,DUL:Agent,Schema:Person,DBpedia:Person,DBpedia:OfficeHolder,DBpedia:Agent',\n",
       "  'surfaceForm': 'Karimov',\n",
       "  'offset': 22,\n",
       "  'similarityScore': 0.9999999994710151,\n",
       "  'percentageOfSecondRank': 0.0},\n",
       " {'URI': 'http://dbpedia.org/resource/Baku',\n",
       "  'support': 10476,\n",
       "  'types': 'Wikidata:Q486972,Schema:Place,DBpedia:Settlement,DBpedia:PopulatedPlace,DBpedia:Place,DBpedia:Location',\n",
       "  'surfaceForm': 'Baku',\n",
       "  'offset': 53,\n",
       "  'similarityScore': 0.9998148103451905,\n",
       "  'percentageOfSecondRank': 0.00018500987497263095},\n",
       " {'URI': 'http://dbpedia.org/resource/Facebook',\n",
       "  'support': 24873,\n",
       "  'types': 'Wikidata:Q43229,Wikidata:Q24229398,DUL:SocialPerson,DUL:Agent,Schema:Organization,DBpedia:Organisation,DBpedia:Company,DBpedia:Agent',\n",
       "  'surfaceForm': 'Facebook',\n",
       "  'offset': 82,\n",
       "  'similarityScore': 0.9999997646980595,\n",
       "  'percentageOfSecondRank': 2.2562008629258093e-07},\n",
       " {'URI': 'http://dbpedia.org/resource/Islam_Karimov',\n",
       "  'support': 363,\n",
       "  'types': 'Http://xmlns.com/foaf/0.1/Person,Wikidata:Q5,Wikidata:Q24229398,Wikidata:Q215627,DUL:NaturalPerson,DUL:Agent,Schema:Person,DBpedia:Person,DBpedia:OfficeHolder,DBpedia:Agent',\n",
       "  'surfaceForm': 'Karimov',\n",
       "  'offset': 142,\n",
       "  'similarityScore': 0.9999999994710151,\n",
       "  'percentageOfSecondRank': 0.0},\n",
       " {'URI': 'http://dbpedia.org/resource/Facebook',\n",
       "  'support': 24873,\n",
       "  'types': 'Wikidata:Q43229,Wikidata:Q24229398,DUL:SocialPerson,DUL:Agent,Schema:Organization,DBpedia:Organisation,DBpedia:Company,DBpedia:Agent',\n",
       "  'surfaceForm': 'Facebook',\n",
       "  'offset': 333,\n",
       "  'similarityScore': 0.9999997646980595,\n",
       "  'percentageOfSecondRank': 2.2562008629258093e-07},\n",
       " {'URI': 'http://dbpedia.org/resource/Turan-Tovuz_IK',\n",
       "  'support': 187,\n",
       "  'types': 'Wikidata:Q476028,Wikidata:Q43229,Wikidata:Q24229398,DUL:SocialPerson,DUL:Agent,Schema:SportsTeam,Schema:Organization,DBpedia:SportsTeam,DBpedia:SoccerClub,DBpedia:Organisation,DBpedia:Agent',\n",
       "  'surfaceForm': 'Turan',\n",
       "  'offset': 460,\n",
       "  'similarityScore': 0.9981186151452393,\n",
       "  'percentageOfSecondRank': 0.001883831524426054},\n",
       " {'URI': 'http://dbpedia.org/resource/Azerbaijan',\n",
       "  'support': 33607,\n",
       "  'types': 'Wikidata:Q6256,Schema:Place,Schema:Country,DBpedia:PopulatedPlace,DBpedia:Place,DBpedia:Location,DBpedia:Country',\n",
       "  'surfaceForm': 'Azerbaijan',\n",
       "  'offset': 480,\n",
       "  'similarityScore': 0.9999320508454828,\n",
       "  'percentageOfSecondRank': 6.167481587304292e-05}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
