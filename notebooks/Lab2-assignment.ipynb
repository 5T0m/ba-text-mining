{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open issues\n",
    "* [Marten]: Filters on what type of tweets?\n",
    "* [Isa]: do they need to research what each sentiment analyzer is doing.\n",
    "* [Marten]: do they need to research td-idf?\n",
    "* [Marten]: do they need to research precision, recall, accuracy, and f1?\n",
    "* [Marten]: Which datasets to propose to train own classifier?\n",
    "\n",
    "\n",
    "# Lab2 - Assignment\n",
    "\n",
    "Due: TODO\n",
    "\n",
    "Please name your ipython notebook with the following naming convention: TODO\n",
    "\n",
    "Please submit your assignment using TODO\n",
    "\n",
    "If you have questions about this topic, TODO\n",
    "\n",
    "This notebook describes the assignment for Lab 2 of the Text Mining course. We assume you have worked through the following notebooks: \n",
    "* **Lab2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab2-Sentiment-analysis.with-NLTK.ipynb**\n",
    "* **Lab2-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "\n",
    "In these notebooks, you have worked with the VADER tool, and trained a Naive Bayes classifier in NLTK and with scikit-learn. \n",
    "\n",
    "You also learned how to create a gold standard and carry out an evaluation with scikit-learn (**Lab2-Sentiment-analysis.with-scikit-learn.ipynb -> Section \"Define your own gold standard and evaluate a system on it\"**).\n",
    "\n",
    "In this assignment, you are going to create your own gold standards set from 50 tweets. You need to apply the VADER, NLTK and scikit-learn classifier to these tweets and evaluate the results using the scikit-learn evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: TODO] Exercise 1: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. Collect for each tweet:\n",
    "* the tweet\n",
    "* the user that tweeted it\n",
    "* date the tweet was posted\n",
    "* indicate whether the tweet is *positive* (1) or *negative* (0).\n",
    "\n",
    "You can store this information in a format such as CSV, TSV, Excel, Json, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: TODO] Exercise 2: Create a gold evaluation set\n",
    "Create a gold standard in the format needed for scikit-learn\n",
    "(**Lab2-Sentiment-analysis.with-scikit-learn.ipynb -> Section \"Define your own gold standard and evaluate a system on it\"**). The list **reviews_new** in the notebook contains your tweets, and **gold_values** contain your annotations (are the tweets positive or negative). **gold** is the gold standard in the format needed in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: TODO]: Perform sentiment analysis using VADER\n",
    "Run the VADER tool on your tweets. The output is a list containing the output of the VADER tool: 0 for negative, 1 for positive. The output of the VADER tool looks like:\n",
    "```\n",
    "{'neg': 0.0, 'neu': 0.714, 'pos': 0.286, 'compound': 0.0516}\n",
    "```\n",
    "Please consider that VADER thinks the tweet is positive if the value for **'pos'** is higher than the value for **'neg'**. If both negative and positive are 0, tag the tweet as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Poins: TODO]: Perform sentiment analysis using NLTK\n",
    "Run the NLTK classifier trained on the movie reviews (see **Lab2-Sentiment-analysis.with-NLTK.ipynb**) on the tweets. The output is a list containing the output of the NLTK classifier: 0 for negative, 1 for positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: TODO] Perform sentiment analysis using scikit-learn\n",
    "Run the scikit-learn classifier (see **Lab2-Sentiment-analysis.with-scikit-learn.ipynb**) on the tweets. The output is a list containing the output of the scikit-learn classifier: 0 for negative, 1 for positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: TODO] evaluate the classifiers\n",
    "Use the scikit-learn evaluation metrics to get the accuracy, recall, precision, and f1 scores for each of the classifiers:\n",
    "* VADER\n",
    "* NLTK trained on movie reviews\n",
    "* scikit-learn classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: TODO] Training your own classifier\n",
    "Next train a classifier (NLTK or Skikit-learn) from another data set. Here are some data sets for sentiment analysis:\n",
    "\n",
    "* https://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "* https://github.com/nltk/nltk/wiki/Sentiment-Analysis\n",
    "\n",
    "Take one of these data sets and train a NaiveBayesClassifier. You need to find a way to obtain the correct training data. You may need to define a loop to read the texts from each file and get the word features from each.\n",
    "\n",
    "* Provide statistics on the data set: how many files per category, how many features. \n",
    "* Divide a split over training and test data. \n",
    "* Train the classifier using the train set and evaluate the classifier on the test set. \n",
    "* Apply this new classifier to the same gold data as above and report again on the evaluation results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
