{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION\n",
    "\n",
    "This notebook describes the LAB-4 assignment of the Text Mining course. It is about Entity linking.\n",
    "\n",
    "**Due**: 10 Mar at 23:59\n",
    "\n",
    "**How to submit**: Please submit your assignment using Canvas (see *Assignments* -> *Lab Session Sentiment*). Convert your notebook to PDF (in JupyterLab, this can be done by clicking on *File* in the menu bar, select *Export Notebook As*, then select *Export Notebook to PDF*)\n",
    "\n",
    "**Points**: each exercise is suffixed with the number of points you can obtain for the exercise.\n",
    "\n",
    "**Questions**: If you have questions about this topic, you can either ask them during the lab session or email:\n",
    "* Filip Ilievski (f.ilievski@vu.nl)\n",
    "* Robert Kajnak: (r.kajnakmisca@student.vu.nl)\n",
    "* Karen Goes: (k.w.m.goes@student.vu.nl)\n",
    "\n",
    "**Assignment goals**:\n",
    "* Learn how to evaluate an entity linking system.\n",
    "* Learn how to run two modern entity linking systems (AGDISTIS and DBpedia Spotlight).\n",
    "* Learn how to interpret the system output and the evaluation results.\n",
    "* Get insight into differences between the two applied systems.\n",
    "* Be able to describe differences between the two methods in terms of their results.\n",
    "* Be able to propose future improvements based on the observed results.\n",
    "* Get insight into the difficulty of NED and how this depends on specific entity mentions.\n",
    "* Get insight into the relation between NED and NER.\n",
    "* Get insight into other challenges of this task.\n",
    "\n",
    "In this assignment, you are going to work with two modern systems for entity linking: AGDISTIS and DBpedia Spotlight. You will run them on several entity linking datasets and evaluate their performance. You will finally perform quantitative and qualitative analysis on their performance.\n",
    "\n",
    " We recommend that you go through the notebooks in the following order:\n",
    "* *Read the assignment (see below)*\n",
    "* *Lab3-Entity-linking-introduction.ipynb*\n",
    "* *Lab3-Entity-linking-evaluation.ipynb*\n",
    "* *Lab3-Entity-linking-tools.ipynb*\n",
    "* *Answer the questions of the assignment (see below) using the provided notebooks and submit*\n",
    "\n",
    "In this assignment you are asked to perform both quantitative and qualitative analysis of existing system outputs, as well as to run these systems on your own text. We will reflect on the results of these tasks.\n",
    "\n",
    "**Note:** We will use the dataset Reuters-128 in this assignment. This dataset was introduced in the notebook 'Lab4-Entity-linking-tools', so you probably have it already (in case you do not have it make sure you download it from Canvas first and put it in the same location as this notebook). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. General understanding questions (8 points)\n",
    "\n",
    "**Question 1a** Why do we need to disambiguate entity mentions in text? (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1b** What is the difference between micro and macro F1-score? (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1c** In which scenario would you expect a difference between the micro and the macro F1-score of a system? You can either provide an example or explain in one sentence. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1d** What is a NIL entity? (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Quantitative analysis (12 points)\n",
    "\n",
    "**Exercise 2a** Run both systems on the full Reuters-128 dataset. (2 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both systems on the full Reuters-128 dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2b** Write a function to compute the precision, recall, and F1-score for each of the systems on this dataset. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to compute the precision, recall, and F1-score for each of the systems on this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2c** What is the F1-score per system? Which system performs better? Is that also the better system in terms of precision and recall? (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2d** For each of the systems, compare their precision against their recall: which one is higher? What does that mean (hint: think of NIL entities)? (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Qualitative analysis (10 points)\n",
    "\n",
    "**Exercise 3a** Take the first five documents of this dataset and look into the behavior of both systems. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Based on what you observe, answer the following questions:_\n",
    "\n",
    "**Question 3b** Which errors are made by the systems? (make a list) (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3c** Which errors are shared by the systems, and which are only made by one of the systems? (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3d** What do you think can help fix this errors? (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3e** What kind of entity mentions are always disambiguated correctly? (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running your own text (10 points)\n",
    "\n",
    "Let's now run one of the tools (you can choose which one) with our own text. You don't need to provide the mentions for this case, you can let the software also perform the recognition of mentions.\n",
    "\n",
    "**Exercise 4a** Add your own text that you would like to be processed. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "REPLACE THIS WITH YOUR OWN TEXT\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4b** Write a function to process this text with Spotlight or another tool of your choice, and run it. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process text with your tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4c** Let's reflect: what can you say about the processing result? Are most entities correctly captured? Which mistakes do you observe? Which of the phases (recognition, candidate generation, disambiguation) seems to have caused this error? (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
