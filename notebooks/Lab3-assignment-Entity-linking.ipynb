{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment\n",
    "This notebook describes the LAB-3b assignment of the Text Mining course. It is about Entity linking.\n",
    "\n",
    "**Due**: 03 Mar at 23:59\n",
    "\n",
    "**How to submit**: Please submit your assignment using Canvas (see *Assignments* -> *Lab Session Sentiment*). Convert your notebook to PDF (in JupyterLab, this can be done by clicking on *File* in the menu bar, select *Export Notebook As*, then select *Export Notebook to PDF*)\n",
    "\n",
    "**Points**: each exercise is suffixed with the number of points you can obtain for the exercise.\n",
    "\n",
    "**Questions**: If you have questions about this topic, you can either ask them during the lab session or email:\n",
    "* Filip Ilievski (f.ilievski@vu.nl)\n",
    "* Marten Postma (m.c.postma@vu.nl)\n",
    "* Robert Kajnak: (r.kajnakmisca@student.vu.nl)\n",
    "* Karen Goes: (k.w.m.goes@student.vu.nl)\n",
    "\n",
    "**Assignment goals**:\n",
    "* Learn how to evaluate an entity linking system.\n",
    "* Learn how to run two modern entity linking systems (AGDISTIS and DBpedia Spotlight).\n",
    "* Learn how to interpret the system output and the evaluation results.\n",
    "* Get insight into differences between the two applied systems.\n",
    "* Be able to describe differences between the two methods in terms of their results.\n",
    "* Be able to propose future improvements based on the observed results.\n",
    "* Get insight into the difficulty of NED and how this depends on specific entity mentions.\n",
    "* Get insight into the relation between NED and NER.\n",
    "* Get insight into other challenges of this task.\n",
    "\n",
    "In this assignment, you are going to work with two modern systems for entity linking: AGDISTIS and DBpedia Spotlight. You will run them on several entity linking datasets and evaluate their performance. You will finally perform quantitative and qualitative analysis on their performance.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3-Entity-linking.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative and qualitative analysis of existing system outputs, as well as to run these systems on your own text. We will reflect on the results of these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Perform quantitative evaluation of both systems on the full Reuters-128 dataset\n",
    "\n",
    "**Task1.1** Write a function to compute the F1-score for each of the systems. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to compute the F1-score for each of the systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2** What is the F1-score per system? Which system performs better? (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Qualitative analysis\n",
    "\n",
    "**Task 2.1** Take the first five documents of this dataset and look into the behavior of both systems. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Based on what you observe, answer the following questions:_\n",
    "\n",
    "**Task 2.2** Which errors are made by the systems? (make a list) (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3** Which errors are shared by the systems, and which are only made by one of the systems? (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.4** What do you think can help fix this errors? (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5** What kind of entity mentions are always disambiguated correctly? (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Running your own text\n",
    "\n",
    "Let's now run one of the tools (you can choose which one) with our own text. You don't need to provide the mentions for this case, you can let the software also perform the recognition of mentions.\n",
    "\n",
    "**Task 3.1** Add your own text that you would like to be processed. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "REPLACE THIS WITH YOUR OWN TEXT\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.2** Write a function to process this text with Spotlight or another tool of your choice, and run it. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process text with your tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3** Let's reflect: what can you say about the processing result? Are most entities correctly captured? Which mistakes do you observe? Which of the phases (recognition, candidate generation, disambiguation) seems to have caused this error? (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
