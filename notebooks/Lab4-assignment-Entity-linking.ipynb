{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4 - Assignment 4 about NED\n",
    "This notebook describes the LAB-4 assignment of the Text Mining course. It is about Entity linking.\n",
    "\n",
    "**Due**: 10 Mar at 23:59\n",
    "\n",
    "**How to submit**: Please submit your assignment using Canvas (see *Assignments* -> *Lab Session Sentiment*). Convert your notebook to PDF (in JupyterLab, this can be done by clicking on *File* in the menu bar, select *Export Notebook As*, then select *Export Notebook to PDF*)\n",
    "\n",
    "**Points**: each exercise is suffixed with the number of points you can obtain for the exercise.\n",
    "\n",
    "**Questions**: If you have questions about this topic, you can either ask them during the lab session or email:\n",
    "* Filip Ilievski (f.ilievski@vu.nl)\n",
    "* Robert Kajnak: (r.kajnakmisca@student.vu.nl)\n",
    "* Karen Goes: (k.w.m.goes@student.vu.nl)\n",
    "\n",
    "**Assignment goals**:\n",
    "* Learn how to evaluate an entity linking system.\n",
    "* Learn how to run two modern entity linking systems (AGDISTIS and DBpedia Spotlight).\n",
    "* Learn how to interpret the system output and the evaluation results.\n",
    "* Get insight into differences between the two applied systems.\n",
    "* Be able to describe differences between the two methods in terms of their results.\n",
    "* Be able to propose future improvements based on the observed results.\n",
    "* Get insight into the difficulty of NED and how this depends on specific entity mentions.\n",
    "* Get insight into the relation between NED and NER.\n",
    "* Get insight into other challenges of this task.\n",
    "\n",
    "In this assignment, you are going to work with two modern systems for entity linking: AGDISTIS and DBpedia Spotlight. You will run them on several entity linking datasets and evaluate their performance. You will finally perform quantitative and qualitative analysis on their performance.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3-Entity-linking-introduction.ipynb**\n",
    "* **Lab3-Entity-linking-evaluation.ipynb**\n",
    "* **Lab3-Entity-linking-tools.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative and qualitative analysis of existing system outputs, as well as to run these systems on your own text. We will reflect on the results of these tasks.\n",
    "\n",
    "**Note:** We will use the dataset Reuters-128 in this assignment. This dataset was introduced in the notebook 'Lab4-Entity-linking-tools', so you probably have it locally already (in case you do not have it make sure you download it from Canvas first). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Quantitative analysis\n",
    "\n",
    "**Exercise 1a** Run both systems on the full Reuters-128 dataset. (2 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both systems on the full Reuters-128 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1b** Write a function to compute the precision, recall, and F1-score for each of the systems on this dataset. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to compute the precision, recall, and F1-score for each of the systems on this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1c** What is the F1-score per system? Which system performs better? Is that also the better system in terms of precision and recall? (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1d** Describe the precision, recall, and F1-score results. What does each of these three metrics mean? (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Qualitative analysis\n",
    "\n",
    "**Exercise 2a** Take the first five documents of this dataset and look into the behavior of both systems. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Based on what you observe, answer the following questions:_\n",
    "\n",
    "**Exercise 2b** Which errors are made by the systems? (make a list) (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2c** Which errors are shared by the systems, and which are only made by one of the systems? (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2d** What do you think can help fix this errors? (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2e** What kind of entity mentions are always disambiguated correctly? (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Running your own text\n",
    "\n",
    "Let's now run one of the tools (you can choose which one) with our own text. You don't need to provide the mentions for this case, you can let the software also perform the recognition of mentions.\n",
    "\n",
    "**Exercise 3a** Add your own text that you would like to be processed. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "REPLACE THIS WITH YOUR OWN TEXT\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3b** Write a function to process this text with Spotlight or another tool of your choice, and run it. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process text with your tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3c** Let's reflect: what can you say about the processing result? Are most entities correctly captured? Which mistakes do you observe? Which of the phases (recognition, candidate generation, disambiguation) seems to have caused this error? (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
