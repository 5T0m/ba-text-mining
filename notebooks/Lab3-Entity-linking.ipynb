{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB3 - Entity Linking/Named Entity Disambiguation\n",
    "\n",
    "### 1. Task definition\n",
    "\n",
    "**Entity tasks so far** So far, we have seen two tasks that relate to the entities mentioned in text: \n",
    "1. recognizing/spotting entity mentions in text in the task of Named Entity Recognition\n",
    "2. classifying these entity mentions in a semantic class that they belong to, in the task of Named Entity Classification/Typing\n",
    "\n",
    "**NED** Here, we will introduce Named Entity Disambiguation - NED, also called (Named) Entity Linking - (N)EL. NED is a central task in information extraction. The goal is to take the entity mentions that were found in text with the task of NER and resolve their meaning. In this sense, the task of Entity Disambiguation builds on top of the output of the NER task. For this reason, sometimes the tasks are combined together in a task called Named Entity Recognition and Disambiguation (NERD).\n",
    "\n",
    "**Disambiguation in practice** The disambiguation in this task is done by linking a phrase found in text (for example, 'JFK') to its existing representation in an entity-centered knowledge base (for example, https://en.wikipedia.org/wiki/John_F._Kennedy_International_Airport ). \n",
    "\n",
    "An entity-centered knowledge base is a collection of facts about entities. It can be in a structured or unstructured format. Wikipedia is an example for an unstructured knowledge base, because most of its content is in unstructured (running text) form. Examples for structured knowledge bases are DBpedia and Wikidata. Here is the representation of the JFK airport in these structured knowledge bases:\n",
    "* http://dbpedia.org/resource/John_F._Kennedy_International_Airport\n",
    "* https://www.wikidata.org/wiki/Q8685\n",
    "\n",
    "**Example** For example, let's consider the following sentence:\n",
    "\n",
    "\"_JetBlue_ begins direct service between _Barnstable Airport_ and _JFK_.\"\n",
    "\n",
    "Let's say that we perform linking to DBpedia. Then, “JetBlue” should be linked to the entity http://dbpedia.org/resource/JetBlue, and “JFK” to http://dbpedia.org/resource/John_F._Kennedy_International_Airport. \n",
    "\n",
    "However, there is no entry in DBpedia for the Barnstable Municipal Airport, which is the meaning of the mention “Barnstable Airport”. We cannot link this entity then. The entities for which there is no representation in a chosen knowledge base are called *NIL entities*. When a system processes the text, it should simply say that the meaning of “Barnstable Airport” is _NIL_.\n",
    "\n",
    "### 2. Opportunities and challenges\n",
    "\n",
    "**Connecting text and knowledge bases** This is the first time we encounter such a connection between the information in text and the knowledge bases in the external world in this course. Note that these knowledge bases were not created to improve the text processing. Instead, they exist in order to provide knowledge about the world - for example, Wikipedia, DBpedia, and Wikidata give us encyclopedic knowledge. \n",
    "\n",
    "**Opportunities** By creating a link between a phrase in text and a unique entry in a knowledge base, we directly get access to much more knowledge that we can use to enhance the information in text. If we know that 'JFK' refers to the airport, we allow our tools to have access to all facts about this airport, such as its location and founding year. In addition, if we want, we can now extract facts from text and store them in these knowledge bases, but this is another task for later :)\n",
    "\n",
    "**Challenges** So, why is entity linking not an easy task? This relates to two aspects: ambiguity and variance. \n",
    "\n",
    "*Ambiguity* is the amount of meanings that a certain entity mention can have. For example, imagine how many people in the world are called \"John Smith\". DBpedia contains entries for a few hundreds of them, see http://dbpedia.org/page/John_Smith. How can we teach a computer to decide which of these is the one mentioned in text? And, what if the John Smith mentioned in text is a NIL entity and is not stored in DBpedia?\n",
    "\n",
    "There are also many cases where it is quite easy to link an entity to a knowledge base. Often the mentions in text have a small ambiguity (for example, \"Barack Obama\"). Or, they have multiple meanings but one of them is used almost always: for example, there are multiple cities called \"Paris\", but the French capital will be most often mentioned in text.\n",
    "\n",
    "*Variance* is the amount of different mentions that refer to the same entity. For example, http://dbpedia.org/resource/John_F._Kennedy_International_Airport can be called \"JFK\", or \"John F. Kennedy Airport\", or \"The NYC airport\" in text.\n",
    "\n",
    "### 3. Named Entity Disambiguation in practice: 3 phases\n",
    "\n",
    "In practice, most NED systems consist of three phases:\n",
    "\n",
    "1. **Entity recognition/spotting** - this is done as described in the NER(C). In the example sentence \"_JetBlue_ begins direct service between _Barnstable Airport_ and _JFK_.\", the recognition phase will detect the entity mentions: \"JetBlue\", \"Barnstable Airport\", and \"JFK\".\n",
    "2. **Candidate generation** - here we take each of the recognized mentions and look up in the knowledge base for potential meanings. For example, the phrase \"JFK\" could have these candidates:\n",
    "    * http://dbpedia.org/resource/John_F._Kennedy\n",
    "    * http://dbpedia.org/resource/John_F._Kennedy_International_Airport\n",
    "    * http://dbpedia.org/resource/JFK_(film)\n",
    "    * http://dbpedia.org/resource/JFK_University\n",
    "    * http://dbpedia.org/resource/Justice_for_Khojaly\n",
    "    \n",
    "   and so on. Similar lists will be generated for the other mentions found in text: \"JetBlue\" and \"Barnstable Airport\". The candidate generation phase is not trivial because of the ambiguity and variation described above. Also, new entities are appearing all the time in news articles, so the number of options grows over time.\n",
    "\n",
    "3. **Disambiguation** - the goal of this final phase is to take the list of potential meanings generated in the candidate generation phase for each of the mentions and make a decision on which instance is the correct one. This decision can either be: choosing one of the possible candidates, or deciding that no candidate is the correct one (NIL entity).\n",
    "\n",
    "### 4. Evaluating entity linking\n",
    "\n",
    "**Metric** The correctness of an entity linking system is measured in terms of precision, recall, and F1-score. For each of the mentions in text, we compare the system decision against the gold data:\n",
    "* If the system chose entity X and the gold entity is also X, then we count a *true positive (TP)*\n",
    "* If the system chose entity X, but the gold entity is Y, then we count a *false positive (FP)* and a *false negative (FN)*\n",
    "* If the system opted for a NIL entity and the gold entity is X, then we count a *false negative (FN)*\n",
    "* If the system opted for an entity X but the gold entity is NIL, then we count a *false positive (FP)*\n",
    "\n",
    "Afterwards, we use these numbers for TP, FP, and FN, to compute precision, recall, and F1-score:\n",
    "\n",
    "* `precision=TP/(TP+FP)` -> From the decisions made by the system, how many were true\n",
    "* `recall=TP/(TP+FN)` -> From the gold entities, how many were found correctly by the system\n",
    "* `f1=2*precision*recall/(precision+recall)` -> compute a harmonic mean between precision and recall, called F1-score\n",
    "\n",
    "Note that precision, recall, and F1-score would all be the same in case all entities in the system output and the gold output are not NIL entities.\n",
    "\n",
    "**Example** For the example sentence above, let's say that a system made the following decisions:\n",
    "* \"JetBlue\" means http://dbpedia.org/resource/JetBlue (true positive)\n",
    "* \"Barnstable Airport\" means http://dbpedia.org/resource/Barnstable,_Massachusetts (false positive)\n",
    "* \"JFK\" means http://dbpedia.org/resource/John_F._Kennedy (false positive and false negative)\n",
    "\n",
    "Then, we have in total: `TP=1, FP=2, FN=1`. \n",
    "\n",
    "The resulting precision is `1/3=0.33` and the resulting recall is `1/2=0.5`. \n",
    "\n",
    "The F1-score of this system on this sentence would be `0.40`. \n",
    "\n",
    "### 5. An example in code\n",
    "\n",
    "Now we provide a code for this scenario. Note that for simplicity we assume that the entity recognition by the system is perfect. Also, we use a simple format of the gold and the system output as a list, in practice this requires some more preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 1; \n",
      "FP: 2, \n",
      "FN: 1\n",
      "Precision: 0.33, \n",
      "recall: 0.50, \n",
      "f1-score: 0.40\n"
     ]
    }
   ],
   "source": [
    "text=\"JetBlue begins direct service between Barnstable Airport and JFK.\"\n",
    "\n",
    "gold_decisions=['http://dbpedia.org/resource/JetBlue', \n",
    "                'NIL',\n",
    "                'http://dbpedia.org/resource/John_F._Kennedy_International_Airport']\n",
    "system_decisions=['http://dbpedia.org/resource/JetBlue', \n",
    "                  'http://dbpedia.org/resource/Barnstable,_Massachusetts',\n",
    "                 'http://dbpedia.org/resource/John_F._Kennedy']\n",
    "\n",
    "num_entities=len(gold_decisions)\n",
    "\n",
    "tp=0\n",
    "fp=0\n",
    "fn=0\n",
    "\n",
    "for mention_id in range(num_entities):\n",
    "    gold_entity=gold_decisions[mention_id]\n",
    "    system_entity=system_decisions[mention_id]\n",
    "    if gold_entity=='NIL' and system_entity=='NIL': continue\n",
    "    if gold_entity==system_entity:\n",
    "        tp+=1\n",
    "    else:\n",
    "        if gold_entity!='NIL':\n",
    "            fn+=1\n",
    "        if system_entity!='NIL':\n",
    "            fp+=1\n",
    "\n",
    "print('TP: %d; \\nFP: %d, \\nFN: %d' % (tp, fp, fn))            \n",
    "            \n",
    "precision=tp/(tp+fp)\n",
    "recall=tp/(tp+fn)\n",
    "f1=2*precision*recall/(precision+recall)\n",
    "\n",
    "print('Precision: %.2f, \\nrecall: %.2f, \\nf1-score: %.2f' % (precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
