{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2 - Sentiment analysis using VADER\n",
    "\n",
    "In this notebook, we introduce how to use the [VADER](https://github.com/cjhutto/vaderSentiment) as part of the NLTK to perform sentiment analysis.\n",
    "\n",
    "**at the end of this notebook, you will**:\n",
    "* have VADER installed on your computer\n",
    "* be able to load the VADER model\n",
    "\n",
    "\n",
    "**If you want to learn more sentiment analysis, you might find the following links useful**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading VADER package\n",
    "Please run the following commands first to download VADER to your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/marten/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that the download was succesful, you can run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import vader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VADER model\n",
    "The model can be loaded in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_model = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tokenize text in order to apply VADER. We will use spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take an arbitrary text and run spaCy on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sometext = \"Here are my sentences. It's a nice day. It's a rainy day.\" \n",
    "doc = nlp(sometext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect how spaCy split the text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are my sentences.\n",
      "It's a nice day.\n",
      "It's a rainy day.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next for loop assigns a sentiment score from VADER to **each sentence**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INPUT SENTENCE Here are my sentences.\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.714, 'pos': 0.286, 'compound': 0.0516}\n",
      "\n",
      "INPUT SENTENCE It's a nice day.\n",
      "VADER OUTPUT {'neg': 0.0, 'neu': 0.417, 'pos': 0.583, 'compound': 0.4215}\n",
      "\n",
      "INPUT SENTENCE It's a rainy day.\n",
      "VADER OUTPUT {'neg': 0.394, 'neu': 0.606, 'pos': 0.0, 'compound': -0.0772}\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    scores = vader_model.polarity_scores(sent.text)\n",
    "    print()\n",
    "    print('INPUT SENTENCE', sent)\n",
    "    print('VADER OUTPUT', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and testing a Sentiment classifier in NLTK\n",
    "\n",
    "http://www.nltk.org/api/nltk.sentiment.html#module-nltk.sentiment.util\n",
    "\n",
    "https://www.nltk.org/book/ch06.html\n",
    "\n",
    "* section 6.1\n",
    "* section 6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "# Loading stuff\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import * # needed for the mark_negation function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the datasets (subjective and objective sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first obtain the subjectivity corpus that is included in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this data set we are going to select 200 sentences for training and testing.\n",
    "The package subjectivity.sents defines which sentences are subjective ('subj') and which ones are objective ('obj')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_instances = 100\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "len(subj_docs), len(obj_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now balanced. Why is this important for a NaiveBayesClassifier? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Document is represented by a tuple (ie. in the form <sentence, label>. The sentence is tokenised, so it is represented by a list of strings. The labels is subj or obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"there's\",\n",
       "  'lots',\n",
       "  'of',\n",
       "  'cool',\n",
       "  'stuff',\n",
       "  'packed',\n",
       "  'into',\n",
       "  \"espn's\",\n",
       "  'ultimate',\n",
       "  'x',\n",
       "  '.'],\n",
       " 'subj')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj_docs[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subjective and objective instances were split separately, to keep a balanced uniform class distribution in both train and test sets. We create the train and test set by taking the first 80 sentences as train and the last 20 sentences as test. We then concatenate the subjective and objective sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subj_docs = subj_docs[:80]\n",
    "test_subj_docs = subj_docs[80:100]\n",
    "train_obj_docs = obj_docs[:80]\n",
    "test_obj_docs = obj_docs[80:100]\n",
    "training_docs = train_subj_docs+train_obj_docs\n",
    "testing_docs = test_subj_docs+test_obj_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initialize a SentimentAnalyser and use a mark_negation function for negative words. mark_negationis a utility function that marks words that are negations that can switch the polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple unigram word features are then used, handling negation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "len(unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'the', ',', 'a', 'and', 'of', 'to', 'is', 'in', 'with']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first 10\n",
    "unigram_feats[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, features are applied to obtain a feature-value representation of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the feature presentation of the test_set. Do you understand what it represents? Why are so many features False?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'contains(.)': True,\n",
       "  'contains(the)': True,\n",
       "  'contains(,)': False,\n",
       "  'contains(a)': True,\n",
       "  'contains(and)': False,\n",
       "  'contains(of)': True,\n",
       "  'contains(to)': False,\n",
       "  'contains(is)': False,\n",
       "  'contains(in)': False,\n",
       "  'contains(with)': True,\n",
       "  'contains(it)': False,\n",
       "  'contains(that)': False,\n",
       "  'contains(his)': False,\n",
       "  'contains(on)': False,\n",
       "  'contains(for)': True,\n",
       "  'contains(an)': False,\n",
       "  'contains(who)': False,\n",
       "  'contains(by)': False,\n",
       "  'contains(he)': False,\n",
       "  'contains(from)': False,\n",
       "  'contains(her)': False,\n",
       "  'contains(\")': False,\n",
       "  'contains(film)': False,\n",
       "  'contains(as)': False,\n",
       "  'contains(this)': False,\n",
       "  'contains(movie)': False,\n",
       "  'contains(their)': False,\n",
       "  'contains(but)': False,\n",
       "  'contains(one)': False,\n",
       "  'contains(at)': False,\n",
       "  'contains(about)': False,\n",
       "  'contains(the_NEG)': False,\n",
       "  'contains(a_NEG)': False,\n",
       "  'contains(to_NEG)': False,\n",
       "  'contains(are)': False,\n",
       "  \"contains(there's)\": False,\n",
       "  'contains(()': False,\n",
       "  'contains(story)': False,\n",
       "  'contains(when)': False,\n",
       "  'contains(so)': False,\n",
       "  'contains(be)': False,\n",
       "  'contains(,_NEG)': False,\n",
       "  'contains())': False,\n",
       "  'contains(they)': False,\n",
       "  'contains(you)': False,\n",
       "  'contains(not)': False,\n",
       "  'contains(have)': False,\n",
       "  'contains(like)': False,\n",
       "  'contains(will)': False,\n",
       "  'contains(all)': False,\n",
       "  'contains(into)': False,\n",
       "  'contains(out)': False,\n",
       "  'contains(she)': False,\n",
       "  'contains(what)': False,\n",
       "  'contains(life)': False,\n",
       "  'contains(has)': False,\n",
       "  'contains(its)': False,\n",
       "  'contains(only)': False,\n",
       "  'contains(more)': False,\n",
       "  'contains(even)': False,\n",
       "  'contains(--)': False,\n",
       "  'contains(:)': False,\n",
       "  'contains(can)': False,\n",
       "  'contains(;)': False,\n",
       "  'contains(home)': False,\n",
       "  'contains(look)': False,\n",
       "  \"contains(it's)\": False,\n",
       "  'contains(if)': False,\n",
       "  'contains(where)': False,\n",
       "  'contains(most)': False,\n",
       "  'contains(him)': False,\n",
       "  'contains(search)': False,\n",
       "  'contains(but_NEG)': False,\n",
       "  'contains(love)': False,\n",
       "  'contains(both)': False,\n",
       "  'contains(make)': False,\n",
       "  'contains(begins)': False,\n",
       "  'contains(some)': False,\n",
       "  'contains(two)': False,\n",
       "  'contains(of_NEG)': False,\n",
       "  'contains(made)': False,\n",
       "  'contains(which)': False,\n",
       "  'contains(them)': False},\n",
       " 'subj')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first\n",
    "test_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we are ready to train our classifier on the training set, and output the evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.8\n",
      "F-measure [obj]: 0.8\n",
      "F-measure [subj]: 0.8\n",
      "Precision [obj]: 0.8\n",
      "Precision [subj]: 0.8\n",
      "Recall [obj]: 0.8\n",
      "Recall [subj]: 0.8\n"
     ]
    }
   ],
   "source": [
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "# output: Training classifier\n",
    "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "    print('{0}: {1}'.format(key, value))\n",
    "#Outputs:\n",
    "#Evaluating NaiveBayesClassifier results...\n",
    "#Accuracy: 0.8\n",
    "#F-measure [obj]: 0.8\n",
    "#F-measure [subj]: 0.8\n",
    "#Precision [obj]: 0.8\n",
    "#Precision [subj]: 0.8\n",
    "#Recall [obj]: 0.8\n",
    "#Recall [subj]: 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a positive and negative classifier from movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first are going to load the movie_reviews data set from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "print(len(negids), len(posids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two data sets, one with the files that are negative reviews and one with the files that are positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg/cv000_29416.txt'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First negative review:\n",
    "negids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next are going to extract texts from each sub data set and create tuples with the labels 'neg' and 'pos', where the first element is the feature representation of the words of the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'plot': True,\n",
       "  ':': True,\n",
       "  'two': True,\n",
       "  'teen': True,\n",
       "  'couples': True,\n",
       "  'go': True,\n",
       "  'to': True,\n",
       "  'a': True,\n",
       "  'church': True,\n",
       "  'party': True,\n",
       "  ',': True,\n",
       "  'drink': True,\n",
       "  'and': True,\n",
       "  'then': True,\n",
       "  'drive': True,\n",
       "  '.': True,\n",
       "  'they': True,\n",
       "  'get': True,\n",
       "  'into': True,\n",
       "  'an': True,\n",
       "  'accident': True,\n",
       "  'one': True,\n",
       "  'of': True,\n",
       "  'the': True,\n",
       "  'guys': True,\n",
       "  'dies': True,\n",
       "  'but': True,\n",
       "  'his': True,\n",
       "  'girlfriend': True,\n",
       "  'continues': True,\n",
       "  'see': True,\n",
       "  'him': True,\n",
       "  'in': True,\n",
       "  'her': True,\n",
       "  'life': True,\n",
       "  'has': True,\n",
       "  'nightmares': True,\n",
       "  'what': True,\n",
       "  \"'\": True,\n",
       "  's': True,\n",
       "  'deal': True,\n",
       "  '?': True,\n",
       "  'watch': True,\n",
       "  'movie': True,\n",
       "  '\"': True,\n",
       "  'sorta': True,\n",
       "  'find': True,\n",
       "  'out': True,\n",
       "  'critique': True,\n",
       "  'mind': True,\n",
       "  '-': True,\n",
       "  'fuck': True,\n",
       "  'for': True,\n",
       "  'generation': True,\n",
       "  'that': True,\n",
       "  'touches': True,\n",
       "  'on': True,\n",
       "  'very': True,\n",
       "  'cool': True,\n",
       "  'idea': True,\n",
       "  'presents': True,\n",
       "  'it': True,\n",
       "  'bad': True,\n",
       "  'package': True,\n",
       "  'which': True,\n",
       "  'is': True,\n",
       "  'makes': True,\n",
       "  'this': True,\n",
       "  'review': True,\n",
       "  'even': True,\n",
       "  'harder': True,\n",
       "  'write': True,\n",
       "  'since': True,\n",
       "  'i': True,\n",
       "  'generally': True,\n",
       "  'applaud': True,\n",
       "  'films': True,\n",
       "  'attempt': True,\n",
       "  'break': True,\n",
       "  'mold': True,\n",
       "  'mess': True,\n",
       "  'with': True,\n",
       "  'your': True,\n",
       "  'head': True,\n",
       "  'such': True,\n",
       "  '(': True,\n",
       "  'lost': True,\n",
       "  'highway': True,\n",
       "  '&': True,\n",
       "  'memento': True,\n",
       "  ')': True,\n",
       "  'there': True,\n",
       "  'are': True,\n",
       "  'good': True,\n",
       "  'ways': True,\n",
       "  'making': True,\n",
       "  'all': True,\n",
       "  'types': True,\n",
       "  'these': True,\n",
       "  'folks': True,\n",
       "  'just': True,\n",
       "  'didn': True,\n",
       "  't': True,\n",
       "  'snag': True,\n",
       "  'correctly': True,\n",
       "  'seem': True,\n",
       "  'have': True,\n",
       "  'taken': True,\n",
       "  'pretty': True,\n",
       "  'neat': True,\n",
       "  'concept': True,\n",
       "  'executed': True,\n",
       "  'terribly': True,\n",
       "  'so': True,\n",
       "  'problems': True,\n",
       "  'well': True,\n",
       "  'its': True,\n",
       "  'main': True,\n",
       "  'problem': True,\n",
       "  'simply': True,\n",
       "  'too': True,\n",
       "  'jumbled': True,\n",
       "  'starts': True,\n",
       "  'off': True,\n",
       "  'normal': True,\n",
       "  'downshifts': True,\n",
       "  'fantasy': True,\n",
       "  'world': True,\n",
       "  'you': True,\n",
       "  'as': True,\n",
       "  'audience': True,\n",
       "  'member': True,\n",
       "  'no': True,\n",
       "  'going': True,\n",
       "  'dreams': True,\n",
       "  'characters': True,\n",
       "  'coming': True,\n",
       "  'back': True,\n",
       "  'from': True,\n",
       "  'dead': True,\n",
       "  'others': True,\n",
       "  'who': True,\n",
       "  'look': True,\n",
       "  'like': True,\n",
       "  'strange': True,\n",
       "  'apparitions': True,\n",
       "  'disappearances': True,\n",
       "  'looooot': True,\n",
       "  'chase': True,\n",
       "  'scenes': True,\n",
       "  'tons': True,\n",
       "  'weird': True,\n",
       "  'things': True,\n",
       "  'happen': True,\n",
       "  'most': True,\n",
       "  'not': True,\n",
       "  'explained': True,\n",
       "  'now': True,\n",
       "  'personally': True,\n",
       "  'don': True,\n",
       "  'trying': True,\n",
       "  'unravel': True,\n",
       "  'film': True,\n",
       "  'every': True,\n",
       "  'when': True,\n",
       "  'does': True,\n",
       "  'give': True,\n",
       "  'me': True,\n",
       "  'same': True,\n",
       "  'clue': True,\n",
       "  'over': True,\n",
       "  'again': True,\n",
       "  'kind': True,\n",
       "  'fed': True,\n",
       "  'up': True,\n",
       "  'after': True,\n",
       "  'while': True,\n",
       "  'biggest': True,\n",
       "  'obviously': True,\n",
       "  'got': True,\n",
       "  'big': True,\n",
       "  'secret': True,\n",
       "  'hide': True,\n",
       "  'seems': True,\n",
       "  'want': True,\n",
       "  'completely': True,\n",
       "  'until': True,\n",
       "  'final': True,\n",
       "  'five': True,\n",
       "  'minutes': True,\n",
       "  'do': True,\n",
       "  'make': True,\n",
       "  'entertaining': True,\n",
       "  'thrilling': True,\n",
       "  'or': True,\n",
       "  'engaging': True,\n",
       "  'meantime': True,\n",
       "  'really': True,\n",
       "  'sad': True,\n",
       "  'part': True,\n",
       "  'arrow': True,\n",
       "  'both': True,\n",
       "  'dig': True,\n",
       "  'flicks': True,\n",
       "  'we': True,\n",
       "  'actually': True,\n",
       "  'figured': True,\n",
       "  'by': True,\n",
       "  'half': True,\n",
       "  'way': True,\n",
       "  'point': True,\n",
       "  'strangeness': True,\n",
       "  'did': True,\n",
       "  'start': True,\n",
       "  'little': True,\n",
       "  'bit': True,\n",
       "  'sense': True,\n",
       "  'still': True,\n",
       "  'more': True,\n",
       "  'guess': True,\n",
       "  'bottom': True,\n",
       "  'line': True,\n",
       "  'movies': True,\n",
       "  'should': True,\n",
       "  'always': True,\n",
       "  'sure': True,\n",
       "  'before': True,\n",
       "  'given': True,\n",
       "  'password': True,\n",
       "  'enter': True,\n",
       "  'understanding': True,\n",
       "  'mean': True,\n",
       "  'showing': True,\n",
       "  'melissa': True,\n",
       "  'sagemiller': True,\n",
       "  'running': True,\n",
       "  'away': True,\n",
       "  'visions': True,\n",
       "  'about': True,\n",
       "  '20': True,\n",
       "  'throughout': True,\n",
       "  'plain': True,\n",
       "  'lazy': True,\n",
       "  '!': True,\n",
       "  'okay': True,\n",
       "  'people': True,\n",
       "  'chasing': True,\n",
       "  'know': True,\n",
       "  'need': True,\n",
       "  'how': True,\n",
       "  'giving': True,\n",
       "  'us': True,\n",
       "  'different': True,\n",
       "  'offering': True,\n",
       "  'further': True,\n",
       "  'insight': True,\n",
       "  'down': True,\n",
       "  'apparently': True,\n",
       "  'studio': True,\n",
       "  'took': True,\n",
       "  'director': True,\n",
       "  'chopped': True,\n",
       "  'themselves': True,\n",
       "  'shows': True,\n",
       "  'might': True,\n",
       "  've': True,\n",
       "  'been': True,\n",
       "  'decent': True,\n",
       "  'here': True,\n",
       "  'somewhere': True,\n",
       "  'suits': True,\n",
       "  'decided': True,\n",
       "  'turning': True,\n",
       "  'music': True,\n",
       "  'video': True,\n",
       "  'edge': True,\n",
       "  'would': True,\n",
       "  'actors': True,\n",
       "  'although': True,\n",
       "  'wes': True,\n",
       "  'bentley': True,\n",
       "  'seemed': True,\n",
       "  'be': True,\n",
       "  'playing': True,\n",
       "  'exact': True,\n",
       "  'character': True,\n",
       "  'he': True,\n",
       "  'american': True,\n",
       "  'beauty': True,\n",
       "  'only': True,\n",
       "  'new': True,\n",
       "  'neighborhood': True,\n",
       "  'my': True,\n",
       "  'kudos': True,\n",
       "  'holds': True,\n",
       "  'own': True,\n",
       "  'entire': True,\n",
       "  'feeling': True,\n",
       "  'unraveling': True,\n",
       "  'overall': True,\n",
       "  'doesn': True,\n",
       "  'stick': True,\n",
       "  'because': True,\n",
       "  'entertain': True,\n",
       "  'confusing': True,\n",
       "  'rarely': True,\n",
       "  'excites': True,\n",
       "  'feels': True,\n",
       "  'redundant': True,\n",
       "  'runtime': True,\n",
       "  'despite': True,\n",
       "  'ending': True,\n",
       "  'explanation': True,\n",
       "  'craziness': True,\n",
       "  'came': True,\n",
       "  'oh': True,\n",
       "  'horror': True,\n",
       "  'slasher': True,\n",
       "  'flick': True,\n",
       "  'packaged': True,\n",
       "  'someone': True,\n",
       "  'assuming': True,\n",
       "  'genre': True,\n",
       "  'hot': True,\n",
       "  'kids': True,\n",
       "  'also': True,\n",
       "  'wrapped': True,\n",
       "  'production': True,\n",
       "  'years': True,\n",
       "  'ago': True,\n",
       "  'sitting': True,\n",
       "  'shelves': True,\n",
       "  'ever': True,\n",
       "  'whatever': True,\n",
       "  'skip': True,\n",
       "  'where': True,\n",
       "  'joblo': True,\n",
       "  'nightmare': True,\n",
       "  'elm': True,\n",
       "  'street': True,\n",
       "  '3': True,\n",
       "  '7': True,\n",
       "  '/': True,\n",
       "  '10': True,\n",
       "  'blair': True,\n",
       "  'witch': True,\n",
       "  '2': True,\n",
       "  'crow': True,\n",
       "  '9': True,\n",
       "  'salvation': True,\n",
       "  '4': True,\n",
       "  'stir': True,\n",
       "  'echoes': True,\n",
       "  '8': True},\n",
       " 'neg')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# lets print the first tuple from the negative set\n",
    "negfeats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "# Define a split over the data for creating a train and test set\n",
    "negcutoff = int(len(negfeats)*3/4)\n",
    "poscutoff = int(len(posfeats)*3/4)\n",
    "\n",
    "print(negcutoff)\n",
    "print(poscutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n"
     ]
    }
   ],
   "source": [
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.728\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to create your own train and test set with labels SPAM and NOTSPAM or POS and NEG and see if you can train a NaiveBayesClassifier in the same way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You first need to find out how to create a data set with labels for training and testing.\n",
    "Check out the subjectivity on your local disk that is included in the NLTK download.\n",
    "On a mac you can find it below /Users/<your username>, e.g.:\n",
    "\n",
    "/Users/piek/nltl_data/corpora/subjectivity\n",
    "\n",
    "On a Windows or Linux machine it is in a slightly different path also in your user directory.\n",
    "\n",
    "Read the README.txt file that comes with the data:\n",
    "\n",
    "  * quote.tok.gt9.5000 contains 5000 subjective sentences (or snippets);\n",
    "\n",
    "  * plot.tok.gt9.5000 contains 5000 objective sentences.\n",
    "  \n",
    "https://www.nltk.org/_modules/nltk/corpus/reader/categorized_sents.html\n",
    "  \n",
    "In order to create another data set you need to create the tuples consisting of a sentence and a label.\n",
    "Remember that we used the subjectivity.sents function to load the tuples from the corpus:\n",
    "\n",
    "n_instances = 100\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subjectivity packages uses a specific format and function to create the tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['smart',\n",
       "   'and',\n",
       "   'alert',\n",
       "   ',',\n",
       "   'thirteen',\n",
       "   'conversations',\n",
       "   'about',\n",
       "   'one',\n",
       "   'thing',\n",
       "   'is',\n",
       "   'a',\n",
       "   'small',\n",
       "   'gem',\n",
       "   '.'],\n",
       "  'subj'),\n",
       " (['color',\n",
       "   ',',\n",
       "   'musical',\n",
       "   'bounce',\n",
       "   'and',\n",
       "   'warm',\n",
       "   'seas',\n",
       "   'lapping',\n",
       "   'on',\n",
       "   'island',\n",
       "   'shores',\n",
       "   '.',\n",
       "   'and',\n",
       "   'just',\n",
       "   'enough',\n",
       "   'science',\n",
       "   'to',\n",
       "   'send',\n",
       "   'you',\n",
       "   'home',\n",
       "   'thinking',\n",
       "   '.'],\n",
       "  'subj')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check out the first two items to see how it is structured\n",
    "subj_docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a very simple example that shows how you can create tuples from two simple sentences, turn them into word features and train a NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'I': True}, 'pos'), ({'l': True, 'i': True, 'k': True, 'e': True}, 'pos'), ({'g': True, 'r': True, 'e': True, 'n': True}, 'pos'), ({'e': True, 'g': True, 's': True}, 'pos'), ({'a': True, 'n': True, 'd': True}, 'pos'), ({'h': True, 'a': True, 'm': True}, 'pos'), ({',': True}, 'pos'), ({'a': True, 'n': True, 'd': True}, 'pos'), ({'I': True}, 'pos'), ({'l': True, 'i': True, 'k': True, 'e': True}, 'pos'), ({'t': True, 'h': True, 'e': True, 'm': True}, 'pos'), ({'t': True, 'o': True}, 'pos'), ({'!': True}, 'pos')]\n"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "#from nltk.corpus import names\n",
    "\n",
    "# simple function that turns a list of words into word_feats (word features)\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "# In a lexical approach, you would predefine the positive, negative and neutral words and only use these to train a classifier\n",
    "positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\n",
    "negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\n",
    "neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]\n",
    "\n",
    "# Assume you have a collections of texts that are negative and neutral\n",
    "negsentence = \"I do not like green eggs and ham, and I do not like them too!\"\n",
    "possentence = \"I like green eggs and ham, and I like them too!\"\n",
    "# By using the tokenization function, you can turn them into word negative and positive lists\n",
    "negtokens = nltk.word_tokenize(negsentence)\n",
    "postokens = nltk.word_tokenize(possentence)\n",
    "\n",
    "# Next we use the simple word feature function to turn them into features that can be used for training the classifier \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in postokens]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negtokens]\n",
    "# for neural we now take the vocabulary given above\n",
    "neutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]\n",
    "print(positive_features) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would be another way to obtain neutral word features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you do this for a data set where positive and negative texts are stored in two separate directories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we simply concatenate the features to create a training set\n",
    "train_set = negative_features + positive_features + neutral_features\n",
    "classifier = NaiveBayesClassifier.train(train_set) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to test this classifier on a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'Awesome eggs, I do not liked them'\n",
      "--------------\n",
      "\n",
      "Positive: 0.375\n",
      "Negative: 0.25\n"
     ]
    }
   ],
   "source": [
    "neg = 0\n",
    "pos = 0\n",
    "testsentence = \"Awesome eggs, I do not liked them\"\n",
    "words = nltk.word_tokenize(testsentence)\n",
    "for word in words:\n",
    "    classResult = classifier.classify(word_feats(word))\n",
    "    if classResult == 'neg':\n",
    "        neg = neg + 1\n",
    "    if classResult == 'pos':\n",
    "        pos = pos + 1\n",
    " \n",
    "print(\"Sentence: '{}'\\n--------------\\n\".format(testsentence))\n",
    "print('Positive: ' + str(float(pos)/len(words)))\n",
    "print('Negative: ' + str(float(neg)/len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not being used\n",
    "## How can you use the panda package to read files and create a data set\n",
    "#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "#from sklearn.model_selection import train_test_split # function for splitting data to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
