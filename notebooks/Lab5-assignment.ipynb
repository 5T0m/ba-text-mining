{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab5 - Assignment 5 about extraction of properties\n",
    "This notebook describes the LAB-5 assignment of the Text Mining course. It is about Property Extraction.\n",
    "\n",
    "**Due**: 17 Mar at 23:59\n",
    "\n",
    "**How to submit**: Please submit your assignment using Canvas (see *Assignments* -> *Lab Session Property Extraction*). Convert your notebook to PDF (in JupyterLab, this can be done by clicking on *File* in the menu bar, select *Export Notebook As*, then select *Export Notebook to PDF*)\n",
    "\n",
    "**Points**: each exercise is suffixed with the number of points you can obtain for the exercise.\n",
    "\n",
    "**Questions**: If you have questions about this topic, you can either ask them during the lab session or email:\n",
    "* Filip Ilievski (f.ilievski@vu.nl)\n",
    "* Robert Kajnak: (r.kajnakmisca@student.vu.nl)\n",
    "* Karen Goes: (k.w.m.goes@student.vu.nl)\n",
    "\n",
    "**Assignment goals**:\n",
    "* Get insight into the challenges of entity property extraction.\n",
    "* Learn how to build a transparent property extraction method based on patterns.\n",
    "* Get insight into the pros and cons of pattern-based property extraction.\n",
    "* Be able to evaluate property extractors.\n",
    "\n",
    "In this assignment, you are going to create your own pattern-based extractors, run them on Wikipedia texts, and evaluate them against gold values. Then, you will reflect on the performance of the pattern-based extractors.\n",
    "\n",
    " We recommend that you go through the notebooks in the following order:\n",
    "* *Read the assignment (see below)*\n",
    "* *Lab5-Property-extraction.ipynb*\n",
    "* *Answer the questions of the assignment (see below) using the provided notebooks and submit*\n",
    "\n",
    "**Good luck & have fun!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Writing your own extractors (18 points)\n",
    "\n",
    "**Exercise 1a** Write code that extracts the following properties of a person: \n",
    "* age\n",
    "* nationality\n",
    "\n",
    "(6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_age(text):\n",
    "    # Extract the age of a person\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nationality(text):\n",
    "    # Extract the nationality of a person\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1b** Write code that extracts the following properties of an organization: \n",
    "* founding year\n",
    "* country\n",
    "\n",
    "(6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_founding_year(text):\n",
    "    # Extract the founding year of an organization\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_founding_year(text):\n",
    "    # Extract the country of an organization\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1c** Write or find a small piece of text on which you expect that the extractors *WILL* work. Run your extractors on this text and print the results. Make sure that the result is as expected. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1d** Write or find a small piece of text on which you expect that the extractors *WILL NOT* work. Run your extractors on this text and print the results. Make sure that the result is as expected. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Running extractors on Wikipedia (16 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2a** Load 50 wikipedia articles about people and 50 wikipedia articles about organizations. The lists of articles can be found in the file \"wiki_entities.json\". (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2b** Run your extractors about age and nationality of people on all 50 documents about people. Save the extracted values in two lists: `extracted_ages` and `extracted_nationalities`. Don't forget to store the empty values when your extractors don't find anything in a document. (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2c** Run your extractors about founding year and country of organizations on all 50 documents about organizations. Save the extracted values in two lists: `extracted_years` and `extracted_countries`. Don't forget to store the empty values when your extractors don't find anything in a document. (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluation against Wikidata information (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3a** Write a function that evaluates your extracted property values against known (\"gold\") property values. The function should return the evaluation scores. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3b** We have extracted the gold values for all four properties from Wikidata. Load these values from the JSON files: `person_age.json`, `person_nationality.json`, `organization_year.json`, and `organization_country.json`. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3c** Run the evaluation function from 2a to compute the performance for each of your four properties. Print the precision, recall, and F1-scores. (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Reflection (16 points)\n",
    "\n",
    "Let's now look into the scores we obtained in exercise 3c and reflect.\n",
    "\n",
    "**Exercise 4a** Which attribute has the highest precision? Why do you think this is the case? (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4b** Which attribute has the highest recall? Why do you think this is the case? (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4c** Which attribute has the lowest precision? Why do you think this is the case? (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4d** Which attribute has the lowest recall? Why do you think this is the case? (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
