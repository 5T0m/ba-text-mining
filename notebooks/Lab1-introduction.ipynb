{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-1: Introduction to Natural Language Processing tools\n",
    "\n",
    "We thank [Chantal van Son](https://chantalvanson.wordpress.com/) for the creation of [a notebook](https://github.com/cltl/python-for-text-analysis/blob/master/Chapters/Chapter%2019%20-%20More%20about%20Natural%20Language%20Processing%20Tools%20(spaCy).ipynb) of which parts were used for the material of Lab 1.\n",
    "\n",
    "Text data is unstructured. But if you want to extract information from text, then you often need to process that data into a more structured representation. The common idea for all Natural Language Processing (NLP) tools is that they try to structure or transform text in some meaningful way. Examples include:\n",
    "\n",
    "* **Sentence splitting:** splitting texts into sentences\n",
    "* **Tokenization:** splitting texts into individual words\n",
    "* **Stop words recognition:** identifying commonly used words (such as 'the', 'a(n)', 'in', etc.) in text, possibly to ignore them in other tasks\n",
    "* **Part-of-speech (POS) tagging:** identifying the parts of speech of words in context (verbs, nouns, adjectives, etc.)\n",
    "* **Morphological analysis:** separating words into morphemes and identifying their classes (e.g. tense/aspect of verbs)\n",
    "* **Stemming:** identifying the stems of words in context by removing inflectional/derivational affixes, such as 'troubl' for 'trouble/troubling/troubled'\n",
    "* **Lemmatization:** identifying the lemmas (dictionary forms) of words in context, such as 'go' for 'go/goes/going/went'\n",
    "* **Word Sense Disambiguation (WSD):** assigning the correct meaning to words in context\n",
    "* **Named Entity Recognition (NER):** identifying people, locations, organizations, etc. in text\n",
    "* **Constituency/dependency parsing:** analyzing the grammatical structure of a sentence\n",
    "* **Semantic Role Labeling (SRL):** analyzing the semantic structure of a sentence (*who* does *what* to *whom*, *where* and *when*)\n",
    "* **Sentiment Analysis:** determining whether a text is mostly positive or negative\n",
    "* **Word Vectors (or Word Embeddings) and Semantic Similarity:** representating the meaning of words as rows of real valued numbers where each point captures a dimension of the word's meaning and where semantically similar words have similar vectors (very popular these days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP toolkits\n",
    "There are toolkits available that allow you to run these NLP steps using Python. In this lab, we introduce two of the most popular NLP toolkits that make use of Python:\n",
    "* [Natural Language Toolkit](https://www.nltk.org/) (see notebook **Lab1-introduction-to-NLTK.ipynb**)\n",
    "* [spaCy](https://spacy.io/) (see notebook **Lab1-introduction-to-Spacy.ipynb**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this Lab, we focus on the following NLP tasks:\n",
    "* **Sentence splitting**\n",
    "* **Tokenization** \n",
    "* **Part-of-speech (POS) tagging** \n",
    "* **Stop words recognition** \n",
    "* **Stemming**\n",
    "* **Lemmatization** \n",
    "* **Constituency/dependency parsing** \n",
    "* **Named Entity Recognition (NER)** \n",
    "\n",
    "In the notebook in which we introduce NLTK, we not only show how to perform these tasks in NLTK, but we also **explain** them.\n",
    "For SpaCy, we show how to **interpret** the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do these toolkits have in common?**\n",
    "They both allow you to run NLP modules on unstructured text.\n",
    "\n",
    "**In what do they differ?**\n",
    "* With NLTK, you run all NLP steps one by one. This allows you to fully go through each step and understand the input and output. Spacy is more of a black-box. You provide it with input and it runs all of the NLP steps for you.\n",
    "* NLTK was developed for teaching and research, whereas spaCy is what they call *industrial-strength*, i.e., meant for use on large amounts of data.\n",
    "\n",
    "**Which one is faster?** [SpaCy](https://spacy.io/usage/facts-figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage you to continue with the two notebooks that introduce NLTK and spaCy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
