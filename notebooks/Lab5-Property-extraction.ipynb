{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab5-Property extraction\n",
    "\n",
    "In this notebook, we provide more information about the task of Property Extraction.\n",
    "\n",
    "Overview of the content covered in this notebook:\n",
    "1. Introduction to property extraction\n",
    "2. Building pattern-based extractors\n",
    "3. Coding pattern-based extractors\n",
    "4. Evaluating extractors\n",
    "\n",
    "**At the end of this notebook, you will be able to**:\n",
    "* understand the task of Property Extraction and its relation to similar tasks, like Relation Extraction\n",
    "* build a pattern-based Property Extractor\n",
    "* apply it to extract properties from text\n",
    "* evaluate the pattern-based extractor\n",
    "\n",
    "**Useful links**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction to property extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of property extraction aims to fill knowledge bases with information about entities that we find in text. There are other tasks that are similar to it, such as: \n",
    "* slot filling, where we attempt to complete entity information according to some schema\n",
    "* relation extraction - given two entities, what is their relation\n",
    "* knowledge base completion, where we usually complete the knowledge base by inference from existing structured information (not from text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Building extractors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Methods\n",
    "\n",
    "In this week, we will create our own property extractors. Building automatic property extractors is not trivial, because it requires multiple steps, and these steps in practice might differ a lot between different attributes.\n",
    "\n",
    "There are two common methods for extracting attributes from text: with distant supervision and with patterns. \n",
    "* The distant supervision method relies on knowledge base information that is loosely based on a text. For example, you can think of a Wikipedia document that describes Donald Trump on one hand, and a structured information for example from wikidata that tells us that he is born in 1950. However, we don't know if this information is explicitly mentioned in text and if so, where. With this method, we can train for example a recurrent neural network on top of this kind of output, and hope that the neural network will learn the patterns in which this attribute is typically given in text.\n",
    "* A second approach for extracting attributes from text is by pattern matching. This approach is more transparent, but it requires us to define the patterns for each of the properties separately. For example, for the attribute \"birthplace\", we can use the pattern: \"X, born in Y\", or \"X from Y\". Typically, the patterns are combined with syntactic information on entity types to help their precision. For example, we will check whether indeed X is a person and Y is indeed a location in the above example.\n",
    "\n",
    "#### 2.2 Building a pattern-based extractor\n",
    "\n",
    "Typically, this consists of several parts:\n",
    "1. find a mention of a specific attribute (for example, money or birth date) in text\n",
    "2. assign this mention to some subject entity\n",
    "3. normalize the value of the attribute value\n",
    "4. normalize the subject entity\n",
    "\n",
    "**Example** Let's say we want to extract values for `founding year` in the following paragraph:\n",
    "\n",
    "\"Juventus F.C. is an Italian professional football club based in Turin. The club was founded in March 1897 by a group of Torinese students.\"\n",
    "\n",
    "\n",
    "First, we need to find attribute values that contain information about founding years. For example, we can use the pattern \"founded in\" to extract the attribute value `March 1897` in the second sentence.\n",
    "\n",
    "Next, we need to see to which subject this attribute belongs. Assuming that we perform dependency parsing of the sentences, we can find that the relation \"founded in\" has a subject `The club`. At this point, we can extract the following relation:\n",
    "\n",
    "The club FOUNDING_YEAR March 1897\n",
    "\n",
    "Syntactially this is the correct way to extract the relation. However, the relation is not really very useful yet - we need to normalize its subject and object somehow to make it useful in a semantic sense.\n",
    "\n",
    "Hence, we can normalize the value \"March 1897\" to a year value `1897`, for example, by looking for 4-digit numbers in the phrase.\n",
    "\n",
    "Then, we can normalize \"The club\" to `Juventus F.C.` by using entity coreference between the two sentences. We can even disambiguate the mention to https://en.wikipedia.org/wiki/Juventus_F.C., or `Juventus_F.C.` for brevity. This finally leads us to the following relation:\n",
    "\n",
    "`Juventus_F.C. FOUNDING_YEAR 1897`\n",
    "\n",
    "which looks much more useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Coding pattern-based extractors\n",
    "\n",
    "A simple example of extracting relations between phrases and entities using spaCy's named entity recognizer and the dependency parse. Here, we extract\n",
    "money and currency values (entities labelled as MONEY) and then check the dependency tree to find the noun phrase they are referring to â€“ for example:\n",
    "$9.4 million --> Net income. Compatible with: spaCy v2.0.0+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Loaded model 'en_core_web_sm'\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "model=\"en_core_web_sm\"\n",
    "\n",
    "nlp = spacy.load(model)\n",
    "print(\"Info: Loaded model '%s'\" % model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_entity(entities, prop_position, e_type):\n",
    "    min_distance=9999\n",
    "    closest_entity=None\n",
    "    for ent in entities:\n",
    "        if ent.label_!=e_type: \n",
    "            continue\n",
    "            \n",
    "        distance=abs(ent.start_char-prop_position)\n",
    "        if min_distance>distance:\n",
    "            min_distance=distance\n",
    "            closest_entity=ent\n",
    "    return closest_entity\n",
    "\n",
    "def extract_year_from_date(date):\n",
    "    match = re.findall('\\d{4}', date)\n",
    "    return int(match[0])\n",
    "\n",
    "def print_relations(r):\n",
    "    for r1, r2, r3 in r:\n",
    "        print('{:<10}\\t{}\\t{}'.format(r1.text, r2, r3))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_relations(doc, patterns):\n",
    "    # merge entities and noun chunks into one token\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "\n",
    "    relations = {}\n",
    "    \n",
    "    for date in filter(lambda w: w.ent_type_ == 'DATE', doc):\n",
    "        if date.dep_ == 'pobj' and date.head.dep_ == 'prep':\n",
    "            pred=date.head.head\n",
    "            if pred.text not in patterns:\n",
    "                continue\n",
    "\n",
    "        year=extract_year_from_date(date.text)\n",
    "        org=find_closest_entity(doc.ents, date.idx, 'ORG')\n",
    "        if year and org:\n",
    "            relations[org.text]=year\n",
    "\n",
    "    return relations        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nationality(doc):\n",
    "    relations={}\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "    \n",
    "    for nationality in filter(lambda w: w.ent_type_ == 'NORP', doc):\n",
    "        per=find_closest_entity(doc.ents, nationality.idx, 'PERSON')\n",
    "        if per and nationality:\n",
    "            relations[per.text]=nationality\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making sure our extractors work**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_relations(TEXTS):\n",
    "    \n",
    "    all_relations={'founding_year':{}, 'nationality': {}}\n",
    "\n",
    "    \n",
    "    founded_patterns=['founded', 'established', 'created']\n",
    "    \n",
    "    for index, text in enumerate(TEXTS):\n",
    "        doc = nlp(text)\n",
    "        all_relations['founding_year'].update(extract_date_relations(doc, founded_patterns))\n",
    "        all_relations['nationality'].update(extract_nationality(doc))\n",
    "        print('\\nRelations in text %d' % (index+1))\n",
    "#        print(founded_relations, nationality_relations)\n",
    "#        print_relations(founded_relations)\n",
    "#        print_relations(nationality_relations)\n",
    "\n",
    "#        all_founded_relations += founded_relations\n",
    "#        all_nationality_relations +=nationality_relations\n",
    "    print(all_relations)\n",
    "    return all_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Processing 2 texts\n",
      "\n",
      "**Extracted relations:**\n",
      "\n",
      "\n",
      "Relations in text 1\n",
      "\n",
      "Relations in text 2\n",
      "{'founding_year': {'Juventus F.C.': 1897}, 'nationality': {'Mike': French, 'John': American}}\n"
     ]
    }
   ],
   "source": [
    "TEXTS = [\n",
    "    'Mike is French, and John is American.',\n",
    "    'Juventus F.C. is an Italian professional football club based in Turin. The club was founded in March 1897 by a group of Torinese students.',\n",
    "]    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Info: Processing %d texts\" % len(TEXTS))\n",
    "    print()\n",
    "    print('**Extracted relations:**')\n",
    "    print()\n",
    "    \n",
    "\n",
    "    get_all_relations(TEXTS)\n",
    "\n",
    "    # Expected output:\n",
    "    # Net income      MONEY   $9.4 million\n",
    "    # the prior year  MONEY   $2.7 million\n",
    "    # Revenue         MONEY   twelve billion dollars\n",
    "    # a loss          MONEY   1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Processing wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Relations in text 1\n",
      "\n",
      "Relations in text 2\n",
      "\n",
      "Relations in text 3\n",
      "{'founding_year': {'the Computational Lexicology & Terminology Lab': 1960, 'Knowledge Representation & Reasoning': 1960, 'the SWI Department': 1990, 'Logic-Based Knowledge Representation': 1989, 'The MIT Press': 1989, 'ISBN': 1991, 'John Wiley & Sons': 2002, 'Handbook of Knowledge Representation': 2008, 'The Journal of Automatic Chemistry': 1892, 'De Geus': 1997, 'Artificial Intelligence in Medicine': 1882, 'PubMed': 2004, 'Jovell, A.': 1504, 'Rosenbrand': 2006, 'Health Technology and Informatics': 1553, 'Hommersom, A.': 1696, 'Lucas': 2009}, 'nationality': {'Schaesberg': \n",
      "\n",
      "\n",
      ", 'Rotterdam': R., 'Dekker': R., 'Balser': M., 'Van Croonenborg': M., 'Serban': R., 'Polo-Conde': M., 'Posen': Harvey}}\n"
     ]
    }
   ],
   "source": [
    "entities=[\"Piek Vossen\", \"Frank van Harmelen\", \"Bremen High School (Midlothian, Illinois)\"]\n",
    "texts=[]\n",
    "\n",
    "for entity in entities:\n",
    "    wp = wikipedia.page(entity)\n",
    "    texts.append(wp.content)\n",
    "    \n",
    "system_data = get_all_relations(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluating extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Piek Vossen, property: nationality, gold value: Dutch, system value: not found\n",
      "Entity: Frank van Harmelen, property: nationality, gold value: Dutch, system value: not found\n",
      "Entity: Bremen High School (Midlothian, Illinois), property: founding_year, gold value: 1953, system value: not found\n"
     ]
    }
   ],
   "source": [
    "properties=['founding_year', 'nationality']\n",
    "gold={}\n",
    "gold['nationality']={'Piek Vossen': 'Dutch', 'Frank van Harmelen': 'Dutch'}\n",
    "gold['founding_year']={'Bremen High School (Midlothian, Illinois)': '1953'}\n",
    "\n",
    "for prop, property_data in gold.items():\n",
    "    for entity, property_value in property_data.items():\n",
    "        if entity in system_data[prop]:\n",
    "            system_value=system_data[prop][entity]\n",
    "        else:\n",
    "            system_value='not found'\n",
    "        print('Entity: %s, property: %s, gold value: %s, system value: %s' % (entity, prop, property_value, system_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
