{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab5-Property extraction\n",
    "\n",
    "In this notebook, we provide more information about the task of Property Extraction.\n",
    "\n",
    "Overview of the content covered in this notebook:\n",
    "1. Introduction to property extraction\n",
    "2. Building pattern-based extractors\n",
    "3. Coding pattern-based extractors\n",
    "4. Evaluating extractors\n",
    "\n",
    "**At the end of this notebook, you will be able to**:\n",
    "* understand the task of Property Extraction and its relation to similar tasks, like Relation Extraction\n",
    "* build a pattern-based Property Extractor\n",
    "* apply it to extract properties from text\n",
    "* evaluate the pattern-based extractor\n",
    "\n",
    "**Useful links**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we start: set up your environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Install Wikipedia client** In this week's lab session we are going to use Wikipedia. You first need to install client package to access Wikipedia. From the terminal (with the settings that you use for notebooks) run:\n",
    "\n",
    "`conda install -c conda-forge wikipedia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Internet** Note that you need to have Internet connection to be able to access Wikipedia. If you are not connected or the connection is too slow you get the following error:\n",
    "\n",
    "`NewConnectionError: <urllib3.connection.HTTPConnection object at 0x1289d3c88>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known`\n",
    "\n",
    "**3. SpaCy** Another library that we will use in this week's lab sesion is SpaCy and its English model \"en_core_web_sm\". You probably have this already installed because we used this setup in assignment 3. If you don't, then please follow the instructrions on the SpaCy website: https://spacy.io.\n",
    "\n",
    "Typically, the installation commands you need are:\n",
    "\n",
    "`conda install -c conda-forge spacy`\n",
    "\n",
    "`python -m spacy download en_core_web_sm`\n",
    "\n",
    "We can now import SpaCy and its English model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Loaded model 'en_core_web_sm'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "model=\"en_core_web_sm\"\n",
    "\n",
    "nlp = spacy.load(model)\n",
    "print(\"Info: Loaded model '%s'\" % model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above code blocks did not yield any error, then you are all set up for this week's session. Let's start ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction to property extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the task of entity linking, we performed disambiguation of entity mentions in text by making a connection to the correct referrent for a mention in a knowledge base. Although these knowledge bases are typically fairly large, they are far from complete. Tasks like property extraction and relation extraction help to make knowledge bases more complete.\n",
    "\n",
    "The task of property extraction aims to fill knowledge bases with information about properties of entities that we find in text. There are other tasks that are similar to it, such as: \n",
    "* slot filling, where we attempt to complete entity information according to some schema\n",
    "* relation extraction - given two entities, what is their relation (for example, in Microsoft X Bill_Gates, the relation X is `hasCEO`)\n",
    "* knowledge base completion, where we usually complete a knowledge base by inference from existing structured information (not from text).\n",
    "* open information extraction - no schema available, disambiguation is non-trivial\n",
    "\n",
    "In all these tasks, including property extraction, we typically extract \"pieces of knowledge\" in the form of **a triple**. A triple consists of three elements: a subject, a predicate, and an object. An example of a triple is:\n",
    "\n",
    "Barack_Obama hasAge 57\n",
    "\n",
    "Here, Barack_Obama is a subject, hasAge is a predicate/relation, and 57 is an object. The subjects and the predicates of a triple are always URIs; the objects can be either a URI (like Barack_Obama), or a literal (like 57, or \"Barack\").\n",
    "\n",
    "Hence, property extraction typically requires us to:\n",
    "1. **detect** a property value in text (e.g., 57) and an entity it belongs to (\"Barack\")\n",
    "2. **interpret** both the property value (57 as a number) and the entity (\"Barack\" means Barack_Obama)\n",
    "3. **find their relation** - the connection between Barack_Obama and 57 is the relation hasAge\n",
    "\n",
    "In this sense, the task of property extraction builds on top of the output of NERC and the NERD.\n",
    "\n",
    "**Challenges** The property extraction task is difficult for reasons similar to those we have discussed with entity linking: ambiguity (of entities, relations, and property values), variation (of entities, values, and relations), and vagueness (when insufficient details or information is given). Futhermore,  relations can sometimes span multiple sentences or require a lot of world knowledge in order to understand them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Building extractors\n",
    "\n",
    "The main focus of this week's lab session is on creating our own property extractors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Methods\n",
    "Building automatic property extractors is not trivial, because it requires multiple steps, and these steps in practice might differ a lot between different attributes.\n",
    "\n",
    "There are two common methods for extracting attributes from text: pattern matching and distant supervision. \n",
    "* The most basic approach for extracting attributes from text is by pattern matching. This approach is transparent, but it requires us to define the patterns for each of the properties separately. For example, for the attribute \"birthplace\", we can use the pattern: \"X, born in Y\", or \"X from Y\". Typically, the patterns are combined with syntactic information on entity types to help their precision. For example, we will check whether indeed X is a person and Y is indeed a location in the above example.\n",
    "* The second approach, distant supervision, relies on knowledge base information that is loosely based on a text. For example, you can think of a Wikipedia document that describes Donald Trump on one hand, and structured information from for example wikidata that tells us that he is born in 1946. However, we don't know if this information is explicitly mentioned in the Wikipedia text and if so, where and how. With the distant supervision method, we train for example a recurrent neural network on top of this kind of output, and hope that the neural network will learn the patterns in which this attribute is typically given in text.\n",
    "\n",
    "We will use a pattern matching approach to build our extractors in this week's lab session. If you are curious about how to build a distant supervision extractor, you can check Snorkel (their [introductory notebooks](https://github.com/HazyResearch/snorkel/tree/master/tutorials/intro) are quite user-friendly).\n",
    "\n",
    "#### 2.2 Building a pattern-based extractor\n",
    "\n",
    "Typically, a pattern-based extractor consists of several parts:\n",
    "1. find a mention of a specific attribute (for example, money or birth date) in text\n",
    "2. assign this mention to some subject entity\n",
    "3. normalize the attribute value\n",
    "4. normalize the subject entity\n",
    "\n",
    "**Example** Let's say we want to extract values for `founding year` in the following paragraph from Wikipedia:\n",
    "\n",
    "\"Juventus F.C. is an Italian professional football club based in Turin. The club was founded in March 1897 by a group of Torinese students.\"\n",
    "\n",
    "**Step I** First, we need to find attribute values that contain information about founding years. For example, we can use the pattern \"founded in\" to extract the attribute value `March 1897` in the second sentence.\n",
    "\n",
    "**Step II** Next, we need to see to which subject this attribute belongs. Assuming that we perform dependency parsing of the sentences, we can find that the relation \"founded in\" has a subject `The club`. At this point, we can extract the following relation:\n",
    "\n",
    "The club FOUNDING_YEAR March 1897\n",
    "\n",
    "Syntactially this is the correct way to extract the relation. However, the relation is not really very useful yet - we need to normalize its subject and object somehow to make it useful in a semantic sense.\n",
    "\n",
    "**Step III** Hence, we can normalize the value \"March 1897\" to a year value `1897`, for example, by looking for 4-digit numbers in the phrase.\n",
    "\n",
    "**Step IV** Then, we can normalize \"The club\" to `Juventus F.C.` by using entity coreference between the two sentences. We can then disambiguate the mention to https://en.wikipedia.org/wiki/Juventus_F.C., or `Juventus_F.C.` for brevity. This finally leads us to the following relation:\n",
    "\n",
    "`Juventus_F.C. FOUNDING_YEAR 1897`\n",
    "\n",
    "which looks much more useful (and it is on a semantic level, so we can store it in a knowledge base if we would like to)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Coding pattern-based extractors\n",
    "\n",
    "We will start by searching entity mentions of some type (e.g., nationality or date) by using SpaCy's named entity recognizer. \n",
    "\n",
    "Often this is not enough for step I, because an entity of a certain type can be a value of different attributes. For example, the date \"1946\" can be a year of birth, a year of death, a founding year of a company, a year of starting/ending professional activity, etc. \n",
    "\n",
    "For this reason, we can check whether we find some keywords before the phrase (such as \"born in\" or \"founded in\"). We will build such an extractor in 3.1. \n",
    "\n",
    "We can also use the dependency tree (also from SpaCy) to find the predicate that is associated with this value and see whether that one matches our patterns. We will do this in 3.2.\n",
    "\n",
    "For step II of assigning the value to some entity phrase, we will look for the closest entity to this attribute value and assign it to that one.\n",
    "\n",
    "We will reuse much of the functions in the two examples. Let's write those first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the default library for pattern matching, called `re`\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities_of_type(a_type, a_doc):\n",
    "    return filter(lambda w: w.ent_type_ == a_type, a_doc)\n",
    "\n",
    "def find_closest_entity(entities, prop_position, e_type):\n",
    "    \"\"\"\n",
    "    Find entities of a certain type and with the smallest distance to the property.\n",
    "    \"\"\"\n",
    "    min_distance=9999\n",
    "    closest_entity=None\n",
    "    for ent in entities:\n",
    "        if ent.label_!=e_type: \n",
    "            continue # skip entities of different types\n",
    "            \n",
    "        # determine the distance between the entity and the property\n",
    "        distance=abs(ent.start_char-prop_position)\n",
    "        if min_distance>distance:\n",
    "            min_distance=distance\n",
    "            closest_entity=ent\n",
    "    return closest_entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Using substring matching\n",
    "\n",
    "We will use substring matching to look for founding years of organizations. We will use three simple patterns for this purpose: 'founded in X', 'established in X', 'created in X'. As mentioned before, we will also make sure that X is an entity of type `DATE`.\n",
    "\n",
    "We will use three helper functions in this example: to find the closest entity (step II), to convert the date to a year (step III), and to print the relations in a visually nicer way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_pattern(doc, i, pattern):\n",
    "    pattern_tokens=pattern.split(' ')\n",
    "    num_tokens=len(pattern_tokens)\n",
    "    tokens=[]\n",
    "    for x in reversed(range(1, num_tokens+1)):\n",
    "        prev_index=i-x\n",
    "        tokens.append(doc[prev_index].text)\n",
    "        \n",
    "    return tokens==pattern_tokens\n",
    "\n",
    "def pattern_found_on_the_left(doc, token_index, patterns):\n",
    "    for pattern in patterns:\n",
    "        if check_for_pattern(doc, token_index, pattern)==True:\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "def extract_year_from_date(date):\n",
    "    \"\"\"\n",
    "    Extract the year value from a date by looking for four consecutive digits.\n",
    "    \"\"\"\n",
    "    match = re.findall('\\d{4}', date)\n",
    "    first_match=match[0]\n",
    "    return int(first_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_relations(doc, patterns):\n",
    "    \"\"\"\n",
    "    Extract date properties from a document and assign them to an entity.\n",
    "    \"\"\"\n",
    "    \n",
    "    # merge entities and noun chunks into one token\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "\n",
    "    relations = {}\n",
    "    \n",
    "    dates=get_entities_of_type('DATE', doc)\n",
    "    \n",
    "    for date in dates:\n",
    "        \n",
    "        if pattern_found_on_the_left(doc, date.i, patterns):\n",
    "            print('yo')\n",
    "            year=extract_year_from_date(date.text)\n",
    "            org=find_closest_entity(doc.ents, date.idx, 'ORG')\n",
    "            print(year, org)\n",
    "            if year and org:\n",
    "                relations[org.text]=year\n",
    "\n",
    "    return relations        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test whether the date extraction works as we expect it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo\n",
      "1991 Airbus\n",
      "{'Airbus': 1991}\n"
     ]
    }
   ],
   "source": [
    "founded_patterns=['founded in', 'established in', 'created in']\n",
    "\n",
    "text='Airbus was founded in December 1991, not in January 1992.'\n",
    "\n",
    "doc = nlp(text)\n",
    "date_relations=extract_date_relations(doc, founded_patterns)\n",
    "print(date_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Using syntax dependencies\n",
    "\n",
    "\n",
    "Here, we extract money and currency values (entities labelled as MONEY) and then check the dependency tree to find the noun phrase they are referring to – for example:\n",
    "$9.4 million --> Net income. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicate_found(token, predicates):\n",
    "    if token.dep_ == 'pobj' and token.head.dep_ == 'prep':\n",
    "        pred=token.head.head\n",
    "        print(pred.text)\n",
    "        if pred.text in predicates:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nationality(doc, predicates):\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "    \n",
    "    relations={}\n",
    "    \n",
    "    nationalities=get_entities_of_type('NORP', doc)\n",
    "    \n",
    "    for nationality in nationalities:\n",
    "        print('yo')\n",
    "        if predicate_found(nationality, predicates):\n",
    "            per=find_closest_entity(doc.ents, nationality.idx, 'PERSON')\n",
    "            if per and nationality:\n",
    "                relations[per.text]=nationality\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "predicates=['is', 'was']\n",
    "text='Bush is French.'\n",
    "\n",
    "doc = nlp(text)\n",
    "nat_relations=extract_nationality(doc, predicates)\n",
    "print(nat_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Running all our extractors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "founded_patterns=['founded', 'established', 'created']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'Mike is French, and John is American.',\n",
    "    'Juventus F.C. is an Italian professional football club based in Turin. The club was founded in March 1897 by a group of Torinese students.',\n",
    "]    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    print(\"Info: Processing %d texts\" % len(texts))\n",
    "    print()\n",
    "    print('**Extracted relations:**')\n",
    "    print()\n",
    "    \n",
    "    all_relations={'founding_year':{}, 'nationality': {}}\n",
    "    \n",
    "    for index, text in enumerate(texts):\n",
    "        doc = nlp(text)\n",
    "        date_relations=extract_date_relations(doc, founded_patterns)\n",
    "        nationality_relations=extract_nationality(doc)\n",
    "        \n",
    "        all_relations['founding_year'].update(date_relations)\n",
    "        all_relations['nationality'].update(nationality_relations)\n",
    "\n",
    "    print(all_relations)\n",
    "    \n",
    "    \n",
    "    # Expected output:\n",
    "    # Net income      MONEY   $9.4 million\n",
    "    # the prior year  MONEY   $2.7 million\n",
    "    # Revenue         MONEY   twelve billion dollars\n",
    "    # a loss          MONEY   1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Processing wikipedia\n",
    "\n",
    "Now that we know how to run our extractors on some text documents, we can do this on a larger scale. As an illustration, here we will load a few Wikipedia documents and try to extract properties from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities=[\"Piek Vossen\", \"Frank van Harmelen\", \"Bremen High School (Midlothian, Illinois)\"]\n",
    "texts=[]\n",
    "\n",
    "for entity in entities:\n",
    "    wp = wikipedia.page(entity)\n",
    "    texts.append(wp.content)\n",
    "    \n",
    "system_data = get_all_relations(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluating extractors\n",
    "\n",
    "We will evaluate the extractors by computing a precision, recall, and F1-score per document. We will only check the extracted values for the main entity in the document, not for any of the others.\n",
    "\n",
    "Similar as with entity linking, we will decide on true positives, false positives, false negatives per textual unit and not per class. In this case, the textual unit we will use is the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties=['founding_year', 'nationality']\n",
    "gold={}\n",
    "gold['nationality']={'Piek Vossen': 'Dutch', 'Frank van Harmelen': 'Dutch'}\n",
    "gold['founding_year']={'Bremen High School (Midlothian, Illinois)': '1953'}\n",
    "\n",
    "for prop, property_data in gold.items():\n",
    "    tp=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "    for entity, gold_value in property_data.items():\n",
    "        if entity in system_data[prop]: \n",
    "            system_value=system_data[prop][entity]\n",
    "            if system_value==gold_value:\n",
    "                tp+=1\n",
    "            else:\n",
    "                fp+=1\n",
    "                fn+=1\n",
    "        else:\n",
    "            fn+=1\n",
    "        print('Entity: %s, property: %s, gold value: %s, system value: %s' % (entity, prop, gold_value, system_value))\n",
    "        \n",
    "    precision=tp/(tp+fp)\n",
    "    recall=tp/(tp+fn)\n",
    "    f1=2*precision*recall/(precision+recall)\n",
    "    \n",
    "    print(\"Evaluation for property %s: \\nprecision: %f, \\nrecall: %f, \\nF1-score: %f\" % (prop, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
