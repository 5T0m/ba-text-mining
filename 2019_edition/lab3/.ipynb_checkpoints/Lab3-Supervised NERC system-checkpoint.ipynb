{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3-Supervised NERC classifier (SVM)\n",
    "\n",
    "In this notebook, we provide more information about Named Entity Recognition and Classification (NERC).\n",
    "\n",
    "**At the end of this notebook, you will be able to**:\n",
    "* understand the IOB format used to format NERC data\n",
    "* represent linguistic features as vectors\n",
    "* train a NERC classifier (SVM)\n",
    "* apply the classifier to unseen data\n",
    "\n",
    "**Useful links**:\n",
    "* [blog about SVM](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)\n",
    "* [blog about SVM in scikit-learn](https://medium.com/@aneesha/visualising-top-features-in-linear-svm-with-scikit-learn-and-matplotlib-3454ab18a14d)\n",
    "* [blog about inspecting top features using scikit-learn](https://towardsdatascience.com/named-entity-recognition-and-classification-with-scikit-learn-f05372f07ba2)\n",
    "* [one hot encoding](https://scikit-learn.org/dev/modules/feature_extraction.html#loading-features-from-dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NERC\n",
    "In Named Entity Recognition and Classification, the goal is to determine which noun phrases refer to named entities as well as classifying them.\n",
    "Named entities can be persons, locations, organizations, etc. (see [NLTK Chapter 7, Section 5](https://www.nltk.org/book/ch07.html) for more information on the task)\n",
    "\n",
    "![title](https://researchkb.files.wordpress.com/2014/02/ner.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not trivial to represent NERC data in a way that we can easily train NLP systems as well as evaluate them. One of the most used formats is called [Inside–outside–beginning (IOB)](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)). Let's look at an example from one of the most popular datasets, which is [CoNLL-2003](http://aclweb.org/anthology/W03-0419).\n",
    "```\n",
    "Germany NNP B-NP B-LOC\n",
    "'s POS B-NP O\n",
    "representative NN I-NP O\n",
    "to TO B-PP O\n",
    "the DT B-NP O\n",
    "European NNP I-NP B-ORG\n",
    "Union NNP I-NP I-ORG\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first observation is that all information is represented at the **token-level**. For each token, e.g., *Germany*, we receive information about:\n",
    "* **the word**: e.g., *Germany*\n",
    "* **the part of speech**: e.g., *NNP* (from [Penn Treebank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html))\n",
    "* **the phrase type**: e.g., a noun phrase\n",
    "* **the NERC label**: e.g., a location (LOC).\n",
    "\n",
    "This example contains two named entities: *Germany* and *European Union*.\n",
    "\n",
    "Every first token of a named entity is prefixed with *B-*. Every token after that, e.g., *Union* in *European Union*, is prefixed with *I-*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the IOB format is at the **token-level**, which means that we also are going to train and evaluate an NLP system at the token-level! The goal will hence not be to classify *European Union* as an *Organization*, but to classify:\n",
    "* *European* as the first token of an entity that is an *Organization*\n",
    "* *Union* as a token inside of an entity that is an *Organization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SVM\n",
    "We are going to train an SVM for the NERC task. The goal of an SVM is to find a hyperplane in an n-dimensional space that distinctly classifies the data points. This is exactly the problem at hand. We have multiple NERC labels and we want to classify them correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Scikit-learn\n",
    "We are going to use the **svm** module from **sklearn**, from which we will select the **LinearSVR** (Linear Support Vector Regression) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.LinearSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Representing features in sklearn.svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to when we trained a Sentiment Analyzer in Lab 2, we need to represent training instances using a vector representation. For each training instance, we need:\n",
    "* **its feature vector**\n",
    "* **the NERC label**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show how to train and evaluate an SVM using a made-up example of multi-class classification for a non-linguistic dataset. The goal is to predict someone's favorite color based on their properties.\n",
    "\n",
    "We use three features:\n",
    "* **age in years**\n",
    "* **height in cms**\n",
    "* **number of ice cream cones eaten per year**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature representation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[30, 180, 1000], \n",
    "     [80, 180, 100],\n",
    "     [50, 180, 100],\n",
    "     [40, 160, 500],\n",
    "     [15, 160, 400]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that each row represents a training instance, i.e., the age, height, and the number of ice cream cones eaten in a year for a specific person. Each column represents a feature, i.e., the first column represents the age feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are represented in the following way, i.e., the favorite colors of each training instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [\"green\", \n",
    "     \"blue\",\n",
    "     \"blue\",\n",
    "     \"red\",\n",
    "     \"red\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[30, 180, 1000], \n",
    "     [80, 180, 100],\n",
    "     [50, 180, 100],\n",
    "     [40, 160, 500],\n",
    "     [15, 160, 400]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instanciate the model that we'll be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model. You might get a warning stating that:\n",
    "```\n",
    "ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to be expected given that we only train using five instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Applying the model to new instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what the model thinks the favorite color is of someone of 35 years, 180cm, and who eats a 100 ice cream cones per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = lin_clf.predict([[35, \n",
    "                                    180, \n",
    "                                    100]])\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently the SVM thinks it is **blue**, which is not surprising since **number of ice cream cones eaten per year** seems to correlate highly with the favorite colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Incorporating linguistic features\n",
    "So far, we dealt with features that were numbers. However, in NLP problems, we often deal with features such as:\n",
    "* part of speech\n",
    "* lemma\n",
    "* ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can an SVM deal with strings? The answer is: not really.\n",
    "How can we then represent linguistic information about each token in the following sentence:\n",
    "* **The cat sleeps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can if we use something called **one hot encoding**! When we represented *age* in the example from above, we used one column for that feature (see first column of the following feature vectors). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[30, 180, 1000], \n",
    "     [80, 180, 100],\n",
    "     [50, 180, 100],\n",
    "     [40, 160, 500],\n",
    "     [15, 160, 400]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one hot encoding, you use a column for **each possible value** of a feature. This means that it is important to know the possible values of a feature since this will be a closed class. We represent for each feature value whether the feature in that value occurs in a training instance. \n",
    "Let's try to represent the features **part of speech** and **lemma**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accomplish this, we use the **DictVectorizer** from sklearn. Our input format is now a list with dictionaries, with each dictionary representing a training instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances = [\n",
    " {'part-of-speech': 'determiner', 'lemma': 'the'},\n",
    " {'part-of-speech': 'noun', 'lemma': 'cat'},\n",
    " {'part-of-speech': 'verb', 'lemma': 'sleep'},\n",
    "]\n",
    "\n",
    "vec = DictVectorizer()\n",
    "\n",
    "the_array = vec.fit_transform(training_instances).toarray()\n",
    "print(the_array)\n",
    "print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the first column now informs us whether a training instances contains the feature *lemma* with the value *cat*. If this is not the case, the value is 0. If this is the case, the value is 1. Please note that the number of columns grows immmensely when using one hot encoding.\n",
    "\n",
    "Let's inspect the first row, i.e. the one hot encoding representation of the following training instance,\n",
    "```\n",
    " {'part-of-speech': 'determiner', 'lemma': 'the'}\n",
    "```\n",
    "The feature vector using one hot encoding is:\n",
    "```\n",
    "[0. 0. 1. 1. 0. 0.]\n",
    "````\n",
    "* **first value: 0**: the feature *lemma* with the value *cat* does not occur in the training instance\n",
    "* **second value: 0**: the feature *lemma* with the value *sleep* does not occur in the training instance\n",
    "* **third value: 1**: the feature *lemma* with the value *the* does occur in the training instance\n",
    "* ....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NERC datasets\n",
    "Now that we've seen how to represent linguistic features, we also need to access relevant linguistic training data for the NERC task. One of the most popular datasets is [CoNLL-2003](http://aclweb.org/anthology/W03-0419), which was provided with the zip file you downloaded from Canvas.\n",
    "You can load it using the following code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "\n",
    "train = ConllCorpusReader('NERC_datasets/CONLL2003', # the folder where ConLL-2003 is stored (you downloaded this with the zip file from canvas) \n",
    "                          'train.txt', # this will load the file 'train.txt', for the exercise you also need to load 'test.xt' \n",
    "                          ['words', 'pos', 'ignore', 'chunk'])\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    print(token, pos, ne_label) # please represent this information using a dictionary for the feature representation\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle\n",
    "Another interesting dataset is the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus), which we also provided in the zip file you downloaded from Canvas. You can load it in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'NERC_datasets/kaggle/ner_v2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_dataset = pandas.read_csv(path, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the following output after running the above code cell:\n",
    "```\n",
    "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n",
    "```\n",
    "You can ignore this.\n",
    "\n",
    "**pandas.read_csv** will load the csv file into a [pandas DataFrame](https://towardsdatascience.com/pandas-dataframe-a-lightweight-intro-680e3a212b96)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect which columns are in the csv file by running the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus), you can read what each column represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You loop can loop through the dataset in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, instance in kaggle_dataset.iterrows():\n",
    "    print()\n",
    "    print(index)\n",
    "    print(instance) # you can access information by using instance['A COLUMN NAME'] which you can use to convert to a dictionary needed for the feature representation.\n",
    "    print('NERC label', instance['tag'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
