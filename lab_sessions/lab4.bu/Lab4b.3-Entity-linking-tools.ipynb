{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB4 - Disambiguating with existing tools\n",
    "\n",
    "There are many existing entity linking tools out there. We will not build our own, but instead run two existing tools and dig deeper into their output. In order to do this, we will do the following steps:\n",
    "1. Setup your environment (installation of modules and download of dataset)\n",
    "2. Load the dataset\n",
    "3. Run disambiguation with our tools on top of recognized mentions\n",
    "4. Run entity annotation from scratch: recognition and disambiguation together\n",
    "\n",
    "### 1. Setup your environment\n",
    "\n",
    "#### 1.1 Install the needed modules\n",
    "\n",
    "For the purpose of this week's coding exercises, we will need some new libraries that are probably not installed on your computer:\n",
    "* [rdflib](https://pypi.org/project/rdflib/)\n",
    "* (perhaps) [lxml](https://pypi.org/project/lxml/)\n",
    "\n",
    "You should install these libraries using `conda` or `pip`.\n",
    "\n",
    "Let's now check if all needed libraries are installed on your computer and can be imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import requests\n",
    "import urllib\n",
    "import urllib.parse\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.parse import urlencode\n",
    "import xml.etree.cElementTree as ET\n",
    "from lxml import etree\n",
    "import time\n",
    "import json\n",
    "\n",
    "# import our own utility functions and classes\n",
    "import lab4_utils as utils\n",
    "import lab4_classes as classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you see any errors with the imports:**\n",
    "* read the error carefully\n",
    "* install the library that is missing\n",
    "* try to execute the imports again\n",
    "\n",
    "Don't proceed before the imports work; we will use these modules in the explanation below and in the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Obtain the N3 Reuters-128 dataset\n",
    "\n",
    "**This step should be done if you downloaded the .zip package from Canvas.**\n",
    "\n",
    "We will work with a small dataset called Reuters-128. This dataset contains 128 Reuters documents annotated with entity mentions and links to DBpedia. \n",
    "You can download this dataset from Canvas or from https://raw.githubusercontent.com/dice-group/n3-collection/master/Reuters-128.ttl.\n",
    "\n",
    "Store the dataset file 'Reuters-128.ttl' in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Before proceding, please verify that your setup of libraries (1.1) is correct and that the dataset is downloaded in the right location (1.2).*\n",
    "\n",
    "Congratulations, you are all set up!\n",
    "\n",
    "### 2. Load the data from N3\n",
    "\n",
    "**Let's now parse this dataset to a list of news items objects that contain: 1) the text and 2) the entity mentions+links for each news item.**\n",
    "\n",
    "The data is in a .ttl format called NIF (don't worry about these formats for now, we provide a function to parse this dataset to python classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_file='Reuters-128.ttl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=utils.load_article_from_nif_file(reuters_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data description**\n",
    "\n",
    "`articles` is a list of news articles (members of the `NewsItem` which we define ourselves). To help you understand, we now present the information that we store for each news article. \n",
    "\n",
    "Each news item contains the following fields: `identifier` (the original identifier of this document in the dataset), `dct` (the document creation time, if known), `content` (the text of this document), and `entity_mentions` (a list of entity mention occurrences that were found in this document).\n",
    "\n",
    "For each of the entity mentions, we store then the following information: `sentence` (in which sentence was this mention found), `mention` (the exact phrase of this mention), `the_type` (its type, if known), `begin_index` (the starting offset of this mention), `end_index` (the ending offset of this mention), `gold_link` (the gold link for this mention), `aida_link` (the link for this mention proposed by AIDA), and `spotlight_link` (the link for this mention proposed by Spotlight)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Disambiguation with existing entity linking tools\n",
    "\n",
    "We will perform entity linking on this dataset with two modern tools, called AIDA and DBpedia Spotlight.\n",
    "\n",
    "#### 3.1 AIDA\n",
    "\n",
    "AIDA is an entity linking system that puts all entity candidates in a graph network and then performs a probabilistic optimization to find the best connected candidate in this graph for each of the entity mentions. We call this **collective** or **global** disambiguation, because all entity mentions are disambiguated together.\n",
    "\n",
    "AIDA uses two kinds of connections in their algorithm: \n",
    "* The first kind is between a mention and an entity instance and tells us how often is a mention referred to by an instance. For example, the mention \"Jordan\" refers most often to the country, while less often to the basketball player, Michael Jordan. \n",
    "* The second kind of connections is between two entity instances; it tells us how well-connected are two entities. For example, the country Jordan is better connected with Saudi Arabia then it is with the basketball club San Antonio.\n",
    "\n",
    "You can play with AIDA by using their demo page: https://gate.d5.mpi-inf.mpg.de/webaida/.\n",
    "\n",
    "More description can be found in their paper: http://www.aclweb.org/anthology/D11-1072.\n",
    "\n",
    "In the function `aida_disambiguation` below, we do the following: \n",
    "* we iterate the 128 documents of the Reuters-128 dataset (called `articles`)\n",
    "* for each document, we combine the text and the marking of the entity mentions in a single string `new_content`\n",
    "* we send a request to AIDA to disambiguate the entities in this string\n",
    "* we store the entity links back to the `articles` in the field `aida_link`\n",
    "\n",
    "**Note:** You might encounter an SSL error such as `SSL: CERTIFICATE_VERIFY_FAILED` when running the urlopen command. This happens sometimes when your browser does not trust the certificate of some pages, it is a safety mechanism. Do the following to tell your browser that it can trust the web service we use:\n",
    "\n",
    "```import ssl\n",
    "context = ssl._create_unverified_context()\n",
    "```\n",
    "Then use this context in the urlopen call:\n",
    "\n",
    "`urllib.urlopen(\"https://your-url\", context=context)`\n",
    "\n",
    "If you can't fix this immediately, please ask us during the lab session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aida_disambiguation(articles, aida_url):\n",
    "    \"\"\"\n",
    "    Perform disambiguation with AIDA.\n",
    "    \"\"\"\n",
    "    with tqdm(total=len(articles), file=sys.stdout) as pbar:\n",
    "        for i, article in enumerate(articles):\n",
    "                                    \n",
    "            # AIDA expects entity mentions that are pre-marked inside text. \n",
    "            # For example, the sentence \"Obama visited Paris today\", \n",
    "            # should be transformed to \"[[Obama]] visited [[Paris]] today.\"\n",
    "            # We do this in the next 5 lines of code.\n",
    "            original_content = article.content\n",
    "            new_content=original_content\n",
    "            for entity in reversed(article.entity_mentions):\n",
    "                entity_span=new_content[entity.begin_index: entity.end_index]\n",
    "                new_content=new_content[:entity.begin_index] + '[[' + entity_span + ']]' + new_content[entity.end_index:]\n",
    "\n",
    "            # Now, we can run the AIDA library with this string.\n",
    "            params={\"text\": new_content, \"tag_mode\": 'manual'}\n",
    "            request = Request(aida_url, urlencode(params).encode())\n",
    "            this_json = urlopen(request).read().decode('unicode-escape')\n",
    "            try:\n",
    "                results=json.loads(this_json)\n",
    "            except:\n",
    "                continue\n",
    "            # Let's normalize the disambiguated enties.\n",
    "            # This means mostly removing the first part of the URI which is always the same (YAGO:)\n",
    "            # and leaving only the entity identification part (e.g., Barack_Obama).\n",
    "            dis_entities={}\n",
    "            for dis_entity in results['mentions']:\n",
    "                if 'bestEntity' in dis_entity.keys():\n",
    "                    best_entity=dis_entity['bestEntity']['kbIdentifier']\n",
    "                    clean_url=best_entity[5:]\n",
    "                else:\n",
    "                    clean_url='NIL'\n",
    "                dis_entities[str(dis_entity['offset'])] = clean_url\n",
    "                \n",
    "            # We can now store the entity to our class instance for later processing.\n",
    "            for entity in article.entity_mentions:\n",
    "                start = entity.begin_index\n",
    "                try:\n",
    "                    dis_url = str(dis_entities[str(start)])\n",
    "                except:\n",
    "                    dis_url='NIL'\n",
    "                entity.aida_link = dis_url\n",
    "\n",
    "            # The next two lines only update the progress bar\n",
    "            pbar.set_description('processed: %d' % (1 + i))\n",
    "            pbar.update(1)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 1:   1%|          | 1/128 [00:00<01:39,  1.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: DeprecationWarning: invalid escape sequence '\\/'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 128:  96%|█████████▌| 123/128 [04:03<00:02,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# AIDA is running in an external location - for this reason, we need to send an HTTP request. This should take a few minutes.\n",
    "aida_disambiguation_url = \"https://gate.d5.mpi-inf.mpg.de/aida/service/disambiguate\"\n",
    "\n",
    "processed_aida=aida_disambiguation(articles, aida_disambiguation_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The progress bar is sometimes stopping before 128. To be sure whether the command is done executing, please pay attention to the asterisk sign left from the cell. Once the '\\*' sign turns into a number, you can be sure that you can proceed with the notebook! Also, look at the first printed number (left from the progress bar), it should say “processed: 128” when it is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 DBpedia Spotlight\n",
    "\n",
    "DBpedia Spotlight (or only \"Spotlight\" for brevity) is an entity recognition and linking tool that performs linking to DBpedia. The core of their method is a vector space model to compute similarity between the text to annotate and the Wikipedia pages of all entity candidates for a mention. Then the entities with largest similarity are chosen.\n",
    "\n",
    "You can try out their demo as well: http://dbpedia-spotlight.github.com/demo/.\n",
    "\n",
    "Here is their paper: http://oa.upm.es/8923/1/DBpedia_Spotlight.pdf\n",
    "\n",
    "In the function `spotlight_disambiguation` below, we do the following: \n",
    "* we iterate the 128 documents of the Reuters-128 dataset (called `articles`)\n",
    "* for each document, we combine the text and the marking of the entity mentions in a single string `new_content`\n",
    "* we send a request to Spotlight to disambiguate the entities in this string\n",
    "* we store the entity links back to the `articles` in the field `spotlight_link`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spotlight_disambiguate(articles, spotlight_url):\n",
    "    \"\"\"\n",
    "    Perform disambiguation with DBpedia Spotlight.\n",
    "    \"\"\"\n",
    "    with tqdm(total=len(articles), file=sys.stdout) as pbar:\n",
    "        for i, article in enumerate(articles):\n",
    "            # Similar as with AIDA, we first prepare the document text and the mentions\n",
    "            # in order to provide these to Spotlight as input.\n",
    "            annotation = etree.Element(\"annotation\", text=article.content)\n",
    "            for mention in article.entity_mentions:\n",
    "                sf = etree.SubElement(annotation, \"surfaceForm\")\n",
    "                sf.set(\"name\", mention.mention)\n",
    "                sf.set(\"offset\", str(mention.begin_index))\n",
    "            my_xml=etree.tostring(annotation, xml_declaration=True, encoding='UTF-8')\n",
    "            \n",
    "            # Send a disambiguation request to spotlight\n",
    "            results=requests.post(spotlight_url, urllib.parse.urlencode({'text':my_xml, 'confidence': 0.5}), \n",
    "                                  headers={'Accept': 'application/json'})\n",
    "            \n",
    "            # Process the results and normalize the entity URIs\n",
    "            j=results.json()\n",
    "            dis_entities={}\n",
    "            if 'Resources' in j: \n",
    "                resources=j['Resources']\n",
    "            else: \n",
    "                resources=[]\n",
    "            for dis_entity in resources:\n",
    "                dis_entities[str(dis_entity['@offset'])] = utils.normalizeURL(dis_entity['@URI'])\n",
    "            \n",
    "            # Let's now store the URLs by Spotlight to our class for later analysis.\n",
    "            for entity in article.entity_mentions:\n",
    "                start = entity.begin_index\n",
    "                if str(start) in dis_entities:\n",
    "                    dis_url = dis_entities[str(start)]\n",
    "                else:\n",
    "                    dis_url = 'NIL'\n",
    "                entity.spotlight_link = dis_url\n",
    "    \n",
    "            # The next two lines only update the progress bar\n",
    "            pbar.set_description('processed: %d' % (1 + i))\n",
    "            pbar.update(1)\n",
    "                \n",
    "            # Pause for 100ms to prevent overloading the server\n",
    "            time.sleep(0.1)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 128: 100%|██████████| 128/128 [00:29<00:00,  7.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Spotlight is running in an external location - for this reason, we need to send an HTTP request. Hopefully, this will not take a long time.\n",
    "# To speed it up, we have set out own server. Please try that first, if this fails then use the public API.\n",
    "\n",
    "spotlight_disambiguation_url=\"http://spotlight.fii800.lod.labs.vu.nl/rest/disambiguate\" # Uses data from April 2016\n",
    "#spotlight_disambiguation_url=\"http://model.dbpedia-spotlight.org/en/disambiguate\" # Uses data from February 2018\n",
    "\n",
    "processed_both=spotlight_disambiguate(processed_aida, spotlight_disambiguation_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Comparing the output of AIDA and Spotlight to the gold links\n",
    "\n",
    "Because we stored the decisions by both tools in our list `articles`, we can now compare their output.\n",
    "\n",
    "Let's pick an article and print the decisions made by AIDA and Spotlight on that article, and then compare that to the gold link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_number=81\n",
    "\n",
    "an_article=processed_both[article_number]\n",
    "doc_id=an_article.identifier\n",
    "print(doc_id)\n",
    "for m in an_article.entity_mentions:\n",
    "    print('%s\\t%s\\t%s\\t%s' % (m.mention, m.gold_link, m.aida_link, m.spotlight_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Entity annotation from scratch: Performing recognition and disambiguation together\n",
    "\n",
    "Some tools only perform disambiguation (AGDISTIS is an example), whereas others (like Spotlight or AIDA) can perform both recognition and disambiguation.\n",
    "\n",
    "Here we will use Spotlight annotate some text with entities without prior marking of entities. Notice that now we call the function `annotate` instead of sending a request to `disambiguate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotlight_annotation_url='http://spotlight.fii800.lod.labs.vu.nl/rest/annotate'\n",
    "headers={'Accept': 'application/json'}\n",
    "min_confidence=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "On November 24, Aziz Karimov, a journalist based in Baku, received an email from Facebook notifying him of a request to reset his password. \n",
    "Karimov knew something was wrong since he hadn’t requested a password change. \n",
    "Ninety minutes later, as he struggled to regain access to his account, he received four more notifications from Facebook. \n",
    "He was informed that he had also been removed as an administrator from four other pages, including one belonging to Turan News Agency, \n",
    "Azerbaijan’s only independent news agency.\n",
    "'''\n",
    "\n",
    "# Send an annotation request to spotlight\n",
    "response=requests.post(spotlight_annotation_url, \n",
    "                        urllib.parse.urlencode({'text':text, \n",
    "                                                'confidence': min_confidence}), \n",
    "                        headers=headers)\n",
    "\n",
    "response_json=response.json()\n",
    "list_of_entities=response_json['Resources']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which entity mentions were recognized by Spotlight and which links were assigned to these entity mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
