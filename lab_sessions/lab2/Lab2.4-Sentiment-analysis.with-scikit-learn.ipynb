{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2 - Sentiment Analysis with Scikit-Learn\n",
    "\n",
    "The focus of this notebook is on performing sentiment analysis using the scikit-learn package. Material from [this notebook](http://www.pitt.edu/~naraehan/presentation/Movie+Reviews+sentiment+analysis+with+Scikit-Learn.html) was used.\n",
    "\n",
    "**at the end of this notebook, you will be able to**:\n",
    "* load the training data, i.e., the movie reviews\n",
    "* inspect the training data, i.e., the movie reviews\n",
    "* extracting features from the training data\n",
    "* training and evaluating the *NaiveBayesClassifier*\n",
    "* apply the classifier to fake movie reviews\n",
    "\n",
    "**If you want to learn more, you might find the following link useful:**\n",
    "* [documentation on dataset loading](http://scikit-learn.org/stable/datasets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sklearn\n",
    "import numpy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "We are first going to load and inspect the **airlinetweets** dataset (which is part of the zip file you downloaded from Github).\n",
    "We are going to use the method **load_files** as part of sklearn.\n",
    "Let's first inspect what the help message of the function **load_files** states. Take your time to read the help instructions and to learn about the load_files function of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_files in module sklearn.datasets.base:\n",
      "\n",
      "load_files(container_path, description=None, categories=None, load_content=True, shuffle=True, encoding=None, decode_error='strict', random_state=0)\n",
      "    Load text files with categories as subfolder names.\n",
      "    \n",
      "    Individual samples are assumed to be files stored a two levels folder\n",
      "    structure such as the following:\n",
      "    \n",
      "        container_folder/\n",
      "            category_1_folder/\n",
      "                file_1.txt\n",
      "                file_2.txt\n",
      "                ...\n",
      "                file_42.txt\n",
      "            category_2_folder/\n",
      "                file_43.txt\n",
      "                file_44.txt\n",
      "                ...\n",
      "    \n",
      "    The folder names are used as supervised signal label names. The individual\n",
      "    file names are not important.\n",
      "    \n",
      "    This function does not try to extract features into a numpy array or scipy\n",
      "    sparse matrix. In addition, if load_content is false it does not try to\n",
      "    load the files in memory.\n",
      "    \n",
      "    To use text files in a scikit-learn classification or clustering algorithm,\n",
      "    you will need to use the `sklearn.feature_extraction.text` module to build\n",
      "    a feature extraction transformer that suits your problem.\n",
      "    \n",
      "    If you set load_content=True, you should also specify the encoding of the\n",
      "    text using the 'encoding' parameter. For many modern text files, 'utf-8'\n",
      "    will be the correct encoding. If you leave encoding equal to None, then the\n",
      "    content will be made of bytes instead of Unicode, and you will not be able\n",
      "    to use most functions in `sklearn.feature_extraction.text`.\n",
      "    \n",
      "    Similar feature extractors should be built for other kind of unstructured\n",
      "    data input such as images, audio, video, ...\n",
      "    \n",
      "    Read more in the :ref:`User Guide <datasets>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    container_path : string or unicode\n",
      "        Path to the main folder holding one subfolder per category\n",
      "    \n",
      "    description : string or unicode, optional (default=None)\n",
      "        A paragraph describing the characteristic of the dataset: its source,\n",
      "        reference, etc.\n",
      "    \n",
      "    categories : A collection of strings or None, optional (default=None)\n",
      "        If None (default), load all the categories. If not None, list of\n",
      "        category names to load (other categories ignored).\n",
      "    \n",
      "    load_content : boolean, optional (default=True)\n",
      "        Whether to load or not the content of the different files. If true a\n",
      "        'data' attribute containing the text information is present in the data\n",
      "        structure returned. If not, a filenames attribute gives the path to the\n",
      "        files.\n",
      "    \n",
      "    shuffle : bool, optional (default=True)\n",
      "        Whether or not to shuffle the data: might be important for models that\n",
      "        make the assumption that the samples are independent and identically\n",
      "        distributed (i.i.d.), such as stochastic gradient descent.\n",
      "    \n",
      "    encoding : string or None (default is None)\n",
      "        If None, do not try to decode the content of the files (e.g. for images\n",
      "        or other non-text content). If not None, encoding to use to decode text\n",
      "        files to Unicode if load_content is True.\n",
      "    \n",
      "    decode_error : {'strict', 'ignore', 'replace'}, optional\n",
      "        Instruction on what to do if a byte sequence is given to analyze that\n",
      "        contains characters not of the given `encoding`. Passed as keyword\n",
      "        argument 'errors' to bytes.decode.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, optional (default=0)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    data : Bunch\n",
      "        Dictionary-like object, the interesting attributes are: either\n",
      "        data, the raw text data to learn, or 'filenames', the files\n",
      "        holding it, 'target', the classification labels (integer index),\n",
      "        'target_names', the meaning of the labels, and 'DESCR', the full\n",
      "        description of the dataset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To get help use the next command!\n",
    "help(load_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the function requires the following structure in order for it to work:\n",
    "* container_folder/\n",
    "    * category_1_folder/ (e.g., 'pos')\n",
    "        * file_1.txt\n",
    "        * file_2.txt\n",
    "        * ...\n",
    "        file_42.txt\n",
    "    * category_2_folder/ (e.g., 'neg')\n",
    "        * file_43.txt\n",
    "        * file_44.txt\n",
    "        * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether our **airlinetweets** corpus has this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: /Users/piek/Desktop/CBS2019/text-mining-ba/lab_sessions/lab2/airlinetweets\n",
      "this will print True if the folder exists: True\n"
     ]
    }
   ],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "print('path:', airline_tweets_folder)\n",
    "print('this will print True if the folder exists:', \n",
    "      airline_tweets_folder.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/piek/Desktop/CBS2019/text-mining-ba/lab_sessions/lab2/airlinetweets'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(airline_tweets_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect whether the corpus has the required structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "....\n",
    "How may categories are represented?\n",
    "How would you add another one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, it is! Let's now load it using the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading all files as training data.\n",
    "airline_tweets_train = load_files(str(airline_tweets_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many files do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4755"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(airline_tweets_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'neutral', 'positive']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target names (\"classes\") are automatically generated from subfolder names\n",
    "airline_tweets_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many do we have for each category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral 1515\n",
      "positive 1490\n",
      "negative 1750\n"
     ]
    }
   ],
   "source": [
    "freqs = Counter(airline_tweets_train.target)\n",
    "for category, frequency in freqs.items():\n",
    "    print(airline_tweets_train.target_names[category], frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'@AmericanAir Why is your cover photo of TWA? Just wondering.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's inspect one file\n",
    "airline_tweets_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/piek/Desktop/CBS2019/text-mining-ba/lab_sessions/lab2/airlinetweets/neutral/AL_570069345818161152.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first file is in \"neutral\" folder\n",
    "airline_tweets_train.filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first file is a neutral review and is mapped to index 1 in target_names\n",
    "airline_tweets_train.target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find out what the index means by inserting it into **target_names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_tweets_train.target_names[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from training data (see notebook Lab2-Feature representation.ipynb for more information)\n",
    "Note: you might get a warning when you run the following cell. You do NOT have to resolve the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize airline object, and then turn airline tweets train data into a vector \n",
    "\n",
    "airline_vec = CountVectorizer(min_df=2, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1947"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'plane' is found in the corpus, mapped to index 1948\n",
    "airline_vec.vocabulary_.get('plane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4755, 2901)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# large dimensions! 4,755 documents, 2902 unique terms. \n",
    "airline_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# print a single value in the matrix, most likely to be zero as it is a sparse matrix\n",
    "print(airline_counts[5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw frequency counts into TF-IDF values\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4755, 2901)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same dimensions, now with tf-idf values instead of raw frequency counts\n",
    "airline_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# the same cell as above now has a real value between 0.0 and 1.0, again most likely 0.0\n",
    "print(airline_tfidf[5,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing a Naive Bayes classifier\n",
    "To train the classifier, we will first split the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now ready to build a classifier. \n",
    "# We will use Multinominal Naive Bayes as our model\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "# from sklearn.cross_validation import train_test_split  # deprecated in 0.18\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose 80% training and 20% test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_tfidf, # the tf-idf model\n",
    "    airline_tweets_train.target, # the labels for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One instance looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.37404769, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_train[55].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's label is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we know is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_tweets_train.target_names[y_train[55]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Multimoda Naive Bayes classifier\n",
    "clf = MultinomialNB().fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results, find macro recall\n",
    "y_pred = clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one tweet review: b'@AmericanAir Why is your cover photo of TWA? Just wondering.'\n",
      "gold label: 1\n",
      "classifier predicted: 2\n"
     ]
    }
   ],
   "source": [
    "print('one tweet review:', airline_tweets_train.data[0])\n",
    "print('gold label:', airline_tweets_train.target[0])\n",
    "print('classifier predicted:', y_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See notebook **Lab2-Evaluation** for information on the evaluation. The micro recall of our classifier is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8201892744479495"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.recall_score(y_true=y_test,\n",
    "                             y_pred=y_pred,\n",
    "                             average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the least and most important features per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in negative documents\n",
      "0 160.72275630128797 united\n",
      "0 112.2073442767972 .\n",
      "0 98.79234170705624 @\n",
      "0 97.49655201448451 ``\n",
      "0 56.325119535359896 flight\n",
      "0 51.97956034583433 ?\n",
      "0 45.91755252577125 #\n",
      "0 43.836247099103375 !\n",
      "0 39.44721235726242 n't\n",
      "0 28.477065915318114 ''\n",
      "0 23.88717386464075 service\n",
      "0 22.857121763689662 virginamerica\n",
      "0 21.76236558264745 's\n",
      "0 21.588775192669267 cancelled\n",
      "0 21.56463354329059 delayed\n",
      "0 20.52035376418351 bag\n",
      "0 20.44345278200883 ...\n",
      "0 19.745566945017014 customer\n",
      "0 19.099319471012162 get\n",
      "0 18.55126125062186 hour\n",
      "0 18.186927548171298 :\n",
      "0 18.182707865007288 time\n",
      "0 17.816540187489803 plane\n",
      "0 17.6780903523429 hours\n",
      "0 17.634155551985636 'm\n",
      "0 17.162355412694428 late\n",
      "0 16.197446183503807 gate\n",
      "0 16.09205915224475 worst\n",
      "0 15.897320223364014 still\n",
      "0 15.86130918153623 waiting\n",
      "0 15.615278187002664 airline\n",
      "0 14.552867697466088 http\n",
      "0 14.102561542991312 -\n",
      "0 14.008315700369566 would\n",
      "0 13.566014432122868 never\n",
      "0 13.377359944506344 ;\n",
      "0 13.361060564225628 delay\n",
      "0 13.337940589873643 flightled\n",
      "0 13.180462014704819 help\n",
      "0 13.060088577951793 2\n",
      "0 12.858945555321055 &\n",
      "0 12.78270351810028 luggage\n",
      "0 12.613068424369928 flights\n",
      "0 12.493564309944686 one\n",
      "0 11.906119791922148 ca\n",
      "0 11.296132135499219 $\n",
      "0 11.180079466573213 amp\n",
      "0 11.114040012785058 like\n",
      "0 11.044910542539265 lost\n",
      "0 10.993461583839938 've\n",
      "0 10.892612997774664 3\n",
      "0 10.84126702150531 really\n",
      "0 10.536629279693264 check\n",
      "0 10.138786703047925 yes\n",
      "0 9.952340850517135 wait\n",
      "0 9.883519634299681 (\n",
      "0 9.87706186670697 hold\n",
      "0 9.765847646800529 ever\n",
      "0 9.749207986201812 fly\n",
      "0 9.738370270232881 bags\n",
      "0 9.62103233985287 day\n",
      "0 9.4266092272823 us\n",
      "0 9.32675987909369 another\n",
      "0 9.251572157439968 people\n",
      "0 9.17613885614764 seat\n",
      "0 9.040271409156576 u\n",
      "0 9.020080220132668 back\n",
      "0 8.936678087644545 baggage\n",
      "0 8.799064586418218 guys\n",
      "0 8.70786723033592 due\n",
      "0 8.658930438071167 trying\n",
      "0 8.57063712151516 terrible\n",
      "0 8.414330905121808 last\n",
      "0 8.25829023052434 days\n",
      "0 8.145420107838543 agent\n",
      "0 8.143690738178691 ticket\n",
      "0 8.099589735461791 airport\n",
      "0 8.047632012405451 )\n",
      "0 7.76670370367392 disappointed\n",
      "0 7.758714700351302 sitting\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "1 104.34307228673613 @\n",
      "1 84.71260964181667 ?\n",
      "1 61.69924394517205 jetblue\n",
      "1 58.136455898456546 .\n",
      "1 56.719822733732634 ``\n",
      "1 56.559105648114596 southwestair\n",
      "1 52.270610894380816 :\n",
      "1 51.55491361393676 americanair\n",
      "1 42.535631326621164 usairways\n",
      "1 38.0028493289961 united\n",
      "1 36.898902991360714 flight\n",
      "1 36.650542433759746 http\n",
      "1 36.30831238919164 #\n",
      "1 31.4123161066252 's\n",
      "1 21.502167650939164 !\n",
      "1 19.92119722196717 get\n",
      "1 19.876243905217326 dm\n",
      "1 18.28609474693336 flights\n",
      "1 18.139368276345436 please\n",
      "1 17.159808256560353 virginamerica\n",
      "1 15.523090888692678 -\n",
      "1 15.503470147058989 help\n",
      "1 14.923127756154399 hi\n",
      "1 14.705943992924725 need\n",
      "1 14.371100643605953 fleek\n",
      "1 14.240770856330274 tomorrow\n",
      "1 14.22270328793294 fleet\n",
      "1 13.400722408555627 ...\n",
      "1 13.361597323924249 )\n",
      "1 13.290239150782904 sent\n",
      "1 12.324767484459262 ”\n",
      "1 12.282357127092766 “\n",
      "1 12.112395849115035 flying\n",
      "1 11.666210672128182 (\n",
      "1 10.932697940613064 n't\n",
      "1 10.737290824779775 would\n",
      "1 10.3373745406357 change\n",
      "1 10.034318372808016 us\n",
      "1 9.978400156058408 follow\n",
      "1 9.912070897686558 ''\n",
      "1 9.48291466157149 'm\n",
      "1 9.453237011966547 ;\n",
      "1 9.358287291544771 number\n",
      "1 8.805229888415116 cancelled\n",
      "1 8.740827813371359 know\n",
      "1 8.70936729330617 thanks\n",
      "1 8.704772236879895 like\n",
      "1 8.290238800698951 yes\n",
      "1 8.169486355294005 way\n",
      "1 7.991994319575437 one\n",
      "1 7.806840784209494 &\n",
      "1 7.741546825559591 check\n",
      "1 7.565330728625046 chance\n",
      "1 7.368851797523492 could\n",
      "1 7.231372932352843 rt\n",
      "1 7.077106230769367 see\n",
      "1 7.0320911932003956 amp\n",
      "1 6.900482231379631 fly\n",
      "1 6.753317263812721 travel\n",
      "1 6.703502973624923 following\n",
      "1 6.678636392981388 ticket\n",
      "1 6.630740541471816 go\n",
      "1 6.604682870026729 airport\n",
      "1 6.461181924029748 question\n",
      "1 6.402900379483936 want\n",
      "1 6.319461133595413 destinationdragons\n",
      "1 6.215729853986124 miles\n",
      "1 6.172914374225824 new\n",
      "1 6.122157591937297 today\n",
      "1 6.116623934200764 time\n",
      "1 6.03812189337969 next\n",
      "1 6.025022727166211 think\n",
      "1 5.9290319861761 possible\n",
      "1 5.922213087591183 dfw\n",
      "1 5.882018732050807 use\n",
      "1 5.8577034322573995 'd\n",
      "1 5.801645724103633 make\n",
      "1 5.74449587257425 name\n",
      "1 5.704434541007756 good\n",
      "1 5.6463419692855314 going\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "2 173.99611496913693 !\n",
      "2 103.79113494027959 @\n",
      "2 86.12231385739526 .\n",
      "2 78.99727475434725 thanks\n",
      "2 78.45767423361508 thank\n",
      "2 71.61896626828323 southwestair\n",
      "2 67.60615816327383 jetblue\n",
      "2 61.374090497401916 ``\n",
      "2 51.81504046705595 #\n",
      "2 45.13473255967032 americanair\n",
      "2 36.76855343273344 great\n",
      "2 35.76252268187004 usairways\n",
      "2 35.107030327141764 :\n",
      "2 34.9642779884341 united\n",
      "2 27.43676556281747 flight\n",
      "2 23.567059663609523 love\n",
      "2 22.730560533117877 much\n",
      "2 22.235853343653964 )\n",
      "2 21.55824945400905 virginamerica\n",
      "2 20.769293174663595 service\n",
      "2 20.704178703844097 best\n",
      "2 20.651707723765362 awesome\n",
      "2 18.796838340959628 guys\n",
      "2 17.38405997259562 http\n",
      "2 15.840400478500337 customer\n",
      "2 15.236264674475233 good\n",
      "2 14.983919739987908 got\n",
      "2 14.901490629431251 amazing\n",
      "2 14.492299726614968 -\n",
      "2 12.834975380634425 airline\n",
      "2 12.707101898023351 appreciate\n",
      "2 12.20406223987976 's\n",
      "2 11.96363984210054 response\n",
      "2 11.62297977317399 ;\n",
      "2 11.569151484024186 made\n",
      "2 11.242665196967641 us\n",
      "2 10.838234219029877 help\n",
      "2 10.556973697882247 today\n",
      "2 10.46814692806557 flying\n",
      "2 9.916520125970504 crew\n",
      "2 9.738921844223455 time\n",
      "2 9.228918239996554 always\n",
      "2 9.000703255031402 fly\n",
      "2 8.901449571034979 n't\n",
      "2 8.310834940573049 new\n",
      "2 8.272974732457559 southwest\n",
      "2 8.084413642659978 &\n",
      "2 7.831370821121082 well\n",
      "2 7.592774172757475 nice\n",
      "2 7.5253806321677175 're\n",
      "2 7.513424469899952 ever\n",
      "2 7.505232449875679 see\n",
      "2 7.496655851334111 get\n",
      "2 7.390207629050953 gate\n",
      "2 7.342812904804479 job\n",
      "2 7.315230995819713 yes\n",
      "2 7.28221407760022 quick\n",
      "2 7.211405261420328 finally\n",
      "2 7.202248569804582 follow\n",
      "2 7.09600910325759 'll\n",
      "2 7.028445147540671 back\n",
      "2 6.842880479877967 day\n",
      "2 6.777637269158552 happy\n",
      "2 6.674441276678484 thx\n",
      "2 6.640303840028532 wait\n",
      "2 6.619580432535538 like\n",
      "2 6.557430701361877 done\n",
      "2 6.42890213742389 ...\n",
      "2 6.422334331864977 work\n",
      "2 6.418197711417835 helpful\n",
      "2 6.382890748783796 amp\n",
      "2 6.325716529886949 ok\n",
      "2 6.269225499282457 home\n",
      "2 6.2520299130177674 rock\n",
      "2 6.077556413536224 tonight\n",
      "2 6.022368910667288 u\n",
      "2 6.010576324045869 agent\n",
      "2 6.009595557624274 way\n",
      "2 5.943202936615921 team\n",
      "2 5.690255614401131 definitely\n"
     ]
    }
   ],
   "source": [
    "def important_features_per_class(vectorizer,classifier,n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "# example of how to call from notebook:\n",
    "important_features_per_class(airline_vec, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying classifier on our own data\n",
    "Now we can apply our classifier to new data.\n",
    "In the example below, these are movie reviews. In the exercise, you will choose tweets that you've selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# very short and fake movie reviews\n",
    "reviews_new = ['This movie was excellent', \n",
    "               'Absolute joy ride', \n",
    "               'Steven Seagal was terrible', \n",
    "               'Steven Seagal shined through.', \n",
    "               'This was certainly a movie', \n",
    "               'Two thumbs up', \n",
    "               'I fell asleep halfway through', \n",
    "               \"We can't wait for the sequel!!\", \n",
    "               'I cannot recommend this highly enough', \n",
    "               'instant classic.', \n",
    "               'Steven Seagal was amazing.']\n",
    "len(reviews_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 2901)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We re-use airline_vec to transform it in the same way as the training data\n",
    "new_counts = airline_vec.transform(reviews_new)\n",
    "new_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compute tf idf values\n",
    "reviews_new_tfidf = tfidf_transformer.transform(new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 2901)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_new_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have classifier make a prediction\n",
    "pred = clf.predict(reviews_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was excellent => positive\n",
      "Absolute joy ride => positive\n",
      "Steven Seagal was terrible => negative\n",
      "Steven Seagal shined through. => negative\n",
      "This was certainly a movie => negative\n",
      "Two thumbs up => negative\n",
      "I fell asleep halfway through => positive\n",
      "We can't wait for the sequel!! => negative\n",
      "I cannot recommend this highly enough => negative\n",
      "instant classic. => negative\n",
      "Steven Seagal was amazing. => positive\n"
     ]
    }
   ],
   "source": [
    "# print out results ()\n",
    "for review, predicted_label in zip(reviews_new, pred):\n",
    "    \n",
    "    print('%s => %s' % (review, \n",
    "                        airline_tweets_train.target_names[predicted_label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
