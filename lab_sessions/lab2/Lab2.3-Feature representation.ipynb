{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2.3 Feature representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A critical component of almost any machine learning approach is **feature representation**. \n",
    "This is not strange since we need to somehow convert a textual unit, e.g., word, sentence, tweet, or document, into something meaningful that can not only be interpreted by a computer, but is also useful for the type of learning we want to do. \n",
    "\n",
    "In this notebook, we show two of the basic feature representation used in machine learning: **bag of words** and **TF-IDF**.\n",
    "\n",
    "**At the end of this notebook, you will be able to:**\n",
    "* build a bag of words representation\n",
    "* build a TF-IDF-based model\n",
    "\n",
    "**If you want to learn more: (information from these blogs was used in this notebook)**\n",
    "* [bag of words introduction](http://www.insightsbot.com/blog/R8fu5/bag-of-words-algorithm-in-python-introduction)\n",
    "* [TF-IDF introduction](https://medium.freecodecamp.org/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3)\n",
    "* [another TF-IDF introduction](https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "The bag of word approach consists of two main steps:\n",
    "\n",
    "* 1 we extract all the unique words from a collection of textual units, e.g., documents\n",
    "* 2 we compute the frequency of each word in each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this for the following three sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = ['A rose is a rose',\n",
    "         'A rose is a flower',\n",
    "         \"A book is nice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our text structure is now a list or string, where each string contains separate words or tokens. We can generate such a structure from any text by calling sentence splitting and tokenisation functions. Here each string is a sequence that represents a full texts of a document. \n",
    "\n",
    "Sklearn can deal such representations to create a vector representation for each unit (here the elements in the list) on the basis of the complete vocabulary of words that occurs across all the units.\n",
    "\n",
    "Such a vector can be seen as a one-hot-encoding for the documents, in which each documents is scored for the words out of the total vocabulary that are present in the document. You could also see it as a word-to-document index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the **CountVectorizer** to create the bag of words representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, # in how many documents the term minimally occurs\n",
    "                             tokenizer=nltk.word_tokenize) # we use the nltk tokenizer\n",
    "sents_counts = vectorizer.fit_transform(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows us that we have 3 documents and 6 unique words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6)\n",
      "unique words: ['a', 'rose', 'is', 'flower', 'book', 'nice']\n"
     ]
    }
   ],
   "source": [
    "# sents_counts has a dimension of 3 (document count) by 6 (# of unique words)\n",
    "print(sents_counts.shape)\n",
    "print('unique words:', list(vectorizer.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag of word representation looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'book', 'flower', 'is', 'nice', 'rose']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 1, 0, 2],\n",
       "       [2, 0, 1, 1, 0, 1],\n",
       "       [1, 1, 0, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this vector is small enough to view in full! \n",
    "print(vectorizer.get_feature_names())\n",
    "sents_counts.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, this means that:\n",
    "* the word **a** occurs two times in the first two sentences, and only once in the third.\n",
    "* the word **book** only occurs in the third sentence.\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to realise that the position in the vector represent specific words.  This means that each document is represented through the same vector. Vector positions and length should be same across data representations.\n",
    "\n",
    "This is the most basic feature representation for machine learning. It is easy to see that we can now compare the documents in terms of similarity by simply comparing the counts of the words in the vectorized representations. \n",
    "\n",
    "The similarity of the documents is defined by the degree to which the same words occur equally frequently. Also note that the vectors will become very large and sparse when we vectorize large data collections.\n",
    "\n",
    "The formal way to calculate the similarity across vectors is using the normalised dot product (NDP). NDP of two vectors is the sum of the product of each vector position normalised by the length of the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "One big problem of the bag of words approach is that it treats all words equally. Why is that a disadvantage? It means that words that occur in many documents, such as *a*, contribute equally to the decision making of the machine learning approach as other words that are much more informative, e.g., *rose*. \n",
    "TF-IDF addresses this problem by assigning less weight to words that occur in many documents.\n",
    "You read [here](https://medium.freecodecamp.org/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3) a nice introduction to TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you can do it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "sents_tfidf = tfidf_transformer.fit_transform(sents_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'book', 'flower', 'is', 'nice', 'rose']\n",
      "[[0.6 0.  0.  0.3 0.  0.8]\n",
      " [0.6 0.  0.5 0.3 0.  0.4]\n",
      " [0.4 0.6 0.  0.4 0.6 0. ]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_array = sents_tfidf.toarray()\n",
    "print(vectorizer.get_feature_names())\n",
    "print(numpy.round(tf_idf_array, decimals=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good result! In the bag of words approach, The words **\"a\"** and **\"book\"** both had a frequency of 1 in the third sentence. Now that we've applied the TF-IDF approach, we see that the word *book* has a higher weight (0.6) than the word *\"a\"* since *\"a\"* occurs in all three sentences and *\"book\"* only in one, which might indicate that it is more informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
